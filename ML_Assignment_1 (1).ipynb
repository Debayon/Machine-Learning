{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qz_PNDj3tn_8",
        "KkTAfQG6Mp7p"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7ZBP5avhXhy",
        "outputId": "e56de8f0-ec11-47c5-fd75-020f27273a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Min_Max Scaling function #####\n",
        "\n",
        "def min_max_scale(df):\n",
        "    return (df - df.min()) / (df.max() - df.min())"
      ],
      "metadata": {
        "id": "ARacEPP2rrDD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k0J4AU0egV1",
        "outputId": "11b265dc-0952-455d-a194-cc0440c36523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 8.450e+03 7.000e+00 8.560e+02 1.710e+03 8.000e+00 2.000e+00\n",
            "  5.480e+02]\n",
            " [1.000e+00 9.600e+03 6.000e+00 1.262e+03 1.262e+03 6.000e+00 2.000e+00\n",
            "  4.600e+02]\n",
            " [1.000e+00 1.125e+04 7.000e+00 9.200e+02 1.786e+03 6.000e+00 2.000e+00\n",
            "  6.080e+02]\n",
            " [1.000e+00 9.550e+03 7.000e+00 9.610e+02 1.717e+03 7.000e+00 3.000e+00\n",
            "  6.420e+02]\n",
            " [1.000e+00 1.426e+04 8.000e+00 1.145e+03 2.198e+03 9.000e+00 3.000e+00\n",
            "  8.360e+02]]\n",
            "<class 'numpy.ndarray'>\n",
            "[ 4.51371306e-01  1.52880487e+04  1.47230310e+01  7.64969452e+01\n",
            " -1.33253870e+04  1.74718948e+04  1.89997435e+01]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Housepriceprediction - Housepriceprediction.csv'  # defining the file path in the GDrive\n",
        "\n",
        "df = pd.read_csv(file_path) #Importing the House Price Data\n",
        "\n",
        "df = df.drop(columns=['Id'], axis=1)  #Remove the Id column as it doesn't convey any information\n",
        "\n",
        "# print(df.head())\n",
        "\n",
        "# print(len(df))\n",
        "\n",
        "X, y = df.iloc[:, :-1], df.iloc[:, -1] #Separating features and target variable\n",
        "# print(X.head())\n",
        "# ones = np.ones((X.shape[0], 1))\n",
        "# X_normal = np.concatenate((ones, X), axis=1)\n",
        "# print(X_normal[:5])\n",
        "# theta_normal = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "# print(type(theta_normal))\n",
        "# print(theta_normal)\n",
        "\n",
        "# print(X.head())\n",
        "# print(y.head())\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)  #splitting the dataset into train set and test set\n",
        "\n",
        "train_X, train_y = train_df.iloc[:, :-1], train_df.iloc[:, -1] #Feature set(X) and target variable(y) of train set\n",
        "test_X, test_y = test_df.iloc[:, :-1], test_df.iloc[:, -1]  #Feature set(X) and target variable(y) of test set\n",
        "\n",
        "# print(train_X.head())\n",
        "# print(train_y.head())\n",
        "# print(test_X.head())\n",
        "# print(test_y.head())\n",
        "\n",
        "\n",
        "\n",
        "#####################   Each feature and target variiable of both train and test set being Scaled   #######################\n",
        "scaled_train_X = min_max_scale(train_X)\n",
        "scaled_train_y = min_max_scale(train_y)\n",
        "scaled_test_X = min_max_scale(test_X)\n",
        "scaled_test_y = min_max_scale(test_y)\n",
        "#####################   Each feature and target variiable of both train and test set being Scaled   #######################\n",
        "\n",
        "# print(X[:5])\n",
        "# print(y.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####  Pearson's Correlation\n",
        "\n",
        "feature_names = X.columns.tolist()\n",
        "# print(feature_names)\n",
        "\n",
        "for i in feature_names:\n",
        "  correlation = X[i].corr(y)\n",
        "  print(f\"Correlation between {i} and target variable: \", correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n4t3GaOYQdy",
        "outputId": "cd5bf447-4f8d-4cff-e877-0c5d96712a67"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation between LotArea and target variable:  0.2638433538714056\n",
            "Correlation between OverallQual and target variable:  0.7909816005838044\n",
            "Correlation between 1stFlrSF and target variable:  0.6058521846919147\n",
            "Correlation between GrLivArea and target variable:  0.7086244776126523\n",
            "Correlation between TotRmsAbvGrd and target variable:  0.5337231555820282\n",
            "Correlation between GarageCars and target variable:  0.6404091972583529\n",
            "Correlation between GarageArea and target variable:  0.6234314389183618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "nZhohe15fLVd"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### Function to calculate linear regression model: theta_1*x_1 + theta_2*x_2 + ... + theta_n*x_n #########\n",
        "\n",
        "def f1(X,Theta):\n",
        "  return torch.matmul(X,Theta)"
      ],
      "metadata": {
        "id": "WXvsN7I5fRRg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### Function to calculate Loss: Mean Squared Error Loss  #########################\n",
        "\n",
        "def J2(X,Theta,y):\n",
        "  return (torch.mean(torch.square(f1(X,Theta) - y))/2)"
      ],
      "metadata": {
        "id": "x3_0Lu8FfZY3"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(scaled_train_X.values, dtype=torch.float32)  ############### Converting pandas dataframe to Torch Tensor to apply the torch.backward() to calculate gradient #####################\n",
        "ones_column = torch.ones(X_train.shape[0], 1)###############  creating the 1's column for cancatenating #########################\n",
        "X_train = torch.cat((ones_column, X_train), dim=1)####################  Concatenating a column of 1's to account for the Bias Term  #####################\n",
        "y_train = torch.tensor(scaled_train_y.values, dtype=torch.float32)  ##########   Converting pandas dataframe to Torch Tensor   ################\n",
        "# print(X_train[0])\n",
        "\n",
        "\n",
        "###############   Doing the same as above for  Test Data  ####################\n",
        "X_test = torch.tensor(scaled_test_X.values, dtype=torch.float32)\n",
        "ones_column = torch.ones(X_test.shape[0], 1)\n",
        "X_test = torch.cat((ones_column, X_test), dim=1)\n",
        "y_test = torch.tensor(scaled_test_y.values, dtype=torch.float32)\n",
        "# print(X_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H22UWmvvfbNY",
        "outputId": "fba7203a-0b5c-495f-ed9f-80678f74c21d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0000, 0.0425, 0.6667, 0.3093, 0.2540, 0.4167, 0.5000, 0.3738])\n",
            "tensor([1.0000, 0.0999, 0.5000, 0.2432, 0.1533, 0.3333, 0.2500, 0.1899])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####### Calculation of Theta using Normal Equation  ###############\n",
        "\n",
        "X_train_tr = X_train.t()  #####  X_t = Transpose(X) #######\n",
        "X_tr_X = torch.matmul(X_train_tr, X_train)  #####  X_t.X  #####\n",
        "X_tr_X_inv = torch.inverse(X_tr_X)  #####  (X_t.X)^-1  #####\n",
        "X_tr_X_inv_X_train_tr = torch.matmul(X_tr_X_inv, X_train_tr)  #####  (X_t.X)^-1.X_t  #####\n",
        "theta_normal = torch.matmul(X_tr_X_inv_X_train_tr, y_train)  #####  Theta = (X_t.X)^-1.X_t.y  #####\n",
        "print(theta_normal)\n",
        "# print(X_train_tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obstLXqQqgAf",
        "outputId": "02ceea55-e4d6-4b94-b5d6-165c89c87392"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1290,  0.1873,  0.3350,  0.1405,  0.2677, -0.0091,  0.1014,  0.0209])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############   Evaluation of Normal Equation using R2 score   ##############\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_test_pred_normal = f1(X_test, theta_normal)\n",
        "loss_test_normal = J2(X_test, theta_normal, y_test)\n",
        "print(f\"Normal Loss: {loss_test_normal.item()}\")\n",
        "\n",
        "score_test = r2_score(y_test.detach().numpy(), y_test_pred_normal.detach().numpy())\n",
        "print(f\"Test R² Score: {score_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkgOccI2sWEr",
        "outputId": "7561c5a5-a4c6-4ff5-e29e-d96724dc3342"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normal Loss: 0.001582955475896597\n",
            "Test R² Score: 0.7650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############   Linear Regression model using Scikit Learn's library function   #################\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Coefficient(s):\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#######  Evaluation of model  ########\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R² Score:\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YquBHVh3vxRw",
        "outputId": "183c395d-7066-4d49-a60a-08161a5715e4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient(s): [ 0.          0.18728438  0.33496222  0.14050885  0.26770347 -0.00908843\n",
            "  0.1014021   0.02090825]\n",
            "Intercept: -0.12898134\n",
            "Mean Squared Error: 0.003165904898196459\n",
            "R² Score: 0.7650092840194702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Implementation"
      ],
      "metadata": {
        "id": "zec2y_8i67R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Theta = torch.tensor([0.0] * len(X_train[0]), requires_grad=True)\n",
        "torch.manual_seed(42)\n",
        "Theta = torch.randn(8, requires_grad=True)"
      ],
      "metadata": {
        "id": "-TZ5wLAZkSxA"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001  #### Learning Rate ####"
      ],
      "metadata": {
        "id": "4_ztKRTmkzIv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "num_iterations = 10000\n",
        "for i in range(num_iterations):\n",
        "    # Forward pass: compute predictions and loss\n",
        "    y_pred = f1(X_train, Theta)        # Model's prediction\n",
        "    loss = J2(X_train, Theta, y_train) # loss function\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Parameter update (gradient descent step)\n",
        "    with torch.no_grad():   ### torch.no_grad() is to ensure that this step of updation is not included in the computation graph of Parameter variable Theta #########\n",
        "        Theta -= lr * Theta.grad\n",
        "\n",
        "    # IMPORTANT: Zero the gradients after updating\n",
        "    Theta.grad.zero_()\n",
        "\n",
        "    # Store the loss (convert from tensor to Python float)\n",
        "    current_loss = loss.item()\n",
        "    loss_history.append(current_loss)\n",
        "\n",
        "    # Print the loss for each iteration\n",
        "    print(f\"Iteration {i+1}/{num_iterations}, Loss: {current_loss}\")\n",
        "\n",
        "# Plot the loss vs. iteration\n",
        "plt.plot(range(1, num_iterations + 1), loss_history)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss over Iterations\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QDm28nWDk66s",
        "outputId": "15ce7102-fe07-4dd4-80ce-4733fc5d9fc6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 5001/10000, Loss: 0.026534339413046837\n",
            "Iteration 5002/10000, Loss: 0.026533514261245728\n",
            "Iteration 5003/10000, Loss: 0.02653268538415432\n",
            "Iteration 5004/10000, Loss: 0.02653185836970806\n",
            "Iteration 5005/10000, Loss: 0.026531031355261803\n",
            "Iteration 5006/10000, Loss: 0.026530206203460693\n",
            "Iteration 5007/10000, Loss: 0.026529377326369286\n",
            "Iteration 5008/10000, Loss: 0.026528552174568176\n",
            "Iteration 5009/10000, Loss: 0.02652772329747677\n",
            "Iteration 5010/10000, Loss: 0.02652689628303051\n",
            "Iteration 5011/10000, Loss: 0.02652606926858425\n",
            "Iteration 5012/10000, Loss: 0.026525242254137993\n",
            "Iteration 5013/10000, Loss: 0.026524418964982033\n",
            "Iteration 5014/10000, Loss: 0.026523590087890625\n",
            "Iteration 5015/10000, Loss: 0.026522763073444366\n",
            "Iteration 5016/10000, Loss: 0.02652193419635296\n",
            "Iteration 5017/10000, Loss: 0.02652110904455185\n",
            "Iteration 5018/10000, Loss: 0.02652028203010559\n",
            "Iteration 5019/10000, Loss: 0.02651945874094963\n",
            "Iteration 5020/10000, Loss: 0.026518629863858223\n",
            "Iteration 5021/10000, Loss: 0.026517802849411964\n",
            "Iteration 5022/10000, Loss: 0.026516975834965706\n",
            "Iteration 5023/10000, Loss: 0.026516148820519447\n",
            "Iteration 5024/10000, Loss: 0.02651532180607319\n",
            "Iteration 5025/10000, Loss: 0.02651449479162693\n",
            "Iteration 5026/10000, Loss: 0.02651366963982582\n",
            "Iteration 5027/10000, Loss: 0.026512842625379562\n",
            "Iteration 5028/10000, Loss: 0.026512013748288155\n",
            "Iteration 5029/10000, Loss: 0.026511190459132195\n",
            "Iteration 5030/10000, Loss: 0.026510365307331085\n",
            "Iteration 5031/10000, Loss: 0.026509536430239677\n",
            "Iteration 5032/10000, Loss: 0.02650870755314827\n",
            "Iteration 5033/10000, Loss: 0.02650788612663746\n",
            "Iteration 5034/10000, Loss: 0.02650705724954605\n",
            "Iteration 5035/10000, Loss: 0.026506230235099792\n",
            "Iteration 5036/10000, Loss: 0.026505401358008385\n",
            "Iteration 5037/10000, Loss: 0.026504576206207275\n",
            "Iteration 5038/10000, Loss: 0.026503747329115868\n",
            "Iteration 5039/10000, Loss: 0.02650292031466961\n",
            "Iteration 5040/10000, Loss: 0.02650209702551365\n",
            "Iteration 5041/10000, Loss: 0.02650127001106739\n",
            "Iteration 5042/10000, Loss: 0.02650044485926628\n",
            "Iteration 5043/10000, Loss: 0.026499617844820023\n",
            "Iteration 5044/10000, Loss: 0.026498790830373764\n",
            "Iteration 5045/10000, Loss: 0.026497965678572655\n",
            "Iteration 5046/10000, Loss: 0.026497138664126396\n",
            "Iteration 5047/10000, Loss: 0.026496311649680138\n",
            "Iteration 5048/10000, Loss: 0.02649548649787903\n",
            "Iteration 5049/10000, Loss: 0.026494663208723068\n",
            "Iteration 5050/10000, Loss: 0.026493839919567108\n",
            "Iteration 5051/10000, Loss: 0.0264930110424757\n",
            "Iteration 5052/10000, Loss: 0.02649218589067459\n",
            "Iteration 5053/10000, Loss: 0.02649136260151863\n",
            "Iteration 5054/10000, Loss: 0.026490535587072372\n",
            "Iteration 5055/10000, Loss: 0.026489710435271263\n",
            "Iteration 5056/10000, Loss: 0.026488887146115303\n",
            "Iteration 5057/10000, Loss: 0.026488061994314194\n",
            "Iteration 5058/10000, Loss: 0.026487238705158234\n",
            "Iteration 5059/10000, Loss: 0.026486411690711975\n",
            "Iteration 5060/10000, Loss: 0.026485586538910866\n",
            "Iteration 5061/10000, Loss: 0.026484763249754906\n",
            "Iteration 5062/10000, Loss: 0.026483938097953796\n",
            "Iteration 5063/10000, Loss: 0.026483112946152687\n",
            "Iteration 5064/10000, Loss: 0.026482291519641876\n",
            "Iteration 5065/10000, Loss: 0.026481468230485916\n",
            "Iteration 5066/10000, Loss: 0.02648063935339451\n",
            "Iteration 5067/10000, Loss: 0.0264798142015934\n",
            "Iteration 5068/10000, Loss: 0.02647899091243744\n",
            "Iteration 5069/10000, Loss: 0.02647816762328148\n",
            "Iteration 5070/10000, Loss: 0.02647734433412552\n",
            "Iteration 5071/10000, Loss: 0.02647651545703411\n",
            "Iteration 5072/10000, Loss: 0.0264756940305233\n",
            "Iteration 5073/10000, Loss: 0.02647487074136734\n",
            "Iteration 5074/10000, Loss: 0.02647404372692108\n",
            "Iteration 5075/10000, Loss: 0.026473218575119972\n",
            "Iteration 5076/10000, Loss: 0.02647239714860916\n",
            "Iteration 5077/10000, Loss: 0.026471571996808052\n",
            "Iteration 5078/10000, Loss: 0.02647075243294239\n",
            "Iteration 5079/10000, Loss: 0.026469923555850983\n",
            "Iteration 5080/10000, Loss: 0.02646910399198532\n",
            "Iteration 5081/10000, Loss: 0.026468276977539062\n",
            "Iteration 5082/10000, Loss: 0.02646745555102825\n",
            "Iteration 5083/10000, Loss: 0.026466630399227142\n",
            "Iteration 5084/10000, Loss: 0.026465805247426033\n",
            "Iteration 5085/10000, Loss: 0.02646498568356037\n",
            "Iteration 5086/10000, Loss: 0.026464158669114113\n",
            "Iteration 5087/10000, Loss: 0.026463335379958153\n",
            "Iteration 5088/10000, Loss: 0.026462510228157043\n",
            "Iteration 5089/10000, Loss: 0.026461686939001083\n",
            "Iteration 5090/10000, Loss: 0.026460861787199974\n",
            "Iteration 5091/10000, Loss: 0.026460038498044014\n",
            "Iteration 5092/10000, Loss: 0.026459215208888054\n",
            "Iteration 5093/10000, Loss: 0.026458393782377243\n",
            "Iteration 5094/10000, Loss: 0.026457568630576134\n",
            "Iteration 5095/10000, Loss: 0.026456745341420174\n",
            "Iteration 5096/10000, Loss: 0.026455922052264214\n",
            "Iteration 5097/10000, Loss: 0.026455098763108253\n",
            "Iteration 5098/10000, Loss: 0.026454275473952293\n",
            "Iteration 5099/10000, Loss: 0.026453450322151184\n",
            "Iteration 5100/10000, Loss: 0.026452627032995224\n",
            "Iteration 5101/10000, Loss: 0.026451805606484413\n",
            "Iteration 5102/10000, Loss: 0.026450980454683304\n",
            "Iteration 5103/10000, Loss: 0.026450155302882195\n",
            "Iteration 5104/10000, Loss: 0.026449333876371384\n",
            "Iteration 5105/10000, Loss: 0.026448510587215424\n",
            "Iteration 5106/10000, Loss: 0.026447687298059464\n",
            "Iteration 5107/10000, Loss: 0.026446862146258354\n",
            "Iteration 5108/10000, Loss: 0.026446040719747543\n",
            "Iteration 5109/10000, Loss: 0.026445215567946434\n",
            "Iteration 5110/10000, Loss: 0.026444392278790474\n",
            "Iteration 5111/10000, Loss: 0.026443570852279663\n",
            "Iteration 5112/10000, Loss: 0.026442745700478554\n",
            "Iteration 5113/10000, Loss: 0.026441926136612892\n",
            "Iteration 5114/10000, Loss: 0.026441100984811783\n",
            "Iteration 5115/10000, Loss: 0.026440275833010674\n",
            "Iteration 5116/10000, Loss: 0.026439454406499863\n",
            "Iteration 5117/10000, Loss: 0.026438632979989052\n",
            "Iteration 5118/10000, Loss: 0.026437809690833092\n",
            "Iteration 5119/10000, Loss: 0.026436984539031982\n",
            "Iteration 5120/10000, Loss: 0.026436161249876022\n",
            "Iteration 5121/10000, Loss: 0.02643533982336521\n",
            "Iteration 5122/10000, Loss: 0.02643452025949955\n",
            "Iteration 5123/10000, Loss: 0.02643369510769844\n",
            "Iteration 5124/10000, Loss: 0.02643287181854248\n",
            "Iteration 5125/10000, Loss: 0.02643205225467682\n",
            "Iteration 5126/10000, Loss: 0.02643122710287571\n",
            "Iteration 5127/10000, Loss: 0.02643040381371975\n",
            "Iteration 5128/10000, Loss: 0.02642958052456379\n",
            "Iteration 5129/10000, Loss: 0.026428760960698128\n",
            "Iteration 5130/10000, Loss: 0.02642793394625187\n",
            "Iteration 5131/10000, Loss: 0.026427114382386208\n",
            "Iteration 5132/10000, Loss: 0.026426292955875397\n",
            "Iteration 5133/10000, Loss: 0.026425469666719437\n",
            "Iteration 5134/10000, Loss: 0.026424646377563477\n",
            "Iteration 5135/10000, Loss: 0.026423821225762367\n",
            "Iteration 5136/10000, Loss: 0.026423001661896706\n",
            "Iteration 5137/10000, Loss: 0.026422180235385895\n",
            "Iteration 5138/10000, Loss: 0.026421356946229935\n",
            "Iteration 5139/10000, Loss: 0.026420535519719124\n",
            "Iteration 5140/10000, Loss: 0.026419710367918015\n",
            "Iteration 5141/10000, Loss: 0.026418890804052353\n",
            "Iteration 5142/10000, Loss: 0.026418065652251244\n",
            "Iteration 5143/10000, Loss: 0.026417244225740433\n",
            "Iteration 5144/10000, Loss: 0.026416422799229622\n",
            "Iteration 5145/10000, Loss: 0.02641560323536396\n",
            "Iteration 5146/10000, Loss: 0.02641477808356285\n",
            "Iteration 5147/10000, Loss: 0.02641395479440689\n",
            "Iteration 5148/10000, Loss: 0.02641313150525093\n",
            "Iteration 5149/10000, Loss: 0.02641231194138527\n",
            "Iteration 5150/10000, Loss: 0.02641148865222931\n",
            "Iteration 5151/10000, Loss: 0.026410669088363647\n",
            "Iteration 5152/10000, Loss: 0.026409843936562538\n",
            "Iteration 5153/10000, Loss: 0.026409024372696877\n",
            "Iteration 5154/10000, Loss: 0.026408201083540916\n",
            "Iteration 5155/10000, Loss: 0.026407379657030106\n",
            "Iteration 5156/10000, Loss: 0.026406554505228996\n",
            "Iteration 5157/10000, Loss: 0.026405733078718185\n",
            "Iteration 5158/10000, Loss: 0.026404913514852524\n",
            "Iteration 5159/10000, Loss: 0.026404092088341713\n",
            "Iteration 5160/10000, Loss: 0.026403268799185753\n",
            "Iteration 5161/10000, Loss: 0.02640244923532009\n",
            "Iteration 5162/10000, Loss: 0.026401622220873833\n",
            "Iteration 5163/10000, Loss: 0.026400800794363022\n",
            "Iteration 5164/10000, Loss: 0.02639997936785221\n",
            "Iteration 5165/10000, Loss: 0.02639915980398655\n",
            "Iteration 5166/10000, Loss: 0.02639833651483059\n",
            "Iteration 5167/10000, Loss: 0.02639751322567463\n",
            "Iteration 5168/10000, Loss: 0.026396693661808968\n",
            "Iteration 5169/10000, Loss: 0.026395872235298157\n",
            "Iteration 5170/10000, Loss: 0.026395050808787346\n",
            "Iteration 5171/10000, Loss: 0.026394225656986237\n",
            "Iteration 5172/10000, Loss: 0.026393406093120575\n",
            "Iteration 5173/10000, Loss: 0.026392586529254913\n",
            "Iteration 5174/10000, Loss: 0.026391761377453804\n",
            "Iteration 5175/10000, Loss: 0.026390941813588142\n",
            "Iteration 5176/10000, Loss: 0.026390118524432182\n",
            "Iteration 5177/10000, Loss: 0.02638929709792137\n",
            "Iteration 5178/10000, Loss: 0.02638847753405571\n",
            "Iteration 5179/10000, Loss: 0.026387657970190048\n",
            "Iteration 5180/10000, Loss: 0.026386838406324387\n",
            "Iteration 5181/10000, Loss: 0.026386020705103874\n",
            "Iteration 5182/10000, Loss: 0.026385201141238213\n",
            "Iteration 5183/10000, Loss: 0.0263843797147274\n",
            "Iteration 5184/10000, Loss: 0.02638356201350689\n",
            "Iteration 5185/10000, Loss: 0.026382744312286377\n",
            "Iteration 5186/10000, Loss: 0.026381924748420715\n",
            "Iteration 5187/10000, Loss: 0.026381107047200203\n",
            "Iteration 5188/10000, Loss: 0.02638028748333454\n",
            "Iteration 5189/10000, Loss: 0.02637946791946888\n",
            "Iteration 5190/10000, Loss: 0.02637864649295807\n",
            "Iteration 5191/10000, Loss: 0.026377828791737556\n",
            "Iteration 5192/10000, Loss: 0.026377009227871895\n",
            "Iteration 5193/10000, Loss: 0.026376189664006233\n",
            "Iteration 5194/10000, Loss: 0.02637537196278572\n",
            "Iteration 5195/10000, Loss: 0.02637455426156521\n",
            "Iteration 5196/10000, Loss: 0.026373736560344696\n",
            "Iteration 5197/10000, Loss: 0.026372915133833885\n",
            "Iteration 5198/10000, Loss: 0.026372097432613373\n",
            "Iteration 5199/10000, Loss: 0.026371276006102562\n",
            "Iteration 5200/10000, Loss: 0.02637045830488205\n",
            "Iteration 5201/10000, Loss: 0.026369642466306686\n",
            "Iteration 5202/10000, Loss: 0.026368819177150726\n",
            "Iteration 5203/10000, Loss: 0.026368003338575363\n",
            "Iteration 5204/10000, Loss: 0.026367181912064552\n",
            "Iteration 5205/10000, Loss: 0.02636636234819889\n",
            "Iteration 5206/10000, Loss: 0.02636554464697838\n",
            "Iteration 5207/10000, Loss: 0.026364726945757866\n",
            "Iteration 5208/10000, Loss: 0.026363907381892204\n",
            "Iteration 5209/10000, Loss: 0.02636309154331684\n",
            "Iteration 5210/10000, Loss: 0.02636227197945118\n",
            "Iteration 5211/10000, Loss: 0.026361452415585518\n",
            "Iteration 5212/10000, Loss: 0.026360632851719856\n",
            "Iteration 5213/10000, Loss: 0.026359813287854195\n",
            "Iteration 5214/10000, Loss: 0.026358993723988533\n",
            "Iteration 5215/10000, Loss: 0.02635817974805832\n",
            "Iteration 5216/10000, Loss: 0.026357358321547508\n",
            "Iteration 5217/10000, Loss: 0.026356542482972145\n",
            "Iteration 5218/10000, Loss: 0.026355722919106483\n",
            "Iteration 5219/10000, Loss: 0.026354903355240822\n",
            "Iteration 5220/10000, Loss: 0.02635408379137516\n",
            "Iteration 5221/10000, Loss: 0.026353266090154648\n",
            "Iteration 5222/10000, Loss: 0.026352448388934135\n",
            "Iteration 5223/10000, Loss: 0.026351630687713623\n",
            "Iteration 5224/10000, Loss: 0.026350809261202812\n",
            "Iteration 5225/10000, Loss: 0.02634999342262745\n",
            "Iteration 5226/10000, Loss: 0.026349171996116638\n",
            "Iteration 5227/10000, Loss: 0.026348354294896126\n",
            "Iteration 5228/10000, Loss: 0.026347538456320763\n",
            "Iteration 5229/10000, Loss: 0.02634672075510025\n",
            "Iteration 5230/10000, Loss: 0.026345903053879738\n",
            "Iteration 5231/10000, Loss: 0.026345083490014076\n",
            "Iteration 5232/10000, Loss: 0.026344265788793564\n",
            "Iteration 5233/10000, Loss: 0.0263434499502182\n",
            "Iteration 5234/10000, Loss: 0.02634263224899769\n",
            "Iteration 5235/10000, Loss: 0.026341812685132027\n",
            "Iteration 5236/10000, Loss: 0.026340994983911514\n",
            "Iteration 5237/10000, Loss: 0.026340177282691002\n",
            "Iteration 5238/10000, Loss: 0.02633935958147049\n",
            "Iteration 5239/10000, Loss: 0.026338543742895126\n",
            "Iteration 5240/10000, Loss: 0.026337726041674614\n",
            "Iteration 5241/10000, Loss: 0.02633691020309925\n",
            "Iteration 5242/10000, Loss: 0.02633609063923359\n",
            "Iteration 5243/10000, Loss: 0.026335272938013077\n",
            "Iteration 5244/10000, Loss: 0.026334457099437714\n",
            "Iteration 5245/10000, Loss: 0.02633364126086235\n",
            "Iteration 5246/10000, Loss: 0.02633282169699669\n",
            "Iteration 5247/10000, Loss: 0.026332003995776176\n",
            "Iteration 5248/10000, Loss: 0.026331188157200813\n",
            "Iteration 5249/10000, Loss: 0.02633037231862545\n",
            "Iteration 5250/10000, Loss: 0.026329554617404938\n",
            "Iteration 5251/10000, Loss: 0.026328738778829575\n",
            "Iteration 5252/10000, Loss: 0.026327919214963913\n",
            "Iteration 5253/10000, Loss: 0.02632710337638855\n",
            "Iteration 5254/10000, Loss: 0.026326285675168037\n",
            "Iteration 5255/10000, Loss: 0.026325469836592674\n",
            "Iteration 5256/10000, Loss: 0.026324650272727013\n",
            "Iteration 5257/10000, Loss: 0.0263238362967968\n",
            "Iteration 5258/10000, Loss: 0.026323020458221436\n",
            "Iteration 5259/10000, Loss: 0.026322202757000923\n",
            "Iteration 5260/10000, Loss: 0.02632138505578041\n",
            "Iteration 5261/10000, Loss: 0.0263205673545599\n",
            "Iteration 5262/10000, Loss: 0.026319751515984535\n",
            "Iteration 5263/10000, Loss: 0.026318931952118874\n",
            "Iteration 5264/10000, Loss: 0.02631811797618866\n",
            "Iteration 5265/10000, Loss: 0.026317300274968147\n",
            "Iteration 5266/10000, Loss: 0.026316484436392784\n",
            "Iteration 5267/10000, Loss: 0.02631566673517227\n",
            "Iteration 5268/10000, Loss: 0.02631485089659691\n",
            "Iteration 5269/10000, Loss: 0.026314033195376396\n",
            "Iteration 5270/10000, Loss: 0.026313217356801033\n",
            "Iteration 5271/10000, Loss: 0.02631240338087082\n",
            "Iteration 5272/10000, Loss: 0.026311583817005157\n",
            "Iteration 5273/10000, Loss: 0.026310767978429794\n",
            "Iteration 5274/10000, Loss: 0.026309950277209282\n",
            "Iteration 5275/10000, Loss: 0.02630913443863392\n",
            "Iteration 5276/10000, Loss: 0.026308316737413406\n",
            "Iteration 5277/10000, Loss: 0.026307499036192894\n",
            "Iteration 5278/10000, Loss: 0.02630668692290783\n",
            "Iteration 5279/10000, Loss: 0.026305867359042168\n",
            "Iteration 5280/10000, Loss: 0.026305051520466805\n",
            "Iteration 5281/10000, Loss: 0.02630423754453659\n",
            "Iteration 5282/10000, Loss: 0.026303419843316078\n",
            "Iteration 5283/10000, Loss: 0.026302602142095566\n",
            "Iteration 5284/10000, Loss: 0.026301784440875053\n",
            "Iteration 5285/10000, Loss: 0.02630097232758999\n",
            "Iteration 5286/10000, Loss: 0.026300154626369476\n",
            "Iteration 5287/10000, Loss: 0.026299336925148964\n",
            "Iteration 5288/10000, Loss: 0.02629851922392845\n",
            "Iteration 5289/10000, Loss: 0.026297705247998238\n",
            "Iteration 5290/10000, Loss: 0.026296885684132576\n",
            "Iteration 5291/10000, Loss: 0.026296069845557213\n",
            "Iteration 5292/10000, Loss: 0.02629525400698185\n",
            "Iteration 5293/10000, Loss: 0.026294440031051636\n",
            "Iteration 5294/10000, Loss: 0.026293624192476273\n",
            "Iteration 5295/10000, Loss: 0.02629280462861061\n",
            "Iteration 5296/10000, Loss: 0.026291988790035248\n",
            "Iteration 5297/10000, Loss: 0.026291174814105034\n",
            "Iteration 5298/10000, Loss: 0.02629035897552967\n",
            "Iteration 5299/10000, Loss: 0.02628954127430916\n",
            "Iteration 5300/10000, Loss: 0.026288729161024094\n",
            "Iteration 5301/10000, Loss: 0.026287907734513283\n",
            "Iteration 5302/10000, Loss: 0.02628709189593792\n",
            "Iteration 5303/10000, Loss: 0.026286277920007706\n",
            "Iteration 5304/10000, Loss: 0.026285458356142044\n",
            "Iteration 5305/10000, Loss: 0.02628464438021183\n",
            "Iteration 5306/10000, Loss: 0.026283832266926765\n",
            "Iteration 5307/10000, Loss: 0.026283012703061104\n",
            "Iteration 5308/10000, Loss: 0.02628219686448574\n",
            "Iteration 5309/10000, Loss: 0.02628137916326523\n",
            "Iteration 5310/10000, Loss: 0.026280565187335014\n",
            "Iteration 5311/10000, Loss: 0.02627974934875965\n",
            "Iteration 5312/10000, Loss: 0.02627893164753914\n",
            "Iteration 5313/10000, Loss: 0.026278117671608925\n",
            "Iteration 5314/10000, Loss: 0.02627730183303356\n",
            "Iteration 5315/10000, Loss: 0.0262764859944582\n",
            "Iteration 5316/10000, Loss: 0.026275670155882835\n",
            "Iteration 5317/10000, Loss: 0.026274854317307472\n",
            "Iteration 5318/10000, Loss: 0.02627403661608696\n",
            "Iteration 5319/10000, Loss: 0.026273220777511597\n",
            "Iteration 5320/10000, Loss: 0.026272408664226532\n",
            "Iteration 5321/10000, Loss: 0.02627159096300602\n",
            "Iteration 5322/10000, Loss: 0.026270775124430656\n",
            "Iteration 5323/10000, Loss: 0.026269957423210144\n",
            "Iteration 5324/10000, Loss: 0.02626914158463478\n",
            "Iteration 5325/10000, Loss: 0.026268327608704567\n",
            "Iteration 5326/10000, Loss: 0.026267511770129204\n",
            "Iteration 5327/10000, Loss: 0.02626669593155384\n",
            "Iteration 5328/10000, Loss: 0.026265880092978477\n",
            "Iteration 5329/10000, Loss: 0.026265066117048264\n",
            "Iteration 5330/10000, Loss: 0.02626424841582775\n",
            "Iteration 5331/10000, Loss: 0.026263434439897537\n",
            "Iteration 5332/10000, Loss: 0.026262618601322174\n",
            "Iteration 5333/10000, Loss: 0.02626180276274681\n",
            "Iteration 5334/10000, Loss: 0.026260990649461746\n",
            "Iteration 5335/10000, Loss: 0.026260172948241234\n",
            "Iteration 5336/10000, Loss: 0.02625935710966587\n",
            "Iteration 5337/10000, Loss: 0.026258543133735657\n",
            "Iteration 5338/10000, Loss: 0.026257725432515144\n",
            "Iteration 5339/10000, Loss: 0.026256907731294632\n",
            "Iteration 5340/10000, Loss: 0.026256095618009567\n",
            "Iteration 5341/10000, Loss: 0.026255279779434204\n",
            "Iteration 5342/10000, Loss: 0.02625446766614914\n",
            "Iteration 5343/10000, Loss: 0.026253648102283478\n",
            "Iteration 5344/10000, Loss: 0.026252832263708115\n",
            "Iteration 5345/10000, Loss: 0.02625202015042305\n",
            "Iteration 5346/10000, Loss: 0.02625120058655739\n",
            "Iteration 5347/10000, Loss: 0.026250390335917473\n",
            "Iteration 5348/10000, Loss: 0.02624957077205181\n",
            "Iteration 5349/10000, Loss: 0.026248754933476448\n",
            "Iteration 5350/10000, Loss: 0.026247940957546234\n",
            "Iteration 5351/10000, Loss: 0.02624712698161602\n",
            "Iteration 5352/10000, Loss: 0.026246309280395508\n",
            "Iteration 5353/10000, Loss: 0.026245499029755592\n",
            "Iteration 5354/10000, Loss: 0.02624468132853508\n",
            "Iteration 5355/10000, Loss: 0.026243867352604866\n",
            "Iteration 5356/10000, Loss: 0.026243049651384354\n",
            "Iteration 5357/10000, Loss: 0.02624223567545414\n",
            "Iteration 5358/10000, Loss: 0.026241421699523926\n",
            "Iteration 5359/10000, Loss: 0.026240603998303413\n",
            "Iteration 5360/10000, Loss: 0.0262397900223732\n",
            "Iteration 5361/10000, Loss: 0.026238976046442986\n",
            "Iteration 5362/10000, Loss: 0.026238160207867622\n",
            "Iteration 5363/10000, Loss: 0.026237348094582558\n",
            "Iteration 5364/10000, Loss: 0.026236534118652344\n",
            "Iteration 5365/10000, Loss: 0.02623571828007698\n",
            "Iteration 5366/10000, Loss: 0.026234904304146767\n",
            "Iteration 5367/10000, Loss: 0.026234086602926254\n",
            "Iteration 5368/10000, Loss: 0.026233278214931488\n",
            "Iteration 5369/10000, Loss: 0.026232464239001274\n",
            "Iteration 5370/10000, Loss: 0.02623164653778076\n",
            "Iteration 5371/10000, Loss: 0.0262308306992054\n",
            "Iteration 5372/10000, Loss: 0.026230014860630035\n",
            "Iteration 5373/10000, Loss: 0.02622920088469982\n",
            "Iteration 5374/10000, Loss: 0.026228386908769608\n",
            "Iteration 5375/10000, Loss: 0.026227574795484543\n",
            "Iteration 5376/10000, Loss: 0.02622676081955433\n",
            "Iteration 5377/10000, Loss: 0.026225944980978966\n",
            "Iteration 5378/10000, Loss: 0.026225131005048752\n",
            "Iteration 5379/10000, Loss: 0.026224317029118538\n",
            "Iteration 5380/10000, Loss: 0.026223503053188324\n",
            "Iteration 5381/10000, Loss: 0.02622268535196781\n",
            "Iteration 5382/10000, Loss: 0.026221875101327896\n",
            "Iteration 5383/10000, Loss: 0.026221059262752533\n",
            "Iteration 5384/10000, Loss: 0.026220247149467468\n",
            "Iteration 5385/10000, Loss: 0.026219431310892105\n",
            "Iteration 5386/10000, Loss: 0.02621861733496189\n",
            "Iteration 5387/10000, Loss: 0.026217801496386528\n",
            "Iteration 5388/10000, Loss: 0.026216987520456314\n",
            "Iteration 5389/10000, Loss: 0.0262161735445261\n",
            "Iteration 5390/10000, Loss: 0.026215363293886185\n",
            "Iteration 5391/10000, Loss: 0.02621454931795597\n",
            "Iteration 5392/10000, Loss: 0.026213733479380608\n",
            "Iteration 5393/10000, Loss: 0.026212919503450394\n",
            "Iteration 5394/10000, Loss: 0.02621210739016533\n",
            "Iteration 5395/10000, Loss: 0.026211295276880264\n",
            "Iteration 5396/10000, Loss: 0.02621048130095005\n",
            "Iteration 5397/10000, Loss: 0.026209667325019836\n",
            "Iteration 5398/10000, Loss: 0.02620885521173477\n",
            "Iteration 5399/10000, Loss: 0.026208041235804558\n",
            "Iteration 5400/10000, Loss: 0.026207225397229195\n",
            "Iteration 5401/10000, Loss: 0.02620641514658928\n",
            "Iteration 5402/10000, Loss: 0.026205601170659065\n",
            "Iteration 5403/10000, Loss: 0.02620478719472885\n",
            "Iteration 5404/10000, Loss: 0.026203971356153488\n",
            "Iteration 5405/10000, Loss: 0.026203157380223274\n",
            "Iteration 5406/10000, Loss: 0.026202348992228508\n",
            "Iteration 5407/10000, Loss: 0.026201533153653145\n",
            "Iteration 5408/10000, Loss: 0.02620072104036808\n",
            "Iteration 5409/10000, Loss: 0.026199908927083015\n",
            "Iteration 5410/10000, Loss: 0.0261990949511528\n",
            "Iteration 5411/10000, Loss: 0.026198282837867737\n",
            "Iteration 5412/10000, Loss: 0.026197470724582672\n",
            "Iteration 5413/10000, Loss: 0.026196662336587906\n",
            "Iteration 5414/10000, Loss: 0.02619585208594799\n",
            "Iteration 5415/10000, Loss: 0.026195038110017776\n",
            "Iteration 5416/10000, Loss: 0.02619422785937786\n",
            "Iteration 5417/10000, Loss: 0.026193417608737946\n",
            "Iteration 5418/10000, Loss: 0.02619260549545288\n",
            "Iteration 5419/10000, Loss: 0.026191795244812965\n",
            "Iteration 5420/10000, Loss: 0.02619098499417305\n",
            "Iteration 5421/10000, Loss: 0.026190176606178284\n",
            "Iteration 5422/10000, Loss: 0.026189368218183517\n",
            "Iteration 5423/10000, Loss: 0.026188556104898453\n",
            "Iteration 5424/10000, Loss: 0.026187743991613388\n",
            "Iteration 5425/10000, Loss: 0.026186933740973473\n",
            "Iteration 5426/10000, Loss: 0.026186123490333557\n",
            "Iteration 5427/10000, Loss: 0.02618531323969364\n",
            "Iteration 5428/10000, Loss: 0.026184499263763428\n",
            "Iteration 5429/10000, Loss: 0.02618369273841381\n",
            "Iteration 5430/10000, Loss: 0.026182880625128746\n",
            "Iteration 5431/10000, Loss: 0.02618207037448883\n",
            "Iteration 5432/10000, Loss: 0.026181260123848915\n",
            "Iteration 5433/10000, Loss: 0.026180449873209\n",
            "Iteration 5434/10000, Loss: 0.026179639622569084\n",
            "Iteration 5435/10000, Loss: 0.02617882750928402\n",
            "Iteration 5436/10000, Loss: 0.026178019121289253\n",
            "Iteration 5437/10000, Loss: 0.026177210733294487\n",
            "Iteration 5438/10000, Loss: 0.026176396757364273\n",
            "Iteration 5439/10000, Loss: 0.026175588369369507\n",
            "Iteration 5440/10000, Loss: 0.02617477811872959\n",
            "Iteration 5441/10000, Loss: 0.026173966005444527\n",
            "Iteration 5442/10000, Loss: 0.02617315761744976\n",
            "Iteration 5443/10000, Loss: 0.026172347366809845\n",
            "Iteration 5444/10000, Loss: 0.02617153711616993\n",
            "Iteration 5445/10000, Loss: 0.026170725002884865\n",
            "Iteration 5446/10000, Loss: 0.02616991475224495\n",
            "Iteration 5447/10000, Loss: 0.026169106364250183\n",
            "Iteration 5448/10000, Loss: 0.026168296113610268\n",
            "Iteration 5449/10000, Loss: 0.0261674877256155\n",
            "Iteration 5450/10000, Loss: 0.026166677474975586\n",
            "Iteration 5451/10000, Loss: 0.02616586536169052\n",
            "Iteration 5452/10000, Loss: 0.026165056973695755\n",
            "Iteration 5453/10000, Loss: 0.02616424486041069\n",
            "Iteration 5454/10000, Loss: 0.026163432747125626\n",
            "Iteration 5455/10000, Loss: 0.02616262622177601\n",
            "Iteration 5456/10000, Loss: 0.026161815971136093\n",
            "Iteration 5457/10000, Loss: 0.026161005720496178\n",
            "Iteration 5458/10000, Loss: 0.026160195469856262\n",
            "Iteration 5459/10000, Loss: 0.026159385219216347\n",
            "Iteration 5460/10000, Loss: 0.026158573105931282\n",
            "Iteration 5461/10000, Loss: 0.026157762855291367\n",
            "Iteration 5462/10000, Loss: 0.02615695632994175\n",
            "Iteration 5463/10000, Loss: 0.026156144216656685\n",
            "Iteration 5464/10000, Loss: 0.026155337691307068\n",
            "Iteration 5465/10000, Loss: 0.026154527440667152\n",
            "Iteration 5466/10000, Loss: 0.026153715327382088\n",
            "Iteration 5467/10000, Loss: 0.02615290693938732\n",
            "Iteration 5468/10000, Loss: 0.026152094826102257\n",
            "Iteration 5469/10000, Loss: 0.02615128830075264\n",
            "Iteration 5470/10000, Loss: 0.026150476187467575\n",
            "Iteration 5471/10000, Loss: 0.02614966779947281\n",
            "Iteration 5472/10000, Loss: 0.026148857548832893\n",
            "Iteration 5473/10000, Loss: 0.026148047298192978\n",
            "Iteration 5474/10000, Loss: 0.02614723891019821\n",
            "Iteration 5475/10000, Loss: 0.026146428659558296\n",
            "Iteration 5476/10000, Loss: 0.02614561840891838\n",
            "Iteration 5477/10000, Loss: 0.026144808158278465\n",
            "Iteration 5478/10000, Loss: 0.02614400163292885\n",
            "Iteration 5479/10000, Loss: 0.026143191382288933\n",
            "Iteration 5480/10000, Loss: 0.026142382994294167\n",
            "Iteration 5481/10000, Loss: 0.02614157274365425\n",
            "Iteration 5482/10000, Loss: 0.026140766218304634\n",
            "Iteration 5483/10000, Loss: 0.02613995596766472\n",
            "Iteration 5484/10000, Loss: 0.026139145717024803\n",
            "Iteration 5485/10000, Loss: 0.026138337329030037\n",
            "Iteration 5486/10000, Loss: 0.026137525215744972\n",
            "Iteration 5487/10000, Loss: 0.026136718690395355\n",
            "Iteration 5488/10000, Loss: 0.02613590843975544\n",
            "Iteration 5489/10000, Loss: 0.026135100051760674\n",
            "Iteration 5490/10000, Loss: 0.026134295389056206\n",
            "Iteration 5491/10000, Loss: 0.02613348327577114\n",
            "Iteration 5492/10000, Loss: 0.026132673025131226\n",
            "Iteration 5493/10000, Loss: 0.02613186277449131\n",
            "Iteration 5494/10000, Loss: 0.026131056249141693\n",
            "Iteration 5495/10000, Loss: 0.026130249723792076\n",
            "Iteration 5496/10000, Loss: 0.02612943761050701\n",
            "Iteration 5497/10000, Loss: 0.026128629222512245\n",
            "Iteration 5498/10000, Loss: 0.02612782083451748\n",
            "Iteration 5499/10000, Loss: 0.026127012446522713\n",
            "Iteration 5500/10000, Loss: 0.026126204058527946\n",
            "Iteration 5501/10000, Loss: 0.02612539380788803\n",
            "Iteration 5502/10000, Loss: 0.026124585419893265\n",
            "Iteration 5503/10000, Loss: 0.026123778894543648\n",
            "Iteration 5504/10000, Loss: 0.026122968643903732\n",
            "Iteration 5505/10000, Loss: 0.026122160255908966\n",
            "Iteration 5506/10000, Loss: 0.02612135000526905\n",
            "Iteration 5507/10000, Loss: 0.026120541617274284\n",
            "Iteration 5508/10000, Loss: 0.026119735091924667\n",
            "Iteration 5509/10000, Loss: 0.026118924841284752\n",
            "Iteration 5510/10000, Loss: 0.026118116453289986\n",
            "Iteration 5511/10000, Loss: 0.02611730620265007\n",
            "Iteration 5512/10000, Loss: 0.026116499677300453\n",
            "Iteration 5513/10000, Loss: 0.026115691289305687\n",
            "Iteration 5514/10000, Loss: 0.02611488290131092\n",
            "Iteration 5515/10000, Loss: 0.026114074513316154\n",
            "Iteration 5516/10000, Loss: 0.026113266125321388\n",
            "Iteration 5517/10000, Loss: 0.026112455874681473\n",
            "Iteration 5518/10000, Loss: 0.026111647486686707\n",
            "Iteration 5519/10000, Loss: 0.02611084096133709\n",
            "Iteration 5520/10000, Loss: 0.026110030710697174\n",
            "Iteration 5521/10000, Loss: 0.026109226047992706\n",
            "Iteration 5522/10000, Loss: 0.02610841765999794\n",
            "Iteration 5523/10000, Loss: 0.026107607409358025\n",
            "Iteration 5524/10000, Loss: 0.02610679715871811\n",
            "Iteration 5525/10000, Loss: 0.026105988770723343\n",
            "Iteration 5526/10000, Loss: 0.026105182245373726\n",
            "Iteration 5527/10000, Loss: 0.02610437199473381\n",
            "Iteration 5528/10000, Loss: 0.026103567332029343\n",
            "Iteration 5529/10000, Loss: 0.026102757081389427\n",
            "Iteration 5530/10000, Loss: 0.02610194869339466\n",
            "Iteration 5531/10000, Loss: 0.026101140305399895\n",
            "Iteration 5532/10000, Loss: 0.02610033191740513\n",
            "Iteration 5533/10000, Loss: 0.026099523529410362\n",
            "Iteration 5534/10000, Loss: 0.026098715141415596\n",
            "Iteration 5535/10000, Loss: 0.02609790861606598\n",
            "Iteration 5536/10000, Loss: 0.026097100228071213\n",
            "Iteration 5537/10000, Loss: 0.026096289977431297\n",
            "Iteration 5538/10000, Loss: 0.02609548345208168\n",
            "Iteration 5539/10000, Loss: 0.026094675064086914\n",
            "Iteration 5540/10000, Loss: 0.026093868538737297\n",
            "Iteration 5541/10000, Loss: 0.02609306015074253\n",
            "Iteration 5542/10000, Loss: 0.026092251762747765\n",
            "Iteration 5543/10000, Loss: 0.02609144151210785\n",
            "Iteration 5544/10000, Loss: 0.026090633124113083\n",
            "Iteration 5545/10000, Loss: 0.026089826598763466\n",
            "Iteration 5546/10000, Loss: 0.026089021936058998\n",
            "Iteration 5547/10000, Loss: 0.026088209822773933\n",
            "Iteration 5548/10000, Loss: 0.026087405160069466\n",
            "Iteration 5549/10000, Loss: 0.02608659490942955\n",
            "Iteration 5550/10000, Loss: 0.026085788384079933\n",
            "Iteration 5551/10000, Loss: 0.026084978133440018\n",
            "Iteration 5552/10000, Loss: 0.02608417347073555\n",
            "Iteration 5553/10000, Loss: 0.026083365082740784\n",
            "Iteration 5554/10000, Loss: 0.026082556694746017\n",
            "Iteration 5555/10000, Loss: 0.02608174830675125\n",
            "Iteration 5556/10000, Loss: 0.026080943644046783\n",
            "Iteration 5557/10000, Loss: 0.026080137118697166\n",
            "Iteration 5558/10000, Loss: 0.02607932686805725\n",
            "Iteration 5559/10000, Loss: 0.026078518480062485\n",
            "Iteration 5560/10000, Loss: 0.02607771009206772\n",
            "Iteration 5561/10000, Loss: 0.02607690915465355\n",
            "Iteration 5562/10000, Loss: 0.026076098904013634\n",
            "Iteration 5563/10000, Loss: 0.026075290516018867\n",
            "Iteration 5564/10000, Loss: 0.02607448399066925\n",
            "Iteration 5565/10000, Loss: 0.026073675602674484\n",
            "Iteration 5566/10000, Loss: 0.026072869077324867\n",
            "Iteration 5567/10000, Loss: 0.02607206255197525\n",
            "Iteration 5568/10000, Loss: 0.026071254163980484\n",
            "Iteration 5569/10000, Loss: 0.026070451363921165\n",
            "Iteration 5570/10000, Loss: 0.0260696429759264\n",
            "Iteration 5571/10000, Loss: 0.026068832725286484\n",
            "Iteration 5572/10000, Loss: 0.026068026199936867\n",
            "Iteration 5573/10000, Loss: 0.0260672215372324\n",
            "Iteration 5574/10000, Loss: 0.026066415011882782\n",
            "Iteration 5575/10000, Loss: 0.026065606623888016\n",
            "Iteration 5576/10000, Loss: 0.0260648000985384\n",
            "Iteration 5577/10000, Loss: 0.026063993573188782\n",
            "Iteration 5578/10000, Loss: 0.026063185185194016\n",
            "Iteration 5579/10000, Loss: 0.0260623786598444\n",
            "Iteration 5580/10000, Loss: 0.02606157213449478\n",
            "Iteration 5581/10000, Loss: 0.026060765609145164\n",
            "Iteration 5582/10000, Loss: 0.026059960946440697\n",
            "Iteration 5583/10000, Loss: 0.02605915255844593\n",
            "Iteration 5584/10000, Loss: 0.026058344170451164\n",
            "Iteration 5585/10000, Loss: 0.026057539507746696\n",
            "Iteration 5586/10000, Loss: 0.02605673298239708\n",
            "Iteration 5587/10000, Loss: 0.026055926457047462\n",
            "Iteration 5588/10000, Loss: 0.026055119931697845\n",
            "Iteration 5589/10000, Loss: 0.02605431154370308\n",
            "Iteration 5590/10000, Loss: 0.02605350688099861\n",
            "Iteration 5591/10000, Loss: 0.026052698493003845\n",
            "Iteration 5592/10000, Loss: 0.02605189010500908\n",
            "Iteration 5593/10000, Loss: 0.02605108730494976\n",
            "Iteration 5594/10000, Loss: 0.026050280779600143\n",
            "Iteration 5595/10000, Loss: 0.026049474254250526\n",
            "Iteration 5596/10000, Loss: 0.02604866772890091\n",
            "Iteration 5597/10000, Loss: 0.02604786306619644\n",
            "Iteration 5598/10000, Loss: 0.026047056540846825\n",
            "Iteration 5599/10000, Loss: 0.02604624815285206\n",
            "Iteration 5600/10000, Loss: 0.02604544535279274\n",
            "Iteration 5601/10000, Loss: 0.026044638827443123\n",
            "Iteration 5602/10000, Loss: 0.026043832302093506\n",
            "Iteration 5603/10000, Loss: 0.026043027639389038\n",
            "Iteration 5604/10000, Loss: 0.026042217388749123\n",
            "Iteration 5605/10000, Loss: 0.026041412726044655\n",
            "Iteration 5606/10000, Loss: 0.026040609925985336\n",
            "Iteration 5607/10000, Loss: 0.02603980153799057\n",
            "Iteration 5608/10000, Loss: 0.026038995012640953\n",
            "Iteration 5609/10000, Loss: 0.026038188487291336\n",
            "Iteration 5610/10000, Loss: 0.02603738196194172\n",
            "Iteration 5611/10000, Loss: 0.0260365791618824\n",
            "Iteration 5612/10000, Loss: 0.026035772636532784\n",
            "Iteration 5613/10000, Loss: 0.026034967973828316\n",
            "Iteration 5614/10000, Loss: 0.0260341614484787\n",
            "Iteration 5615/10000, Loss: 0.02603335492312908\n",
            "Iteration 5616/10000, Loss: 0.026032548397779465\n",
            "Iteration 5617/10000, Loss: 0.026031743735074997\n",
            "Iteration 5618/10000, Loss: 0.02603093907237053\n",
            "Iteration 5619/10000, Loss: 0.026030132547020912\n",
            "Iteration 5620/10000, Loss: 0.026029326021671295\n",
            "Iteration 5621/10000, Loss: 0.026028521358966827\n",
            "Iteration 5622/10000, Loss: 0.026027711108326912\n",
            "Iteration 5623/10000, Loss: 0.026026908308267593\n",
            "Iteration 5624/10000, Loss: 0.026026103645563126\n",
            "Iteration 5625/10000, Loss: 0.026025298982858658\n",
            "Iteration 5626/10000, Loss: 0.02602449245750904\n",
            "Iteration 5627/10000, Loss: 0.026023685932159424\n",
            "Iteration 5628/10000, Loss: 0.026022881269454956\n",
            "Iteration 5629/10000, Loss: 0.02602207474410534\n",
            "Iteration 5630/10000, Loss: 0.02602127008140087\n",
            "Iteration 5631/10000, Loss: 0.026020463556051254\n",
            "Iteration 5632/10000, Loss: 0.026019660755991936\n",
            "Iteration 5633/10000, Loss: 0.02601885236799717\n",
            "Iteration 5634/10000, Loss: 0.0260180477052927\n",
            "Iteration 5635/10000, Loss: 0.026017243042588234\n",
            "Iteration 5636/10000, Loss: 0.026016434654593468\n",
            "Iteration 5637/10000, Loss: 0.02601563185453415\n",
            "Iteration 5638/10000, Loss: 0.02601482719182968\n",
            "Iteration 5639/10000, Loss: 0.026014020666480064\n",
            "Iteration 5640/10000, Loss: 0.026013217866420746\n",
            "Iteration 5641/10000, Loss: 0.02601241134107113\n",
            "Iteration 5642/10000, Loss: 0.026011602953076363\n",
            "Iteration 5643/10000, Loss: 0.026010800153017044\n",
            "Iteration 5644/10000, Loss: 0.026009993627667427\n",
            "Iteration 5645/10000, Loss: 0.02600918710231781\n",
            "Iteration 5646/10000, Loss: 0.02600838430225849\n",
            "Iteration 5647/10000, Loss: 0.026007575914263725\n",
            "Iteration 5648/10000, Loss: 0.026006773114204407\n",
            "Iteration 5649/10000, Loss: 0.02600596658885479\n",
            "Iteration 5650/10000, Loss: 0.026005161926150322\n",
            "Iteration 5651/10000, Loss: 0.026004359126091003\n",
            "Iteration 5652/10000, Loss: 0.026003552600741386\n",
            "Iteration 5653/10000, Loss: 0.02600274607539177\n",
            "Iteration 5654/10000, Loss: 0.0260019451379776\n",
            "Iteration 5655/10000, Loss: 0.026001140475273132\n",
            "Iteration 5656/10000, Loss: 0.026000337675213814\n",
            "Iteration 5657/10000, Loss: 0.025999536737799644\n",
            "Iteration 5658/10000, Loss: 0.025998735800385475\n",
            "Iteration 5659/10000, Loss: 0.025997931137681007\n",
            "Iteration 5660/10000, Loss: 0.025997132062911987\n",
            "Iteration 5661/10000, Loss: 0.02599632740020752\n",
            "Iteration 5662/10000, Loss: 0.0259955246001482\n",
            "Iteration 5663/10000, Loss: 0.025994719937443733\n",
            "Iteration 5664/10000, Loss: 0.025993920862674713\n",
            "Iteration 5665/10000, Loss: 0.025993114337325096\n",
            "Iteration 5666/10000, Loss: 0.025992311537265778\n",
            "Iteration 5667/10000, Loss: 0.02599151059985161\n",
            "Iteration 5668/10000, Loss: 0.02599070779979229\n",
            "Iteration 5669/10000, Loss: 0.02598990499973297\n",
            "Iteration 5670/10000, Loss: 0.025989104062318802\n",
            "Iteration 5671/10000, Loss: 0.025988301262259483\n",
            "Iteration 5672/10000, Loss: 0.025987498462200165\n",
            "Iteration 5673/10000, Loss: 0.025986695662140846\n",
            "Iteration 5674/10000, Loss: 0.025985892862081528\n",
            "Iteration 5675/10000, Loss: 0.02598509006202221\n",
            "Iteration 5676/10000, Loss: 0.02598429284989834\n",
            "Iteration 5677/10000, Loss: 0.02598348818719387\n",
            "Iteration 5678/10000, Loss: 0.0259826872497797\n",
            "Iteration 5679/10000, Loss: 0.025981882587075233\n",
            "Iteration 5680/10000, Loss: 0.025981081649661064\n",
            "Iteration 5681/10000, Loss: 0.025980276986956596\n",
            "Iteration 5682/10000, Loss: 0.025979474186897278\n",
            "Iteration 5683/10000, Loss: 0.025978675112128258\n",
            "Iteration 5684/10000, Loss: 0.02597787231206894\n",
            "Iteration 5685/10000, Loss: 0.02597707137465477\n",
            "Iteration 5686/10000, Loss: 0.02597626857459545\n",
            "Iteration 5687/10000, Loss: 0.025975465774536133\n",
            "Iteration 5688/10000, Loss: 0.025974661111831665\n",
            "Iteration 5689/10000, Loss: 0.025973863899707794\n",
            "Iteration 5690/10000, Loss: 0.025973061099648476\n",
            "Iteration 5691/10000, Loss: 0.025972258299589157\n",
            "Iteration 5692/10000, Loss: 0.025971457362174988\n",
            "Iteration 5693/10000, Loss: 0.02597065269947052\n",
            "Iteration 5694/10000, Loss: 0.0259698498994112\n",
            "Iteration 5695/10000, Loss: 0.025969048961997032\n",
            "Iteration 5696/10000, Loss: 0.025968249887228012\n",
            "Iteration 5697/10000, Loss: 0.025967447087168694\n",
            "Iteration 5698/10000, Loss: 0.025966642424464226\n",
            "Iteration 5699/10000, Loss: 0.025965841487050056\n",
            "Iteration 5700/10000, Loss: 0.025965038686990738\n",
            "Iteration 5701/10000, Loss: 0.02596423774957657\n",
            "Iteration 5702/10000, Loss: 0.02596343867480755\n",
            "Iteration 5703/10000, Loss: 0.02596263401210308\n",
            "Iteration 5704/10000, Loss: 0.02596183307468891\n",
            "Iteration 5705/10000, Loss: 0.025961032137274742\n",
            "Iteration 5706/10000, Loss: 0.025960229337215424\n",
            "Iteration 5707/10000, Loss: 0.025959428399801254\n",
            "Iteration 5708/10000, Loss: 0.025958627462387085\n",
            "Iteration 5709/10000, Loss: 0.025957828387618065\n",
            "Iteration 5710/10000, Loss: 0.025957023724913597\n",
            "Iteration 5711/10000, Loss: 0.02595622092485428\n",
            "Iteration 5712/10000, Loss: 0.02595541998744011\n",
            "Iteration 5713/10000, Loss: 0.02595462277531624\n",
            "Iteration 5714/10000, Loss: 0.02595381624996662\n",
            "Iteration 5715/10000, Loss: 0.0259530171751976\n",
            "Iteration 5716/10000, Loss: 0.02595221810042858\n",
            "Iteration 5717/10000, Loss: 0.025951417163014412\n",
            "Iteration 5718/10000, Loss: 0.025950614362955093\n",
            "Iteration 5719/10000, Loss: 0.025949811562895775\n",
            "Iteration 5720/10000, Loss: 0.025949012488126755\n",
            "Iteration 5721/10000, Loss: 0.025948211550712585\n",
            "Iteration 5722/10000, Loss: 0.025947410613298416\n",
            "Iteration 5723/10000, Loss: 0.025946609675884247\n",
            "Iteration 5724/10000, Loss: 0.025945808738470078\n",
            "Iteration 5725/10000, Loss: 0.02594500593841076\n",
            "Iteration 5726/10000, Loss: 0.02594420686364174\n",
            "Iteration 5727/10000, Loss: 0.02594340778887272\n",
            "Iteration 5728/10000, Loss: 0.02594260685145855\n",
            "Iteration 5729/10000, Loss: 0.02594180591404438\n",
            "Iteration 5730/10000, Loss: 0.02594100497663021\n",
            "Iteration 5731/10000, Loss: 0.025940202176570892\n",
            "Iteration 5732/10000, Loss: 0.025939403101801872\n",
            "Iteration 5733/10000, Loss: 0.025938605889678\n",
            "Iteration 5734/10000, Loss: 0.02593780681490898\n",
            "Iteration 5735/10000, Loss: 0.025937002152204514\n",
            "Iteration 5736/10000, Loss: 0.025936203077435493\n",
            "Iteration 5737/10000, Loss: 0.025935402140021324\n",
            "Iteration 5738/10000, Loss: 0.025934599339962006\n",
            "Iteration 5739/10000, Loss: 0.025933803990483284\n",
            "Iteration 5740/10000, Loss: 0.025933003053069115\n",
            "Iteration 5741/10000, Loss: 0.025932200253009796\n",
            "Iteration 5742/10000, Loss: 0.025931401178240776\n",
            "Iteration 5743/10000, Loss: 0.025930600240826607\n",
            "Iteration 5744/10000, Loss: 0.025929797440767288\n",
            "Iteration 5745/10000, Loss: 0.025929003953933716\n",
            "Iteration 5746/10000, Loss: 0.025928203016519547\n",
            "Iteration 5747/10000, Loss: 0.025927400216460228\n",
            "Iteration 5748/10000, Loss: 0.02592659927904606\n",
            "Iteration 5749/10000, Loss: 0.02592580020427704\n",
            "Iteration 5750/10000, Loss: 0.02592500112950802\n",
            "Iteration 5751/10000, Loss: 0.0259241983294487\n",
            "Iteration 5752/10000, Loss: 0.02592339925467968\n",
            "Iteration 5753/10000, Loss: 0.02592260204255581\n",
            "Iteration 5754/10000, Loss: 0.02592179924249649\n",
            "Iteration 5755/10000, Loss: 0.02592100016772747\n",
            "Iteration 5756/10000, Loss: 0.0259201992303133\n",
            "Iteration 5757/10000, Loss: 0.02591940015554428\n",
            "Iteration 5758/10000, Loss: 0.02591860108077526\n",
            "Iteration 5759/10000, Loss: 0.02591780200600624\n",
            "Iteration 5760/10000, Loss: 0.025916999205946922\n",
            "Iteration 5761/10000, Loss: 0.025916200131177902\n",
            "Iteration 5762/10000, Loss: 0.025915399193763733\n",
            "Iteration 5763/10000, Loss: 0.025914600118994713\n",
            "Iteration 5764/10000, Loss: 0.025913801044225693\n",
            "Iteration 5765/10000, Loss: 0.025913000106811523\n",
            "Iteration 5766/10000, Loss: 0.025912202894687653\n",
            "Iteration 5767/10000, Loss: 0.025911398231983185\n",
            "Iteration 5768/10000, Loss: 0.025910599157214165\n",
            "Iteration 5769/10000, Loss: 0.025909801945090294\n",
            "Iteration 5770/10000, Loss: 0.025909002870321274\n",
            "Iteration 5771/10000, Loss: 0.025908203795552254\n",
            "Iteration 5772/10000, Loss: 0.025907402858138084\n",
            "Iteration 5773/10000, Loss: 0.025906601920723915\n",
            "Iteration 5774/10000, Loss: 0.025905802845954895\n",
            "Iteration 5775/10000, Loss: 0.025905003771185875\n",
            "Iteration 5776/10000, Loss: 0.025904204696416855\n",
            "Iteration 5777/10000, Loss: 0.025903403759002686\n",
            "Iteration 5778/10000, Loss: 0.025902604684233665\n",
            "Iteration 5779/10000, Loss: 0.025901805609464645\n",
            "Iteration 5780/10000, Loss: 0.025901004672050476\n",
            "Iteration 5781/10000, Loss: 0.025900207459926605\n",
            "Iteration 5782/10000, Loss: 0.025899406522512436\n",
            "Iteration 5783/10000, Loss: 0.025898605585098267\n",
            "Iteration 5784/10000, Loss: 0.025897808372974396\n",
            "Iteration 5785/10000, Loss: 0.025897009298205376\n",
            "Iteration 5786/10000, Loss: 0.025896208360791206\n",
            "Iteration 5787/10000, Loss: 0.025895409286022186\n",
            "Iteration 5788/10000, Loss: 0.025894608348608017\n",
            "Iteration 5789/10000, Loss: 0.025893812999129295\n",
            "Iteration 5790/10000, Loss: 0.025893012061715126\n",
            "Iteration 5791/10000, Loss: 0.025892211124300957\n",
            "Iteration 5792/10000, Loss: 0.025891410186886787\n",
            "Iteration 5793/10000, Loss: 0.025890612974762917\n",
            "Iteration 5794/10000, Loss: 0.025889813899993896\n",
            "Iteration 5795/10000, Loss: 0.025889014825224876\n",
            "Iteration 5796/10000, Loss: 0.025888213887810707\n",
            "Iteration 5797/10000, Loss: 0.025887416675686836\n",
            "Iteration 5798/10000, Loss: 0.025886617600917816\n",
            "Iteration 5799/10000, Loss: 0.025885814800858498\n",
            "Iteration 5800/10000, Loss: 0.025885015726089478\n",
            "Iteration 5801/10000, Loss: 0.025884220376610756\n",
            "Iteration 5802/10000, Loss: 0.025883419439196587\n",
            "Iteration 5803/10000, Loss: 0.025882622227072716\n",
            "Iteration 5804/10000, Loss: 0.025881821289658546\n",
            "Iteration 5805/10000, Loss: 0.025881022214889526\n",
            "Iteration 5806/10000, Loss: 0.025880221277475357\n",
            "Iteration 5807/10000, Loss: 0.025879425927996635\n",
            "Iteration 5808/10000, Loss: 0.025878623127937317\n",
            "Iteration 5809/10000, Loss: 0.025877825915813446\n",
            "Iteration 5810/10000, Loss: 0.025877024978399277\n",
            "Iteration 5811/10000, Loss: 0.025876225903630257\n",
            "Iteration 5812/10000, Loss: 0.025875428691506386\n",
            "Iteration 5813/10000, Loss: 0.025874631479382515\n",
            "Iteration 5814/10000, Loss: 0.025873832404613495\n",
            "Iteration 5815/10000, Loss: 0.025873031467199326\n",
            "Iteration 5816/10000, Loss: 0.025872234255075455\n",
            "Iteration 5817/10000, Loss: 0.025871435180306435\n",
            "Iteration 5818/10000, Loss: 0.025870637968182564\n",
            "Iteration 5819/10000, Loss: 0.025869838893413544\n",
            "Iteration 5820/10000, Loss: 0.025869041681289673\n",
            "Iteration 5821/10000, Loss: 0.025868240743875504\n",
            "Iteration 5822/10000, Loss: 0.025867445394396782\n",
            "Iteration 5823/10000, Loss: 0.025866644456982613\n",
            "Iteration 5824/10000, Loss: 0.025865845382213593\n",
            "Iteration 5825/10000, Loss: 0.02586504817008972\n",
            "Iteration 5826/10000, Loss: 0.0258642490953207\n",
            "Iteration 5827/10000, Loss: 0.02586345002055168\n",
            "Iteration 5828/10000, Loss: 0.02586265467107296\n",
            "Iteration 5829/10000, Loss: 0.02586185559630394\n",
            "Iteration 5830/10000, Loss: 0.02586105838418007\n",
            "Iteration 5831/10000, Loss: 0.0258602574467659\n",
            "Iteration 5832/10000, Loss: 0.025859462097287178\n",
            "Iteration 5833/10000, Loss: 0.02585866115987301\n",
            "Iteration 5834/10000, Loss: 0.025857863947749138\n",
            "Iteration 5835/10000, Loss: 0.025857064872980118\n",
            "Iteration 5836/10000, Loss: 0.025856267660856247\n",
            "Iteration 5837/10000, Loss: 0.025855470448732376\n",
            "Iteration 5838/10000, Loss: 0.025854675099253654\n",
            "Iteration 5839/10000, Loss: 0.025853874161839485\n",
            "Iteration 5840/10000, Loss: 0.025853075087070465\n",
            "Iteration 5841/10000, Loss: 0.025852277874946594\n",
            "Iteration 5842/10000, Loss: 0.025851478800177574\n",
            "Iteration 5843/10000, Loss: 0.025850679725408554\n",
            "Iteration 5844/10000, Loss: 0.02584988623857498\n",
            "Iteration 5845/10000, Loss: 0.025849085301160812\n",
            "Iteration 5846/10000, Loss: 0.02584828808903694\n",
            "Iteration 5847/10000, Loss: 0.02584748901426792\n",
            "Iteration 5848/10000, Loss: 0.02584669180214405\n",
            "Iteration 5849/10000, Loss: 0.02584589459002018\n",
            "Iteration 5850/10000, Loss: 0.02584509551525116\n",
            "Iteration 5851/10000, Loss: 0.025844300165772438\n",
            "Iteration 5852/10000, Loss: 0.025843501091003418\n",
            "Iteration 5853/10000, Loss: 0.025842703878879547\n",
            "Iteration 5854/10000, Loss: 0.025841904804110527\n",
            "Iteration 5855/10000, Loss: 0.025841105729341507\n",
            "Iteration 5856/10000, Loss: 0.025840308517217636\n",
            "Iteration 5857/10000, Loss: 0.025839513167738914\n",
            "Iteration 5858/10000, Loss: 0.025838714092969894\n",
            "Iteration 5859/10000, Loss: 0.025837915018200874\n",
            "Iteration 5860/10000, Loss: 0.025837117806077003\n",
            "Iteration 5861/10000, Loss: 0.025836322456598282\n",
            "Iteration 5862/10000, Loss: 0.02583552524447441\n",
            "Iteration 5863/10000, Loss: 0.02583472616970539\n",
            "Iteration 5864/10000, Loss: 0.02583392895758152\n",
            "Iteration 5865/10000, Loss: 0.02583312802016735\n",
            "Iteration 5866/10000, Loss: 0.02583233080804348\n",
            "Iteration 5867/10000, Loss: 0.02583153359591961\n",
            "Iteration 5868/10000, Loss: 0.025830736383795738\n",
            "Iteration 5869/10000, Loss: 0.025829941034317017\n",
            "Iteration 5870/10000, Loss: 0.025829141959547997\n",
            "Iteration 5871/10000, Loss: 0.025828344747424126\n",
            "Iteration 5872/10000, Loss: 0.025827547535300255\n",
            "Iteration 5873/10000, Loss: 0.025826750323176384\n",
            "Iteration 5874/10000, Loss: 0.025825951248407364\n",
            "Iteration 5875/10000, Loss: 0.025825154036283493\n",
            "Iteration 5876/10000, Loss: 0.02582435868680477\n",
            "Iteration 5877/10000, Loss: 0.02582355961203575\n",
            "Iteration 5878/10000, Loss: 0.02582276239991188\n",
            "Iteration 5879/10000, Loss: 0.02582196518778801\n",
            "Iteration 5880/10000, Loss: 0.02582116611301899\n",
            "Iteration 5881/10000, Loss: 0.025820370763540268\n",
            "Iteration 5882/10000, Loss: 0.025819573551416397\n",
            "Iteration 5883/10000, Loss: 0.025818776339292526\n",
            "Iteration 5884/10000, Loss: 0.025817975401878357\n",
            "Iteration 5885/10000, Loss: 0.025817181915044785\n",
            "Iteration 5886/10000, Loss: 0.025816382840275764\n",
            "Iteration 5887/10000, Loss: 0.025815583765506744\n",
            "Iteration 5888/10000, Loss: 0.025814788416028023\n",
            "Iteration 5889/10000, Loss: 0.0258139930665493\n",
            "Iteration 5890/10000, Loss: 0.025813192129135132\n",
            "Iteration 5891/10000, Loss: 0.02581239491701126\n",
            "Iteration 5892/10000, Loss: 0.02581160143017769\n",
            "Iteration 5893/10000, Loss: 0.025810806080698967\n",
            "Iteration 5894/10000, Loss: 0.025810007005929947\n",
            "Iteration 5895/10000, Loss: 0.025809211656451225\n",
            "Iteration 5896/10000, Loss: 0.025808412581682205\n",
            "Iteration 5897/10000, Loss: 0.025807613506913185\n",
            "Iteration 5898/10000, Loss: 0.025806820020079613\n",
            "Iteration 5899/10000, Loss: 0.025806022807955742\n",
            "Iteration 5900/10000, Loss: 0.02580522745847702\n",
            "Iteration 5901/10000, Loss: 0.0258044321089983\n",
            "Iteration 5902/10000, Loss: 0.025803636759519577\n",
            "Iteration 5903/10000, Loss: 0.025802841410040855\n",
            "Iteration 5904/10000, Loss: 0.025802047923207283\n",
            "Iteration 5905/10000, Loss: 0.02580125443637371\n",
            "Iteration 5906/10000, Loss: 0.02580045908689499\n",
            "Iteration 5907/10000, Loss: 0.025799667462706566\n",
            "Iteration 5908/10000, Loss: 0.025798872113227844\n",
            "Iteration 5909/10000, Loss: 0.025798076763749123\n",
            "Iteration 5910/10000, Loss: 0.0257972814142704\n",
            "Iteration 5911/10000, Loss: 0.025796489790081978\n",
            "Iteration 5912/10000, Loss: 0.025795694440603256\n",
            "Iteration 5913/10000, Loss: 0.025794902816414833\n",
            "Iteration 5914/10000, Loss: 0.025794105604290962\n",
            "Iteration 5915/10000, Loss: 0.02579331211745739\n",
            "Iteration 5916/10000, Loss: 0.025792518630623817\n",
            "Iteration 5917/10000, Loss: 0.025791728869080544\n",
            "Iteration 5918/10000, Loss: 0.025790933519601822\n",
            "Iteration 5919/10000, Loss: 0.0257901381701231\n",
            "Iteration 5920/10000, Loss: 0.025789344683289528\n",
            "Iteration 5921/10000, Loss: 0.025788549333810806\n",
            "Iteration 5922/10000, Loss: 0.025787755846977234\n",
            "Iteration 5923/10000, Loss: 0.02578696422278881\n",
            "Iteration 5924/10000, Loss: 0.02578616887331009\n",
            "Iteration 5925/10000, Loss: 0.025785377249121666\n",
            "Iteration 5926/10000, Loss: 0.025784581899642944\n",
            "Iteration 5927/10000, Loss: 0.02578379027545452\n",
            "Iteration 5928/10000, Loss: 0.0257829949259758\n",
            "Iteration 5929/10000, Loss: 0.025782201439142227\n",
            "Iteration 5930/10000, Loss: 0.025781409814953804\n",
            "Iteration 5931/10000, Loss: 0.02578061632812023\n",
            "Iteration 5932/10000, Loss: 0.02577982097864151\n",
            "Iteration 5933/10000, Loss: 0.025779029354453087\n",
            "Iteration 5934/10000, Loss: 0.025778237730264664\n",
            "Iteration 5935/10000, Loss: 0.02577744424343109\n",
            "Iteration 5936/10000, Loss: 0.02577665075659752\n",
            "Iteration 5937/10000, Loss: 0.025775859132409096\n",
            "Iteration 5938/10000, Loss: 0.025775065645575523\n",
            "Iteration 5939/10000, Loss: 0.02577427215874195\n",
            "Iteration 5940/10000, Loss: 0.02577347867190838\n",
            "Iteration 5941/10000, Loss: 0.025772688910365105\n",
            "Iteration 5942/10000, Loss: 0.025771893560886383\n",
            "Iteration 5943/10000, Loss: 0.02577110007405281\n",
            "Iteration 5944/10000, Loss: 0.02577030658721924\n",
            "Iteration 5945/10000, Loss: 0.025769514963030815\n",
            "Iteration 5946/10000, Loss: 0.025768719613552094\n",
            "Iteration 5947/10000, Loss: 0.02576792798936367\n",
            "Iteration 5948/10000, Loss: 0.025767136365175247\n",
            "Iteration 5949/10000, Loss: 0.025766342878341675\n",
            "Iteration 5950/10000, Loss: 0.025765549391508102\n",
            "Iteration 5951/10000, Loss: 0.02576475590467453\n",
            "Iteration 5952/10000, Loss: 0.025763964280486107\n",
            "Iteration 5953/10000, Loss: 0.025763170793652534\n",
            "Iteration 5954/10000, Loss: 0.02576237916946411\n",
            "Iteration 5955/10000, Loss: 0.02576158381998539\n",
            "Iteration 5956/10000, Loss: 0.025760792195796967\n",
            "Iteration 5957/10000, Loss: 0.025760000571608543\n",
            "Iteration 5958/10000, Loss: 0.025759205222129822\n",
            "Iteration 5959/10000, Loss: 0.025758415460586548\n",
            "Iteration 5960/10000, Loss: 0.025757621973752975\n",
            "Iteration 5961/10000, Loss: 0.025756830349564552\n",
            "Iteration 5962/10000, Loss: 0.02575603500008583\n",
            "Iteration 5963/10000, Loss: 0.025755245238542557\n",
            "Iteration 5964/10000, Loss: 0.025754449889063835\n",
            "Iteration 5965/10000, Loss: 0.02575366012752056\n",
            "Iteration 5966/10000, Loss: 0.02575286664068699\n",
            "Iteration 5967/10000, Loss: 0.025752075016498566\n",
            "Iteration 5968/10000, Loss: 0.025751281529664993\n",
            "Iteration 5969/10000, Loss: 0.02575048990547657\n",
            "Iteration 5970/10000, Loss: 0.025749696418642998\n",
            "Iteration 5971/10000, Loss: 0.025748906657099724\n",
            "Iteration 5972/10000, Loss: 0.025748111307621002\n",
            "Iteration 5973/10000, Loss: 0.02574731968343258\n",
            "Iteration 5974/10000, Loss: 0.025746526196599007\n",
            "Iteration 5975/10000, Loss: 0.025745736435055733\n",
            "Iteration 5976/10000, Loss: 0.02574494108557701\n",
            "Iteration 5977/10000, Loss: 0.025744151324033737\n",
            "Iteration 5978/10000, Loss: 0.025743355974555016\n",
            "Iteration 5979/10000, Loss: 0.025742564350366592\n",
            "Iteration 5980/10000, Loss: 0.02574177086353302\n",
            "Iteration 5981/10000, Loss: 0.025740979239344597\n",
            "Iteration 5982/10000, Loss: 0.025740189477801323\n",
            "Iteration 5983/10000, Loss: 0.02573939599096775\n",
            "Iteration 5984/10000, Loss: 0.02573860064148903\n",
            "Iteration 5985/10000, Loss: 0.025737809017300606\n",
            "Iteration 5986/10000, Loss: 0.025737019255757332\n",
            "Iteration 5987/10000, Loss: 0.02573622390627861\n",
            "Iteration 5988/10000, Loss: 0.025735432282090187\n",
            "Iteration 5989/10000, Loss: 0.025734642520546913\n",
            "Iteration 5990/10000, Loss: 0.02573384903371334\n",
            "Iteration 5991/10000, Loss: 0.025733059272170067\n",
            "Iteration 5992/10000, Loss: 0.025732263922691345\n",
            "Iteration 5993/10000, Loss: 0.025731472298502922\n",
            "Iteration 5994/10000, Loss: 0.025730682536959648\n",
            "Iteration 5995/10000, Loss: 0.025729890912771225\n",
            "Iteration 5996/10000, Loss: 0.025729097425937653\n",
            "Iteration 5997/10000, Loss: 0.02572830580174923\n",
            "Iteration 5998/10000, Loss: 0.025727512314915657\n",
            "Iteration 5999/10000, Loss: 0.025726720690727234\n",
            "Iteration 6000/10000, Loss: 0.02572593092918396\n",
            "Iteration 6001/10000, Loss: 0.025725139304995537\n",
            "Iteration 6002/10000, Loss: 0.025724345818161964\n",
            "Iteration 6003/10000, Loss: 0.02572355419397354\n",
            "Iteration 6004/10000, Loss: 0.025722762569785118\n",
            "Iteration 6005/10000, Loss: 0.025721969082951546\n",
            "Iteration 6006/10000, Loss: 0.025721177458763123\n",
            "Iteration 6007/10000, Loss: 0.02572038769721985\n",
            "Iteration 6008/10000, Loss: 0.025719594210386276\n",
            "Iteration 6009/10000, Loss: 0.025718804448843002\n",
            "Iteration 6010/10000, Loss: 0.02571801096200943\n",
            "Iteration 6011/10000, Loss: 0.025717217475175858\n",
            "Iteration 6012/10000, Loss: 0.025716427713632584\n",
            "Iteration 6013/10000, Loss: 0.02571563608944416\n",
            "Iteration 6014/10000, Loss: 0.02571484073996544\n",
            "Iteration 6015/10000, Loss: 0.025714052841067314\n",
            "Iteration 6016/10000, Loss: 0.025713259354233742\n",
            "Iteration 6017/10000, Loss: 0.02571246773004532\n",
            "Iteration 6018/10000, Loss: 0.025711677968502045\n",
            "Iteration 6019/10000, Loss: 0.02571088820695877\n",
            "Iteration 6020/10000, Loss: 0.0257100947201252\n",
            "Iteration 6021/10000, Loss: 0.025709303095936775\n",
            "Iteration 6022/10000, Loss: 0.025708509609103203\n",
            "Iteration 6023/10000, Loss: 0.02570771798491478\n",
            "Iteration 6024/10000, Loss: 0.025706928223371506\n",
            "Iteration 6025/10000, Loss: 0.025706136599183083\n",
            "Iteration 6026/10000, Loss: 0.02570534311234951\n",
            "Iteration 6027/10000, Loss: 0.025704553350806236\n",
            "Iteration 6028/10000, Loss: 0.02570376545190811\n",
            "Iteration 6029/10000, Loss: 0.02570297382771969\n",
            "Iteration 6030/10000, Loss: 0.025702184066176414\n",
            "Iteration 6031/10000, Loss: 0.02570139430463314\n",
            "Iteration 6032/10000, Loss: 0.025700602680444717\n",
            "Iteration 6033/10000, Loss: 0.025699812918901443\n",
            "Iteration 6034/10000, Loss: 0.02569902502000332\n",
            "Iteration 6035/10000, Loss: 0.025698233395814896\n",
            "Iteration 6036/10000, Loss: 0.02569744363427162\n",
            "Iteration 6037/10000, Loss: 0.025696655735373497\n",
            "Iteration 6038/10000, Loss: 0.025695867836475372\n",
            "Iteration 6039/10000, Loss: 0.02569507621228695\n",
            "Iteration 6040/10000, Loss: 0.025694284588098526\n",
            "Iteration 6041/10000, Loss: 0.02569349855184555\n",
            "Iteration 6042/10000, Loss: 0.025692708790302277\n",
            "Iteration 6043/10000, Loss: 0.025691919028759003\n",
            "Iteration 6044/10000, Loss: 0.02569112926721573\n",
            "Iteration 6045/10000, Loss: 0.025690339505672455\n",
            "Iteration 6046/10000, Loss: 0.02568954788148403\n",
            "Iteration 6047/10000, Loss: 0.025688761845231056\n",
            "Iteration 6048/10000, Loss: 0.025687970221042633\n",
            "Iteration 6049/10000, Loss: 0.02568718232214451\n",
            "Iteration 6050/10000, Loss: 0.025686390697956085\n",
            "Iteration 6051/10000, Loss: 0.02568560279905796\n",
            "Iteration 6052/10000, Loss: 0.025684813037514687\n",
            "Iteration 6053/10000, Loss: 0.02568402700126171\n",
            "Iteration 6054/10000, Loss: 0.025683235377073288\n",
            "Iteration 6055/10000, Loss: 0.025682445615530014\n",
            "Iteration 6056/10000, Loss: 0.02568165399134159\n",
            "Iteration 6057/10000, Loss: 0.025680867955088615\n",
            "Iteration 6058/10000, Loss: 0.02568007819354534\n",
            "Iteration 6059/10000, Loss: 0.025679290294647217\n",
            "Iteration 6060/10000, Loss: 0.025678498670458794\n",
            "Iteration 6061/10000, Loss: 0.02567771077156067\n",
            "Iteration 6062/10000, Loss: 0.025676921010017395\n",
            "Iteration 6063/10000, Loss: 0.025676129385828972\n",
            "Iteration 6064/10000, Loss: 0.025675343349575996\n",
            "Iteration 6065/10000, Loss: 0.02567455545067787\n",
            "Iteration 6066/10000, Loss: 0.025673765689134598\n",
            "Iteration 6067/10000, Loss: 0.025672975927591324\n",
            "Iteration 6068/10000, Loss: 0.0256721880286932\n",
            "Iteration 6069/10000, Loss: 0.025671396404504776\n",
            "Iteration 6070/10000, Loss: 0.02567060850560665\n",
            "Iteration 6071/10000, Loss: 0.025669820606708527\n",
            "Iteration 6072/10000, Loss: 0.025669030845165253\n",
            "Iteration 6073/10000, Loss: 0.025668242946267128\n",
            "Iteration 6074/10000, Loss: 0.025667453184723854\n",
            "Iteration 6075/10000, Loss: 0.02566666528582573\n",
            "Iteration 6076/10000, Loss: 0.025665879249572754\n",
            "Iteration 6077/10000, Loss: 0.02566508948802948\n",
            "Iteration 6078/10000, Loss: 0.025664301589131355\n",
            "Iteration 6079/10000, Loss: 0.02566351555287838\n",
            "Iteration 6080/10000, Loss: 0.025662722066044807\n",
            "Iteration 6081/10000, Loss: 0.025661934167146683\n",
            "Iteration 6082/10000, Loss: 0.025661148130893707\n",
            "Iteration 6083/10000, Loss: 0.025660360231995583\n",
            "Iteration 6084/10000, Loss: 0.02565957047045231\n",
            "Iteration 6085/10000, Loss: 0.025658778846263885\n",
            "Iteration 6086/10000, Loss: 0.02565799653530121\n",
            "Iteration 6087/10000, Loss: 0.025657208636403084\n",
            "Iteration 6088/10000, Loss: 0.02565641887485981\n",
            "Iteration 6089/10000, Loss: 0.025655630975961685\n",
            "Iteration 6090/10000, Loss: 0.02565484121441841\n",
            "Iteration 6091/10000, Loss: 0.025654053315520287\n",
            "Iteration 6092/10000, Loss: 0.025653263553977013\n",
            "Iteration 6093/10000, Loss: 0.025652477517724037\n",
            "Iteration 6094/10000, Loss: 0.025651689618825912\n",
            "Iteration 6095/10000, Loss: 0.025650901719927788\n",
            "Iteration 6096/10000, Loss: 0.025650111958384514\n",
            "Iteration 6097/10000, Loss: 0.02564932405948639\n",
            "Iteration 6098/10000, Loss: 0.025648534297943115\n",
            "Iteration 6099/10000, Loss: 0.02564775012433529\n",
            "Iteration 6100/10000, Loss: 0.025646960362792015\n",
            "Iteration 6101/10000, Loss: 0.02564617432653904\n",
            "Iteration 6102/10000, Loss: 0.025645382702350616\n",
            "Iteration 6103/10000, Loss: 0.02564459666609764\n",
            "Iteration 6104/10000, Loss: 0.025643808767199516\n",
            "Iteration 6105/10000, Loss: 0.02564302273094654\n",
            "Iteration 6106/10000, Loss: 0.025642231106758118\n",
            "Iteration 6107/10000, Loss: 0.025641443207859993\n",
            "Iteration 6108/10000, Loss: 0.02564065530896187\n",
            "Iteration 6109/10000, Loss: 0.025639867410063744\n",
            "Iteration 6110/10000, Loss: 0.025639081373810768\n",
            "Iteration 6111/10000, Loss: 0.025638295337557793\n",
            "Iteration 6112/10000, Loss: 0.02563750557601452\n",
            "Iteration 6113/10000, Loss: 0.025636717677116394\n",
            "Iteration 6114/10000, Loss: 0.02563592791557312\n",
            "Iteration 6115/10000, Loss: 0.025635140016674995\n",
            "Iteration 6116/10000, Loss: 0.02563435398042202\n",
            "Iteration 6117/10000, Loss: 0.025633567944169044\n",
            "Iteration 6118/10000, Loss: 0.02563277818262577\n",
            "Iteration 6119/10000, Loss: 0.025631992146372795\n",
            "Iteration 6120/10000, Loss: 0.025631200522184372\n",
            "Iteration 6121/10000, Loss: 0.025630420073866844\n",
            "Iteration 6122/10000, Loss: 0.02562963031232357\n",
            "Iteration 6123/10000, Loss: 0.025628842413425446\n",
            "Iteration 6124/10000, Loss: 0.025628050789237022\n",
            "Iteration 6125/10000, Loss: 0.025627266615629196\n",
            "Iteration 6126/10000, Loss: 0.02562647871673107\n",
            "Iteration 6127/10000, Loss: 0.025625688955187798\n",
            "Iteration 6128/10000, Loss: 0.02562490478157997\n",
            "Iteration 6129/10000, Loss: 0.025624116882681847\n",
            "Iteration 6130/10000, Loss: 0.025623328983783722\n",
            "Iteration 6131/10000, Loss: 0.025622541084885597\n",
            "Iteration 6132/10000, Loss: 0.025621751323342323\n",
            "Iteration 6133/10000, Loss: 0.025620967149734497\n",
            "Iteration 6134/10000, Loss: 0.025620177388191223\n",
            "Iteration 6135/10000, Loss: 0.0256193894892931\n",
            "Iteration 6136/10000, Loss: 0.025618603453040123\n",
            "Iteration 6137/10000, Loss: 0.025617815554142\n",
            "Iteration 6138/10000, Loss: 0.025617025792598724\n",
            "Iteration 6139/10000, Loss: 0.025616241618990898\n",
            "Iteration 6140/10000, Loss: 0.025615451857447624\n",
            "Iteration 6141/10000, Loss: 0.025614667683839798\n",
            "Iteration 6142/10000, Loss: 0.025613879784941673\n",
            "Iteration 6143/10000, Loss: 0.02561309188604355\n",
            "Iteration 6144/10000, Loss: 0.025612302124500275\n",
            "Iteration 6145/10000, Loss: 0.025611519813537598\n",
            "Iteration 6146/10000, Loss: 0.025610730051994324\n",
            "Iteration 6147/10000, Loss: 0.02560994401574135\n",
            "Iteration 6148/10000, Loss: 0.025609156116843224\n",
            "Iteration 6149/10000, Loss: 0.0256083682179451\n",
            "Iteration 6150/10000, Loss: 0.025607582181692123\n",
            "Iteration 6151/10000, Loss: 0.025606798008084297\n",
            "Iteration 6152/10000, Loss: 0.025606010109186172\n",
            "Iteration 6153/10000, Loss: 0.025605222210288048\n",
            "Iteration 6154/10000, Loss: 0.02560443803668022\n",
            "Iteration 6155/10000, Loss: 0.025603653863072395\n",
            "Iteration 6156/10000, Loss: 0.02560287155210972\n",
            "Iteration 6157/10000, Loss: 0.025602085515856743\n",
            "Iteration 6158/10000, Loss: 0.025601301342248917\n",
            "Iteration 6159/10000, Loss: 0.02560051530599594\n",
            "Iteration 6160/10000, Loss: 0.025599732995033264\n",
            "Iteration 6161/10000, Loss: 0.02559894695878029\n",
            "Iteration 6162/10000, Loss: 0.02559816464781761\n",
            "Iteration 6163/10000, Loss: 0.025597380474209785\n",
            "Iteration 6164/10000, Loss: 0.02559659816324711\n",
            "Iteration 6165/10000, Loss: 0.025595812126994133\n",
            "Iteration 6166/10000, Loss: 0.025595026090741158\n",
            "Iteration 6167/10000, Loss: 0.02559424377977848\n",
            "Iteration 6168/10000, Loss: 0.025593457743525505\n",
            "Iteration 6169/10000, Loss: 0.025592677295207977\n",
            "Iteration 6170/10000, Loss: 0.025591891258955002\n",
            "Iteration 6171/10000, Loss: 0.025591107085347176\n",
            "Iteration 6172/10000, Loss: 0.02559032291173935\n",
            "Iteration 6173/10000, Loss: 0.025589538738131523\n",
            "Iteration 6174/10000, Loss: 0.025588758289813995\n",
            "Iteration 6175/10000, Loss: 0.02558797411620617\n",
            "Iteration 6176/10000, Loss: 0.025587189942598343\n",
            "Iteration 6177/10000, Loss: 0.025586403906345367\n",
            "Iteration 6178/10000, Loss: 0.02558562159538269\n",
            "Iteration 6179/10000, Loss: 0.025584835559129715\n",
            "Iteration 6180/10000, Loss: 0.02558405138552189\n",
            "Iteration 6181/10000, Loss: 0.025583267211914062\n",
            "Iteration 6182/10000, Loss: 0.025582483038306236\n",
            "Iteration 6183/10000, Loss: 0.02558170072734356\n",
            "Iteration 6184/10000, Loss: 0.025580916553735733\n",
            "Iteration 6185/10000, Loss: 0.025580132380127907\n",
            "Iteration 6186/10000, Loss: 0.02557935006916523\n",
            "Iteration 6187/10000, Loss: 0.025578565895557404\n",
            "Iteration 6188/10000, Loss: 0.025577779859304428\n",
            "Iteration 6189/10000, Loss: 0.025576995685696602\n",
            "Iteration 6190/10000, Loss: 0.025576217100024223\n",
            "Iteration 6191/10000, Loss: 0.025575432926416397\n",
            "Iteration 6192/10000, Loss: 0.02557464689016342\n",
            "Iteration 6193/10000, Loss: 0.025573864579200745\n",
            "Iteration 6194/10000, Loss: 0.02557308040559292\n",
            "Iteration 6195/10000, Loss: 0.025572294369339943\n",
            "Iteration 6196/10000, Loss: 0.025571513921022415\n",
            "Iteration 6197/10000, Loss: 0.02557072974741459\n",
            "Iteration 6198/10000, Loss: 0.025569945573806763\n",
            "Iteration 6199/10000, Loss: 0.025569161400198936\n",
            "Iteration 6200/10000, Loss: 0.02556837536394596\n",
            "Iteration 6201/10000, Loss: 0.025567596778273582\n",
            "Iteration 6202/10000, Loss: 0.025566812604665756\n",
            "Iteration 6203/10000, Loss: 0.02556602843105793\n",
            "Iteration 6204/10000, Loss: 0.025565244257450104\n",
            "Iteration 6205/10000, Loss: 0.025564461946487427\n",
            "Iteration 6206/10000, Loss: 0.0255636777728796\n",
            "Iteration 6207/10000, Loss: 0.025562891736626625\n",
            "Iteration 6208/10000, Loss: 0.025562113150954247\n",
            "Iteration 6209/10000, Loss: 0.02556133083999157\n",
            "Iteration 6210/10000, Loss: 0.025560546666383743\n",
            "Iteration 6211/10000, Loss: 0.02555975876748562\n",
            "Iteration 6212/10000, Loss: 0.02555897831916809\n",
            "Iteration 6213/10000, Loss: 0.025558196008205414\n",
            "Iteration 6214/10000, Loss: 0.025557415559887886\n",
            "Iteration 6215/10000, Loss: 0.02555662766098976\n",
            "Iteration 6216/10000, Loss: 0.025555845350027084\n",
            "Iteration 6217/10000, Loss: 0.025555063039064407\n",
            "Iteration 6218/10000, Loss: 0.025554277002811432\n",
            "Iteration 6219/10000, Loss: 0.025553494691848755\n",
            "Iteration 6220/10000, Loss: 0.025552712380886078\n",
            "Iteration 6221/10000, Loss: 0.0255519300699234\n",
            "Iteration 6222/10000, Loss: 0.025551147758960724\n",
            "Iteration 6223/10000, Loss: 0.02555036172270775\n",
            "Iteration 6224/10000, Loss: 0.02554958127439022\n",
            "Iteration 6225/10000, Loss: 0.025548797100782394\n",
            "Iteration 6226/10000, Loss: 0.025548012927174568\n",
            "Iteration 6227/10000, Loss: 0.02554723061621189\n",
            "Iteration 6228/10000, Loss: 0.025546448305249214\n",
            "Iteration 6229/10000, Loss: 0.025545667856931686\n",
            "Iteration 6230/10000, Loss: 0.02554488182067871\n",
            "Iteration 6231/10000, Loss: 0.025544097647070885\n",
            "Iteration 6232/10000, Loss: 0.025543315336108208\n",
            "Iteration 6233/10000, Loss: 0.02554253302514553\n",
            "Iteration 6234/10000, Loss: 0.025541746988892555\n",
            "Iteration 6235/10000, Loss: 0.025540970265865326\n",
            "Iteration 6236/10000, Loss: 0.0255401860922575\n",
            "Iteration 6237/10000, Loss: 0.025539403781294823\n",
            "Iteration 6238/10000, Loss: 0.025538617745041847\n",
            "Iteration 6239/10000, Loss: 0.02553783357143402\n",
            "Iteration 6240/10000, Loss: 0.025537053123116493\n",
            "Iteration 6241/10000, Loss: 0.025536274537444115\n",
            "Iteration 6242/10000, Loss: 0.02553548850119114\n",
            "Iteration 6243/10000, Loss: 0.025534706190228462\n",
            "Iteration 6244/10000, Loss: 0.025533922016620636\n",
            "Iteration 6245/10000, Loss: 0.02553313970565796\n",
            "Iteration 6246/10000, Loss: 0.02553236111998558\n",
            "Iteration 6247/10000, Loss: 0.025531578809022903\n",
            "Iteration 6248/10000, Loss: 0.025530792772769928\n",
            "Iteration 6249/10000, Loss: 0.0255300123244524\n",
            "Iteration 6250/10000, Loss: 0.025529228150844574\n",
            "Iteration 6251/10000, Loss: 0.025528447702527046\n",
            "Iteration 6252/10000, Loss: 0.02552766725420952\n",
            "Iteration 6253/10000, Loss: 0.025526883080601692\n",
            "Iteration 6254/10000, Loss: 0.025526098906993866\n",
            "Iteration 6255/10000, Loss: 0.025525318458676338\n",
            "Iteration 6256/10000, Loss: 0.02552453614771366\n",
            "Iteration 6257/10000, Loss: 0.025523753836750984\n",
            "Iteration 6258/10000, Loss: 0.025522975251078606\n",
            "Iteration 6259/10000, Loss: 0.02552219107747078\n",
            "Iteration 6260/10000, Loss: 0.02552141062915325\n",
            "Iteration 6261/10000, Loss: 0.025520628318190575\n",
            "Iteration 6262/10000, Loss: 0.02551984414458275\n",
            "Iteration 6263/10000, Loss: 0.02551906555891037\n",
            "Iteration 6264/10000, Loss: 0.025518285110592842\n",
            "Iteration 6265/10000, Loss: 0.025517502799630165\n",
            "Iteration 6266/10000, Loss: 0.025516722351312637\n",
            "Iteration 6267/10000, Loss: 0.02551593817770481\n",
            "Iteration 6268/10000, Loss: 0.025515157729387283\n",
            "Iteration 6269/10000, Loss: 0.025514377281069756\n",
            "Iteration 6270/10000, Loss: 0.02551359497010708\n",
            "Iteration 6271/10000, Loss: 0.0255128126591444\n",
            "Iteration 6272/10000, Loss: 0.025512030348181725\n",
            "Iteration 6273/10000, Loss: 0.025511251762509346\n",
            "Iteration 6274/10000, Loss: 0.02551046945154667\n",
            "Iteration 6275/10000, Loss: 0.02550968900322914\n",
            "Iteration 6276/10000, Loss: 0.025508908554911613\n",
            "Iteration 6277/10000, Loss: 0.025508126243948936\n",
            "Iteration 6278/10000, Loss: 0.02550734579563141\n",
            "Iteration 6279/10000, Loss: 0.02550656348466873\n",
            "Iteration 6280/10000, Loss: 0.025505781173706055\n",
            "Iteration 6281/10000, Loss: 0.025505002588033676\n",
            "Iteration 6282/10000, Loss: 0.025504220277071\n",
            "Iteration 6283/10000, Loss: 0.025503437966108322\n",
            "Iteration 6284/10000, Loss: 0.025502659380435944\n",
            "Iteration 6285/10000, Loss: 0.025501878932118416\n",
            "Iteration 6286/10000, Loss: 0.02550109662115574\n",
            "Iteration 6287/10000, Loss: 0.025500314310193062\n",
            "Iteration 6288/10000, Loss: 0.025499533861875534\n",
            "Iteration 6289/10000, Loss: 0.025498753413558006\n",
            "Iteration 6290/10000, Loss: 0.02549797110259533\n",
            "Iteration 6291/10000, Loss: 0.02549719251692295\n",
            "Iteration 6292/10000, Loss: 0.025496410205960274\n",
            "Iteration 6293/10000, Loss: 0.025495629757642746\n",
            "Iteration 6294/10000, Loss: 0.025494849309325218\n",
            "Iteration 6295/10000, Loss: 0.02549406886100769\n",
            "Iteration 6296/10000, Loss: 0.025493286550045013\n",
            "Iteration 6297/10000, Loss: 0.025492506101727486\n",
            "Iteration 6298/10000, Loss: 0.025491725653409958\n",
            "Iteration 6299/10000, Loss: 0.02549094147980213\n",
            "Iteration 6300/10000, Loss: 0.025490162894129753\n",
            "Iteration 6301/10000, Loss: 0.025489384308457375\n",
            "Iteration 6302/10000, Loss: 0.025488601997494698\n",
            "Iteration 6303/10000, Loss: 0.02548782154917717\n",
            "Iteration 6304/10000, Loss: 0.025487037375569344\n",
            "Iteration 6305/10000, Loss: 0.025486260652542114\n",
            "Iteration 6306/10000, Loss: 0.025485478341579437\n",
            "Iteration 6307/10000, Loss: 0.02548469789326191\n",
            "Iteration 6308/10000, Loss: 0.02548391930758953\n",
            "Iteration 6309/10000, Loss: 0.025483136996626854\n",
            "Iteration 6310/10000, Loss: 0.025482356548309326\n",
            "Iteration 6311/10000, Loss: 0.0254815723747015\n",
            "Iteration 6312/10000, Loss: 0.02548079378902912\n",
            "Iteration 6313/10000, Loss: 0.025480011478066444\n",
            "Iteration 6314/10000, Loss: 0.025479232892394066\n",
            "Iteration 6315/10000, Loss: 0.025478452444076538\n",
            "Iteration 6316/10000, Loss: 0.02547767199575901\n",
            "Iteration 6317/10000, Loss: 0.025476891547441483\n",
            "Iteration 6318/10000, Loss: 0.025476111099123955\n",
            "Iteration 6319/10000, Loss: 0.025475332513451576\n",
            "Iteration 6320/10000, Loss: 0.02547454833984375\n",
            "Iteration 6321/10000, Loss: 0.02547376975417137\n",
            "Iteration 6322/10000, Loss: 0.025472987443208694\n",
            "Iteration 6323/10000, Loss: 0.025472208857536316\n",
            "Iteration 6324/10000, Loss: 0.02547142654657364\n",
            "Iteration 6325/10000, Loss: 0.02547064982354641\n",
            "Iteration 6326/10000, Loss: 0.025469867512583733\n",
            "Iteration 6327/10000, Loss: 0.025469088926911354\n",
            "Iteration 6328/10000, Loss: 0.025468304753303528\n",
            "Iteration 6329/10000, Loss: 0.0254675280302763\n",
            "Iteration 6330/10000, Loss: 0.02546674758195877\n",
            "Iteration 6331/10000, Loss: 0.025465967133641243\n",
            "Iteration 6332/10000, Loss: 0.025465186685323715\n",
            "Iteration 6333/10000, Loss: 0.025464406237006187\n",
            "Iteration 6334/10000, Loss: 0.02546362578868866\n",
            "Iteration 6335/10000, Loss: 0.025462845340371132\n",
            "Iteration 6336/10000, Loss: 0.025462063029408455\n",
            "Iteration 6337/10000, Loss: 0.025461284443736076\n",
            "Iteration 6338/10000, Loss: 0.025460505858063698\n",
            "Iteration 6339/10000, Loss: 0.02545972540974617\n",
            "Iteration 6340/10000, Loss: 0.02545894682407379\n",
            "Iteration 6341/10000, Loss: 0.025458166375756264\n",
            "Iteration 6342/10000, Loss: 0.025457384064793587\n",
            "Iteration 6343/10000, Loss: 0.025456605479121208\n",
            "Iteration 6344/10000, Loss: 0.02545582316815853\n",
            "Iteration 6345/10000, Loss: 0.025455046445131302\n",
            "Iteration 6346/10000, Loss: 0.025454265996813774\n",
            "Iteration 6347/10000, Loss: 0.025453485548496246\n",
            "Iteration 6348/10000, Loss: 0.02545270510017872\n",
            "Iteration 6349/10000, Loss: 0.02545192278921604\n",
            "Iteration 6350/10000, Loss: 0.025451140478253365\n",
            "Iteration 6351/10000, Loss: 0.025450363755226135\n",
            "Iteration 6352/10000, Loss: 0.025449585169553757\n",
            "Iteration 6353/10000, Loss: 0.02544880472123623\n",
            "Iteration 6354/10000, Loss: 0.0254480242729187\n",
            "Iteration 6355/10000, Loss: 0.025447243824601173\n",
            "Iteration 6356/10000, Loss: 0.025446467101573944\n",
            "Iteration 6357/10000, Loss: 0.025445686653256416\n",
            "Iteration 6358/10000, Loss: 0.02544490620493889\n",
            "Iteration 6359/10000, Loss: 0.02544412575662136\n",
            "Iteration 6360/10000, Loss: 0.025443347170948982\n",
            "Iteration 6361/10000, Loss: 0.025442566722631454\n",
            "Iteration 6362/10000, Loss: 0.025441788136959076\n",
            "Iteration 6363/10000, Loss: 0.025441007688641548\n",
            "Iteration 6364/10000, Loss: 0.02544023096561432\n",
            "Iteration 6365/10000, Loss: 0.025439446792006493\n",
            "Iteration 6366/10000, Loss: 0.025438666343688965\n",
            "Iteration 6367/10000, Loss: 0.025437889620661736\n",
            "Iteration 6368/10000, Loss: 0.025437109172344208\n",
            "Iteration 6369/10000, Loss: 0.02543633058667183\n",
            "Iteration 6370/10000, Loss: 0.0254355501383543\n",
            "Iteration 6371/10000, Loss: 0.025434769690036774\n",
            "Iteration 6372/10000, Loss: 0.025433991104364395\n",
            "Iteration 6373/10000, Loss: 0.025433214381337166\n",
            "Iteration 6374/10000, Loss: 0.025432433933019638\n",
            "Iteration 6375/10000, Loss: 0.02543165534734726\n",
            "Iteration 6376/10000, Loss: 0.025430873036384583\n",
            "Iteration 6377/10000, Loss: 0.025430094450712204\n",
            "Iteration 6378/10000, Loss: 0.025429317727684975\n",
            "Iteration 6379/10000, Loss: 0.025428537279367447\n",
            "Iteration 6380/10000, Loss: 0.02542775869369507\n",
            "Iteration 6381/10000, Loss: 0.02542697824537754\n",
            "Iteration 6382/10000, Loss: 0.025426199659705162\n",
            "Iteration 6383/10000, Loss: 0.025425421074032784\n",
            "Iteration 6384/10000, Loss: 0.025424638763070107\n",
            "Iteration 6385/10000, Loss: 0.025423865765333176\n",
            "Iteration 6386/10000, Loss: 0.0254230797290802\n",
            "Iteration 6387/10000, Loss: 0.02542230300605297\n",
            "Iteration 6388/10000, Loss: 0.025421524420380592\n",
            "Iteration 6389/10000, Loss: 0.025420747697353363\n",
            "Iteration 6390/10000, Loss: 0.025419967249035835\n",
            "Iteration 6391/10000, Loss: 0.025419186800718307\n",
            "Iteration 6392/10000, Loss: 0.02541840635240078\n",
            "Iteration 6393/10000, Loss: 0.02541762962937355\n",
            "Iteration 6394/10000, Loss: 0.025416849181056023\n",
            "Iteration 6395/10000, Loss: 0.025416070595383644\n",
            "Iteration 6396/10000, Loss: 0.025415292009711266\n",
            "Iteration 6397/10000, Loss: 0.025414513424038887\n",
            "Iteration 6398/10000, Loss: 0.02541373483836651\n",
            "Iteration 6399/10000, Loss: 0.02541295439004898\n",
            "Iteration 6400/10000, Loss: 0.02541217766702175\n",
            "Iteration 6401/10000, Loss: 0.025411400943994522\n",
            "Iteration 6402/10000, Loss: 0.025410616770386696\n",
            "Iteration 6403/10000, Loss: 0.025409840047359467\n",
            "Iteration 6404/10000, Loss: 0.02540905959904194\n",
            "Iteration 6405/10000, Loss: 0.02540828473865986\n",
            "Iteration 6406/10000, Loss: 0.02540750429034233\n",
            "Iteration 6407/10000, Loss: 0.0254067275673151\n",
            "Iteration 6408/10000, Loss: 0.02540595270693302\n",
            "Iteration 6409/10000, Loss: 0.025405175983905792\n",
            "Iteration 6410/10000, Loss: 0.025404399260878563\n",
            "Iteration 6411/10000, Loss: 0.025403624400496483\n",
            "Iteration 6412/10000, Loss: 0.025402847677469254\n",
            "Iteration 6413/10000, Loss: 0.025402070954442024\n",
            "Iteration 6414/10000, Loss: 0.025401296094059944\n",
            "Iteration 6415/10000, Loss: 0.025400517508387566\n",
            "Iteration 6416/10000, Loss: 0.025399740785360336\n",
            "Iteration 6417/10000, Loss: 0.025398967787623405\n",
            "Iteration 6418/10000, Loss: 0.025398191064596176\n",
            "Iteration 6419/10000, Loss: 0.025397416204214096\n",
            "Iteration 6420/10000, Loss: 0.025396637618541718\n",
            "Iteration 6421/10000, Loss: 0.025395862758159637\n",
            "Iteration 6422/10000, Loss: 0.025395089760422707\n",
            "Iteration 6423/10000, Loss: 0.025394311174750328\n",
            "Iteration 6424/10000, Loss: 0.025393536314368248\n",
            "Iteration 6425/10000, Loss: 0.025392763316631317\n",
            "Iteration 6426/10000, Loss: 0.025391986593604088\n",
            "Iteration 6427/10000, Loss: 0.025391213595867157\n",
            "Iteration 6428/10000, Loss: 0.02539043501019478\n",
            "Iteration 6429/10000, Loss: 0.0253896601498127\n",
            "Iteration 6430/10000, Loss: 0.025388887152075768\n",
            "Iteration 6431/10000, Loss: 0.025388110429048538\n",
            "Iteration 6432/10000, Loss: 0.025387335568666458\n",
            "Iteration 6433/10000, Loss: 0.025386560708284378\n",
            "Iteration 6434/10000, Loss: 0.025385785847902298\n",
            "Iteration 6435/10000, Loss: 0.02538500912487507\n",
            "Iteration 6436/10000, Loss: 0.02538423240184784\n",
            "Iteration 6437/10000, Loss: 0.02538345754146576\n",
            "Iteration 6438/10000, Loss: 0.02538268268108368\n",
            "Iteration 6439/10000, Loss: 0.0253819078207016\n",
            "Iteration 6440/10000, Loss: 0.02538113296031952\n",
            "Iteration 6441/10000, Loss: 0.02538035623729229\n",
            "Iteration 6442/10000, Loss: 0.02537958323955536\n",
            "Iteration 6443/10000, Loss: 0.025378810241818428\n",
            "Iteration 6444/10000, Loss: 0.0253780297935009\n",
            "Iteration 6445/10000, Loss: 0.02537725865840912\n",
            "Iteration 6446/10000, Loss: 0.02537648193538189\n",
            "Iteration 6447/10000, Loss: 0.02537570893764496\n",
            "Iteration 6448/10000, Loss: 0.02537493035197258\n",
            "Iteration 6449/10000, Loss: 0.02537415735423565\n",
            "Iteration 6450/10000, Loss: 0.02537338249385357\n",
            "Iteration 6451/10000, Loss: 0.02537260763347149\n",
            "Iteration 6452/10000, Loss: 0.02537183091044426\n",
            "Iteration 6453/10000, Loss: 0.02537105605006218\n",
            "Iteration 6454/10000, Loss: 0.02537027932703495\n",
            "Iteration 6455/10000, Loss: 0.02536950632929802\n",
            "Iteration 6456/10000, Loss: 0.02536873333156109\n",
            "Iteration 6457/10000, Loss: 0.02536795847117901\n",
            "Iteration 6458/10000, Loss: 0.02536718361079693\n",
            "Iteration 6459/10000, Loss: 0.025366410613059998\n",
            "Iteration 6460/10000, Loss: 0.025365635752677917\n",
            "Iteration 6461/10000, Loss: 0.025364859029650688\n",
            "Iteration 6462/10000, Loss: 0.02536408230662346\n",
            "Iteration 6463/10000, Loss: 0.02536330744624138\n",
            "Iteration 6464/10000, Loss: 0.025362534448504448\n",
            "Iteration 6465/10000, Loss: 0.025361761450767517\n",
            "Iteration 6466/10000, Loss: 0.025360988453030586\n",
            "Iteration 6467/10000, Loss: 0.025360211730003357\n",
            "Iteration 6468/10000, Loss: 0.025359435006976128\n",
            "Iteration 6469/10000, Loss: 0.025358662009239197\n",
            "Iteration 6470/10000, Loss: 0.025357889011502266\n",
            "Iteration 6471/10000, Loss: 0.025357114151120186\n",
            "Iteration 6472/10000, Loss: 0.025356339290738106\n",
            "Iteration 6473/10000, Loss: 0.025355564430356026\n",
            "Iteration 6474/10000, Loss: 0.025354789569973946\n",
            "Iteration 6475/10000, Loss: 0.025354018434882164\n",
            "Iteration 6476/10000, Loss: 0.025353241711854935\n",
            "Iteration 6477/10000, Loss: 0.025352466851472855\n",
            "Iteration 6478/10000, Loss: 0.025351693853735924\n",
            "Iteration 6479/10000, Loss: 0.025350918993353844\n",
            "Iteration 6480/10000, Loss: 0.025350145995616913\n",
            "Iteration 6481/10000, Loss: 0.025349369272589684\n",
            "Iteration 6482/10000, Loss: 0.025348598137497902\n",
            "Iteration 6483/10000, Loss: 0.025347821414470673\n",
            "Iteration 6484/10000, Loss: 0.025347046554088593\n",
            "Iteration 6485/10000, Loss: 0.02534627355635166\n",
            "Iteration 6486/10000, Loss: 0.02534550055861473\n",
            "Iteration 6487/10000, Loss: 0.02534472569823265\n",
            "Iteration 6488/10000, Loss: 0.02534395270049572\n",
            "Iteration 6489/10000, Loss: 0.02534317411482334\n",
            "Iteration 6490/10000, Loss: 0.02534240297973156\n",
            "Iteration 6491/10000, Loss: 0.02534162998199463\n",
            "Iteration 6492/10000, Loss: 0.02534085512161255\n",
            "Iteration 6493/10000, Loss: 0.025340082123875618\n",
            "Iteration 6494/10000, Loss: 0.02533930540084839\n",
            "Iteration 6495/10000, Loss: 0.025338534265756607\n",
            "Iteration 6496/10000, Loss: 0.025337761268019676\n",
            "Iteration 6497/10000, Loss: 0.025336984544992447\n",
            "Iteration 6498/10000, Loss: 0.025336209684610367\n",
            "Iteration 6499/10000, Loss: 0.025335434824228287\n",
            "Iteration 6500/10000, Loss: 0.025334661826491356\n",
            "Iteration 6501/10000, Loss: 0.025333890691399574\n",
            "Iteration 6502/10000, Loss: 0.025333113968372345\n",
            "Iteration 6503/10000, Loss: 0.025332342833280563\n",
            "Iteration 6504/10000, Loss: 0.025331564247608185\n",
            "Iteration 6505/10000, Loss: 0.025330791249871254\n",
            "Iteration 6506/10000, Loss: 0.02533002197742462\n",
            "Iteration 6507/10000, Loss: 0.025329245254397392\n",
            "Iteration 6508/10000, Loss: 0.025328470394015312\n",
            "Iteration 6509/10000, Loss: 0.02532769925892353\n",
            "Iteration 6510/10000, Loss: 0.02532692439854145\n",
            "Iteration 6511/10000, Loss: 0.02532615140080452\n",
            "Iteration 6512/10000, Loss: 0.02532537840306759\n",
            "Iteration 6513/10000, Loss: 0.02532460354268551\n",
            "Iteration 6514/10000, Loss: 0.025323830544948578\n",
            "Iteration 6515/10000, Loss: 0.025323057547211647\n",
            "Iteration 6516/10000, Loss: 0.025322284549474716\n",
            "Iteration 6517/10000, Loss: 0.025321511551737785\n",
            "Iteration 6518/10000, Loss: 0.025320736691355705\n",
            "Iteration 6519/10000, Loss: 0.025319963693618774\n",
            "Iteration 6520/10000, Loss: 0.025319190695881844\n",
            "Iteration 6521/10000, Loss: 0.025318415835499763\n",
            "Iteration 6522/10000, Loss: 0.025317640975117683\n",
            "Iteration 6523/10000, Loss: 0.02531687170267105\n",
            "Iteration 6524/10000, Loss: 0.02531609684228897\n",
            "Iteration 6525/10000, Loss: 0.02531532384455204\n",
            "Iteration 6526/10000, Loss: 0.02531455084681511\n",
            "Iteration 6527/10000, Loss: 0.02531377412378788\n",
            "Iteration 6528/10000, Loss: 0.025313004851341248\n",
            "Iteration 6529/10000, Loss: 0.025312229990959167\n",
            "Iteration 6530/10000, Loss: 0.025311456993222237\n",
            "Iteration 6531/10000, Loss: 0.025310683995485306\n",
            "Iteration 6532/10000, Loss: 0.025309909135103226\n",
            "Iteration 6533/10000, Loss: 0.025309138000011444\n",
            "Iteration 6534/10000, Loss: 0.025308365002274513\n",
            "Iteration 6535/10000, Loss: 0.025307590141892433\n",
            "Iteration 6536/10000, Loss: 0.025306817144155502\n",
            "Iteration 6537/10000, Loss: 0.02530604414641857\n",
            "Iteration 6538/10000, Loss: 0.02530527301132679\n",
            "Iteration 6539/10000, Loss: 0.02530450001358986\n",
            "Iteration 6540/10000, Loss: 0.025303727015852928\n",
            "Iteration 6541/10000, Loss: 0.025302954018115997\n",
            "Iteration 6542/10000, Loss: 0.025302181020379066\n",
            "Iteration 6543/10000, Loss: 0.025301406159996986\n",
            "Iteration 6544/10000, Loss: 0.025300635024905205\n",
            "Iteration 6545/10000, Loss: 0.025299862027168274\n",
            "Iteration 6546/10000, Loss: 0.025299087166786194\n",
            "Iteration 6547/10000, Loss: 0.025298314169049263\n",
            "Iteration 6548/10000, Loss: 0.025297541171312332\n",
            "Iteration 6549/10000, Loss: 0.02529677003622055\n",
            "Iteration 6550/10000, Loss: 0.02529599703848362\n",
            "Iteration 6551/10000, Loss: 0.02529522404074669\n",
            "Iteration 6552/10000, Loss: 0.02529444918036461\n",
            "Iteration 6553/10000, Loss: 0.02529367431998253\n",
            "Iteration 6554/10000, Loss: 0.025292908772826195\n",
            "Iteration 6555/10000, Loss: 0.025292133912444115\n",
            "Iteration 6556/10000, Loss: 0.025291359052062035\n",
            "Iteration 6557/10000, Loss: 0.025290584191679955\n",
            "Iteration 6558/10000, Loss: 0.025289813056588173\n",
            "Iteration 6559/10000, Loss: 0.02528904192149639\n",
            "Iteration 6560/10000, Loss: 0.02528827078640461\n",
            "Iteration 6561/10000, Loss: 0.02528749778866768\n",
            "Iteration 6562/10000, Loss: 0.025286724790930748\n",
            "Iteration 6563/10000, Loss: 0.025285951793193817\n",
            "Iteration 6564/10000, Loss: 0.025285178795456886\n",
            "Iteration 6565/10000, Loss: 0.025284405797719955\n",
            "Iteration 6566/10000, Loss: 0.025283634662628174\n",
            "Iteration 6567/10000, Loss: 0.025282859802246094\n",
            "Iteration 6568/10000, Loss: 0.025282088667154312\n",
            "Iteration 6569/10000, Loss: 0.02528131753206253\n",
            "Iteration 6570/10000, Loss: 0.0252805445343256\n",
            "Iteration 6571/10000, Loss: 0.02527977153658867\n",
            "Iteration 6572/10000, Loss: 0.025278998538851738\n",
            "Iteration 6573/10000, Loss: 0.025278225541114807\n",
            "Iteration 6574/10000, Loss: 0.025277456268668175\n",
            "Iteration 6575/10000, Loss: 0.025276683270931244\n",
            "Iteration 6576/10000, Loss: 0.025275910273194313\n",
            "Iteration 6577/10000, Loss: 0.02527513913810253\n",
            "Iteration 6578/10000, Loss: 0.0252743661403656\n",
            "Iteration 6579/10000, Loss: 0.02527359500527382\n",
            "Iteration 6580/10000, Loss: 0.02527282014489174\n",
            "Iteration 6581/10000, Loss: 0.025272049009799957\n",
            "Iteration 6582/10000, Loss: 0.025271277874708176\n",
            "Iteration 6583/10000, Loss: 0.025270506739616394\n",
            "Iteration 6584/10000, Loss: 0.025269731879234314\n",
            "Iteration 6585/10000, Loss: 0.025268960744142532\n",
            "Iteration 6586/10000, Loss: 0.02526818960905075\n",
            "Iteration 6587/10000, Loss: 0.02526741847395897\n",
            "Iteration 6588/10000, Loss: 0.02526664547622204\n",
            "Iteration 6589/10000, Loss: 0.025265870615839958\n",
            "Iteration 6590/10000, Loss: 0.025265101343393326\n",
            "Iteration 6591/10000, Loss: 0.025264330208301544\n",
            "Iteration 6592/10000, Loss: 0.025263560935854912\n",
            "Iteration 6593/10000, Loss: 0.02526278607547283\n",
            "Iteration 6594/10000, Loss: 0.02526201494038105\n",
            "Iteration 6595/10000, Loss: 0.02526124194264412\n",
            "Iteration 6596/10000, Loss: 0.02526046894490719\n",
            "Iteration 6597/10000, Loss: 0.025259699672460556\n",
            "Iteration 6598/10000, Loss: 0.025258926674723625\n",
            "Iteration 6599/10000, Loss: 0.025258155539631844\n",
            "Iteration 6600/10000, Loss: 0.025257382541894913\n",
            "Iteration 6601/10000, Loss: 0.025256609544157982\n",
            "Iteration 6602/10000, Loss: 0.0252558421343565\n",
            "Iteration 6603/10000, Loss: 0.025255069136619568\n",
            "Iteration 6604/10000, Loss: 0.025254298001527786\n",
            "Iteration 6605/10000, Loss: 0.025253526866436005\n",
            "Iteration 6606/10000, Loss: 0.025252755731344223\n",
            "Iteration 6607/10000, Loss: 0.025251982733607292\n",
            "Iteration 6608/10000, Loss: 0.02525121532380581\n",
            "Iteration 6609/10000, Loss: 0.025250444188714027\n",
            "Iteration 6610/10000, Loss: 0.025249667465686798\n",
            "Iteration 6611/10000, Loss: 0.025248898193240166\n",
            "Iteration 6612/10000, Loss: 0.025248128920793533\n",
            "Iteration 6613/10000, Loss: 0.02524735778570175\n",
            "Iteration 6614/10000, Loss: 0.02524658665060997\n",
            "Iteration 6615/10000, Loss: 0.02524581551551819\n",
            "Iteration 6616/10000, Loss: 0.025245044380426407\n",
            "Iteration 6617/10000, Loss: 0.025244273245334625\n",
            "Iteration 6618/10000, Loss: 0.025243502110242844\n",
            "Iteration 6619/10000, Loss: 0.025242730975151062\n",
            "Iteration 6620/10000, Loss: 0.02524195984005928\n",
            "Iteration 6621/10000, Loss: 0.02524118684232235\n",
            "Iteration 6622/10000, Loss: 0.025240419432520866\n",
            "Iteration 6623/10000, Loss: 0.025239650160074234\n",
            "Iteration 6624/10000, Loss: 0.025238877162337303\n",
            "Iteration 6625/10000, Loss: 0.02523810788989067\n",
            "Iteration 6626/10000, Loss: 0.02523733489215374\n",
            "Iteration 6627/10000, Loss: 0.02523656375706196\n",
            "Iteration 6628/10000, Loss: 0.025235794484615326\n",
            "Iteration 6629/10000, Loss: 0.025235023349523544\n",
            "Iteration 6630/10000, Loss: 0.025234252214431763\n",
            "Iteration 6631/10000, Loss: 0.02523348294198513\n",
            "Iteration 6632/10000, Loss: 0.0252327099442482\n",
            "Iteration 6633/10000, Loss: 0.025231942534446716\n",
            "Iteration 6634/10000, Loss: 0.025231169536709785\n",
            "Iteration 6635/10000, Loss: 0.025230398401618004\n",
            "Iteration 6636/10000, Loss: 0.025229627266526222\n",
            "Iteration 6637/10000, Loss: 0.02522885613143444\n",
            "Iteration 6638/10000, Loss: 0.025228090584278107\n",
            "Iteration 6639/10000, Loss: 0.025227317586541176\n",
            "Iteration 6640/10000, Loss: 0.025226548314094543\n",
            "Iteration 6641/10000, Loss: 0.025225777179002762\n",
            "Iteration 6642/10000, Loss: 0.02522500604391098\n",
            "Iteration 6643/10000, Loss: 0.0252242349088192\n",
            "Iteration 6644/10000, Loss: 0.025223465636372566\n",
            "Iteration 6645/10000, Loss: 0.025222696363925934\n",
            "Iteration 6646/10000, Loss: 0.025221925228834152\n",
            "Iteration 6647/10000, Loss: 0.02522115409374237\n",
            "Iteration 6648/10000, Loss: 0.02522038295865059\n",
            "Iteration 6649/10000, Loss: 0.025219617411494255\n",
            "Iteration 6650/10000, Loss: 0.025218844413757324\n",
            "Iteration 6651/10000, Loss: 0.025218075141310692\n",
            "Iteration 6652/10000, Loss: 0.025217300280928612\n",
            "Iteration 6653/10000, Loss: 0.02521653287112713\n",
            "Iteration 6654/10000, Loss: 0.025215763598680496\n",
            "Iteration 6655/10000, Loss: 0.025214996188879013\n",
            "Iteration 6656/10000, Loss: 0.025214223191142082\n",
            "Iteration 6657/10000, Loss: 0.0252134520560503\n",
            "Iteration 6658/10000, Loss: 0.025212684646248817\n",
            "Iteration 6659/10000, Loss: 0.025211913511157036\n",
            "Iteration 6660/10000, Loss: 0.025211144238710403\n",
            "Iteration 6661/10000, Loss: 0.025210373103618622\n",
            "Iteration 6662/10000, Loss: 0.02520960383117199\n",
            "Iteration 6663/10000, Loss: 0.025208832696080208\n",
            "Iteration 6664/10000, Loss: 0.025208063423633575\n",
            "Iteration 6665/10000, Loss: 0.025207292288541794\n",
            "Iteration 6666/10000, Loss: 0.025206521153450012\n",
            "Iteration 6667/10000, Loss: 0.02520575560629368\n",
            "Iteration 6668/10000, Loss: 0.025204988196492195\n",
            "Iteration 6669/10000, Loss: 0.025204218924045563\n",
            "Iteration 6670/10000, Loss: 0.02520345151424408\n",
            "Iteration 6671/10000, Loss: 0.025202684104442596\n",
            "Iteration 6672/10000, Loss: 0.025201914831995964\n",
            "Iteration 6673/10000, Loss: 0.02520114742219448\n",
            "Iteration 6674/10000, Loss: 0.025200383737683296\n",
            "Iteration 6675/10000, Loss: 0.025199616327881813\n",
            "Iteration 6676/10000, Loss: 0.02519884519279003\n",
            "Iteration 6677/10000, Loss: 0.02519807778298855\n",
            "Iteration 6678/10000, Loss: 0.025197312235832214\n",
            "Iteration 6679/10000, Loss: 0.025196542963385582\n",
            "Iteration 6680/10000, Loss: 0.025195777416229248\n",
            "Iteration 6681/10000, Loss: 0.025195010006427765\n",
            "Iteration 6682/10000, Loss: 0.025194242596626282\n",
            "Iteration 6683/10000, Loss: 0.025193477049469948\n",
            "Iteration 6684/10000, Loss: 0.025192707777023315\n",
            "Iteration 6685/10000, Loss: 0.025191940367221832\n",
            "Iteration 6686/10000, Loss: 0.02519117295742035\n",
            "Iteration 6687/10000, Loss: 0.025190407410264015\n",
            "Iteration 6688/10000, Loss: 0.025189640000462532\n",
            "Iteration 6689/10000, Loss: 0.0251888707280159\n",
            "Iteration 6690/10000, Loss: 0.025188105180859566\n",
            "Iteration 6691/10000, Loss: 0.025187339633703232\n",
            "Iteration 6692/10000, Loss: 0.02518657222390175\n",
            "Iteration 6693/10000, Loss: 0.025185804814100266\n",
            "Iteration 6694/10000, Loss: 0.025185035541653633\n",
            "Iteration 6695/10000, Loss: 0.02518427185714245\n",
            "Iteration 6696/10000, Loss: 0.025183502584695816\n",
            "Iteration 6697/10000, Loss: 0.025182737037539482\n",
            "Iteration 6698/10000, Loss: 0.025181971490383148\n",
            "Iteration 6699/10000, Loss: 0.025181200355291367\n",
            "Iteration 6700/10000, Loss: 0.025180436670780182\n",
            "Iteration 6701/10000, Loss: 0.0251796692609787\n",
            "Iteration 6702/10000, Loss: 0.025178905576467514\n",
            "Iteration 6703/10000, Loss: 0.025178132578730583\n",
            "Iteration 6704/10000, Loss: 0.02517736703157425\n",
            "Iteration 6705/10000, Loss: 0.025176603347063065\n",
            "Iteration 6706/10000, Loss: 0.02517583593726158\n",
            "Iteration 6707/10000, Loss: 0.0251750685274601\n",
            "Iteration 6708/10000, Loss: 0.025174301117658615\n",
            "Iteration 6709/10000, Loss: 0.025173533707857132\n",
            "Iteration 6710/10000, Loss: 0.025172768160700798\n",
            "Iteration 6711/10000, Loss: 0.025172002613544464\n",
            "Iteration 6712/10000, Loss: 0.02517123520374298\n",
            "Iteration 6713/10000, Loss: 0.02517046593129635\n",
            "Iteration 6714/10000, Loss: 0.025169700384140015\n",
            "Iteration 6715/10000, Loss: 0.02516893297433853\n",
            "Iteration 6716/10000, Loss: 0.025168169289827347\n",
            "Iteration 6717/10000, Loss: 0.025167403742671013\n",
            "Iteration 6718/10000, Loss: 0.02516663447022438\n",
            "Iteration 6719/10000, Loss: 0.025165867060422897\n",
            "Iteration 6720/10000, Loss: 0.025165101513266563\n",
            "Iteration 6721/10000, Loss: 0.02516433596611023\n",
            "Iteration 6722/10000, Loss: 0.025163570418953896\n",
            "Iteration 6723/10000, Loss: 0.025162803009152412\n",
            "Iteration 6724/10000, Loss: 0.02516203746199608\n",
            "Iteration 6725/10000, Loss: 0.025161268189549446\n",
            "Iteration 6726/10000, Loss: 0.025160502642393112\n",
            "Iteration 6727/10000, Loss: 0.025159737095236778\n",
            "Iteration 6728/10000, Loss: 0.025158969685435295\n",
            "Iteration 6729/10000, Loss: 0.02515820413827896\n",
            "Iteration 6730/10000, Loss: 0.02515743486583233\n",
            "Iteration 6731/10000, Loss: 0.025156671181321144\n",
            "Iteration 6732/10000, Loss: 0.02515590563416481\n",
            "Iteration 6733/10000, Loss: 0.025155136361718178\n",
            "Iteration 6734/10000, Loss: 0.025154372677206993\n",
            "Iteration 6735/10000, Loss: 0.02515360899269581\n",
            "Iteration 6736/10000, Loss: 0.025152841582894325\n",
            "Iteration 6737/10000, Loss: 0.025152074173092842\n",
            "Iteration 6738/10000, Loss: 0.025151310488581657\n",
            "Iteration 6739/10000, Loss: 0.025150543078780174\n",
            "Iteration 6740/10000, Loss: 0.02514977566897869\n",
            "Iteration 6741/10000, Loss: 0.025149011984467506\n",
            "Iteration 6742/10000, Loss: 0.025148246437311172\n",
            "Iteration 6743/10000, Loss: 0.02514747902750969\n",
            "Iteration 6744/10000, Loss: 0.025146711617708206\n",
            "Iteration 6745/10000, Loss: 0.025145946070551872\n",
            "Iteration 6746/10000, Loss: 0.025145182386040688\n",
            "Iteration 6747/10000, Loss: 0.025144413113594055\n",
            "Iteration 6748/10000, Loss: 0.02514365129172802\n",
            "Iteration 6749/10000, Loss: 0.025142882019281387\n",
            "Iteration 6750/10000, Loss: 0.025142114609479904\n",
            "Iteration 6751/10000, Loss: 0.02514135092496872\n",
            "Iteration 6752/10000, Loss: 0.025140585377812386\n",
            "Iteration 6753/10000, Loss: 0.02513981983065605\n",
            "Iteration 6754/10000, Loss: 0.02513905242085457\n",
            "Iteration 6755/10000, Loss: 0.025138290598988533\n",
            "Iteration 6756/10000, Loss: 0.02513752318918705\n",
            "Iteration 6757/10000, Loss: 0.025136755779385567\n",
            "Iteration 6758/10000, Loss: 0.025135990232229233\n",
            "Iteration 6759/10000, Loss: 0.025135226547718048\n",
            "Iteration 6760/10000, Loss: 0.025134459137916565\n",
            "Iteration 6761/10000, Loss: 0.02513369359076023\n",
            "Iteration 6762/10000, Loss: 0.025132928043603897\n",
            "Iteration 6763/10000, Loss: 0.025132164359092712\n",
            "Iteration 6764/10000, Loss: 0.02513139694929123\n",
            "Iteration 6765/10000, Loss: 0.025130629539489746\n",
            "Iteration 6766/10000, Loss: 0.02512986771762371\n",
            "Iteration 6767/10000, Loss: 0.025129102170467377\n",
            "Iteration 6768/10000, Loss: 0.025128336623311043\n",
            "Iteration 6769/10000, Loss: 0.02512756735086441\n",
            "Iteration 6770/10000, Loss: 0.025126803666353226\n",
            "Iteration 6771/10000, Loss: 0.02512603998184204\n",
            "Iteration 6772/10000, Loss: 0.025125274434685707\n",
            "Iteration 6773/10000, Loss: 0.025124508887529373\n",
            "Iteration 6774/10000, Loss: 0.02512374334037304\n",
            "Iteration 6775/10000, Loss: 0.025122977793216705\n",
            "Iteration 6776/10000, Loss: 0.025122210383415222\n",
            "Iteration 6777/10000, Loss: 0.025121444836258888\n",
            "Iteration 6778/10000, Loss: 0.025120679289102554\n",
            "Iteration 6779/10000, Loss: 0.02511991560459137\n",
            "Iteration 6780/10000, Loss: 0.025119151920080185\n",
            "Iteration 6781/10000, Loss: 0.02511838637292385\n",
            "Iteration 6782/10000, Loss: 0.025117622688412666\n",
            "Iteration 6783/10000, Loss: 0.025116855278611183\n",
            "Iteration 6784/10000, Loss: 0.02511608973145485\n",
            "Iteration 6785/10000, Loss: 0.025115327909588814\n",
            "Iteration 6786/10000, Loss: 0.02511456049978733\n",
            "Iteration 6787/10000, Loss: 0.025113798677921295\n",
            "Iteration 6788/10000, Loss: 0.02511303313076496\n",
            "Iteration 6789/10000, Loss: 0.025112265720963478\n",
            "Iteration 6790/10000, Loss: 0.025111502036452293\n",
            "Iteration 6791/10000, Loss: 0.025110740214586258\n",
            "Iteration 6792/10000, Loss: 0.025109974667429924\n",
            "Iteration 6793/10000, Loss: 0.02510921098291874\n",
            "Iteration 6794/10000, Loss: 0.025108445435762405\n",
            "Iteration 6795/10000, Loss: 0.02510768175125122\n",
            "Iteration 6796/10000, Loss: 0.025106918066740036\n",
            "Iteration 6797/10000, Loss: 0.025106152519583702\n",
            "Iteration 6798/10000, Loss: 0.025105388835072517\n",
            "Iteration 6799/10000, Loss: 0.025104625150561333\n",
            "Iteration 6800/10000, Loss: 0.02510385774075985\n",
            "Iteration 6801/10000, Loss: 0.025103097781538963\n",
            "Iteration 6802/10000, Loss: 0.02510233223438263\n",
            "Iteration 6803/10000, Loss: 0.025101570412516594\n",
            "Iteration 6804/10000, Loss: 0.02510080114006996\n",
            "Iteration 6805/10000, Loss: 0.025100035592913628\n",
            "Iteration 6806/10000, Loss: 0.025099271908402443\n",
            "Iteration 6807/10000, Loss: 0.025098510086536407\n",
            "Iteration 6808/10000, Loss: 0.025097744539380074\n",
            "Iteration 6809/10000, Loss: 0.02509697899222374\n",
            "Iteration 6810/10000, Loss: 0.025096215307712555\n",
            "Iteration 6811/10000, Loss: 0.02509545162320137\n",
            "Iteration 6812/10000, Loss: 0.025094689801335335\n",
            "Iteration 6813/10000, Loss: 0.025093924254179\n",
            "Iteration 6814/10000, Loss: 0.025093158707022667\n",
            "Iteration 6815/10000, Loss: 0.02509239688515663\n",
            "Iteration 6816/10000, Loss: 0.025091631338000298\n",
            "Iteration 6817/10000, Loss: 0.025090869516134262\n",
            "Iteration 6818/10000, Loss: 0.025090105831623077\n",
            "Iteration 6819/10000, Loss: 0.025089342147111893\n",
            "Iteration 6820/10000, Loss: 0.02508857659995556\n",
            "Iteration 6821/10000, Loss: 0.025087811052799225\n",
            "Iteration 6822/10000, Loss: 0.02508704923093319\n",
            "Iteration 6823/10000, Loss: 0.025086285546422005\n",
            "Iteration 6824/10000, Loss: 0.02508551999926567\n",
            "Iteration 6825/10000, Loss: 0.025084758177399635\n",
            "Iteration 6826/10000, Loss: 0.0250839926302433\n",
            "Iteration 6827/10000, Loss: 0.025083232671022415\n",
            "Iteration 6828/10000, Loss: 0.02508246712386608\n",
            "Iteration 6829/10000, Loss: 0.025081701576709747\n",
            "Iteration 6830/10000, Loss: 0.025080937892198563\n",
            "Iteration 6831/10000, Loss: 0.025080174207687378\n",
            "Iteration 6832/10000, Loss: 0.025079410523176193\n",
            "Iteration 6833/10000, Loss: 0.02507864683866501\n",
            "Iteration 6834/10000, Loss: 0.025077883154153824\n",
            "Iteration 6835/10000, Loss: 0.02507711946964264\n",
            "Iteration 6836/10000, Loss: 0.025076355785131454\n",
            "Iteration 6837/10000, Loss: 0.02507559210062027\n",
            "Iteration 6838/10000, Loss: 0.025074828416109085\n",
            "Iteration 6839/10000, Loss: 0.0250740647315979\n",
            "Iteration 6840/10000, Loss: 0.025073299184441566\n",
            "Iteration 6841/10000, Loss: 0.025072535499930382\n",
            "Iteration 6842/10000, Loss: 0.025071773678064346\n",
            "Iteration 6843/10000, Loss: 0.02507101185619831\n",
            "Iteration 6844/10000, Loss: 0.025070246309041977\n",
            "Iteration 6845/10000, Loss: 0.025069482624530792\n",
            "Iteration 6846/10000, Loss: 0.025068718940019608\n",
            "Iteration 6847/10000, Loss: 0.025067957118153572\n",
            "Iteration 6848/10000, Loss: 0.025067193433642387\n",
            "Iteration 6849/10000, Loss: 0.025066429749131203\n",
            "Iteration 6850/10000, Loss: 0.025065666064620018\n",
            "Iteration 6851/10000, Loss: 0.025064904242753983\n",
            "Iteration 6852/10000, Loss: 0.025064144283533096\n",
            "Iteration 6853/10000, Loss: 0.025063378736376762\n",
            "Iteration 6854/10000, Loss: 0.025062616914510727\n",
            "Iteration 6855/10000, Loss: 0.025061851367354393\n",
            "Iteration 6856/10000, Loss: 0.02506108768284321\n",
            "Iteration 6857/10000, Loss: 0.025060325860977173\n",
            "Iteration 6858/10000, Loss: 0.025059562176465988\n",
            "Iteration 6859/10000, Loss: 0.025058798491954803\n",
            "Iteration 6860/10000, Loss: 0.02505803480744362\n",
            "Iteration 6861/10000, Loss: 0.025057272985577583\n",
            "Iteration 6862/10000, Loss: 0.0250565093010664\n",
            "Iteration 6863/10000, Loss: 0.025055745616555214\n",
            "Iteration 6864/10000, Loss: 0.02505498193204403\n",
            "Iteration 6865/10000, Loss: 0.025054220110177994\n",
            "Iteration 6866/10000, Loss: 0.02505345642566681\n",
            "Iteration 6867/10000, Loss: 0.025052694603800774\n",
            "Iteration 6868/10000, Loss: 0.02505193091928959\n",
            "Iteration 6869/10000, Loss: 0.025051167234778404\n",
            "Iteration 6870/10000, Loss: 0.02505040541291237\n",
            "Iteration 6871/10000, Loss: 0.025049641728401184\n",
            "Iteration 6872/10000, Loss: 0.02504887804389\n",
            "Iteration 6873/10000, Loss: 0.025048118084669113\n",
            "Iteration 6874/10000, Loss: 0.02504735253751278\n",
            "Iteration 6875/10000, Loss: 0.025046590715646744\n",
            "Iteration 6876/10000, Loss: 0.02504582703113556\n",
            "Iteration 6877/10000, Loss: 0.025045067071914673\n",
            "Iteration 6878/10000, Loss: 0.025044303387403488\n",
            "Iteration 6879/10000, Loss: 0.025043539702892303\n",
            "Iteration 6880/10000, Loss: 0.02504277601838112\n",
            "Iteration 6881/10000, Loss: 0.025042014196515083\n",
            "Iteration 6882/10000, Loss: 0.025041254237294197\n",
            "Iteration 6883/10000, Loss: 0.025040490552783012\n",
            "Iteration 6884/10000, Loss: 0.02503972500562668\n",
            "Iteration 6885/10000, Loss: 0.025038961321115494\n",
            "Iteration 6886/10000, Loss: 0.02503819949924946\n",
            "Iteration 6887/10000, Loss: 0.02503744140267372\n",
            "Iteration 6888/10000, Loss: 0.025036675855517387\n",
            "Iteration 6889/10000, Loss: 0.025035914033651352\n",
            "Iteration 6890/10000, Loss: 0.025035148486495018\n",
            "Iteration 6891/10000, Loss: 0.025034388527274132\n",
            "Iteration 6892/10000, Loss: 0.025033624842762947\n",
            "Iteration 6893/10000, Loss: 0.02503286302089691\n",
            "Iteration 6894/10000, Loss: 0.025032103061676025\n",
            "Iteration 6895/10000, Loss: 0.02503133751451969\n",
            "Iteration 6896/10000, Loss: 0.025030575692653656\n",
            "Iteration 6897/10000, Loss: 0.02502981387078762\n",
            "Iteration 6898/10000, Loss: 0.025029050186276436\n",
            "Iteration 6899/10000, Loss: 0.0250282883644104\n",
            "Iteration 6900/10000, Loss: 0.025027528405189514\n",
            "Iteration 6901/10000, Loss: 0.02502676472067833\n",
            "Iteration 6902/10000, Loss: 0.025026002898812294\n",
            "Iteration 6903/10000, Loss: 0.02502524107694626\n",
            "Iteration 6904/10000, Loss: 0.025024477392435074\n",
            "Iteration 6905/10000, Loss: 0.02502371184527874\n",
            "Iteration 6906/10000, Loss: 0.025022953748703003\n",
            "Iteration 6907/10000, Loss: 0.025022193789482117\n",
            "Iteration 6908/10000, Loss: 0.02502143196761608\n",
            "Iteration 6909/10000, Loss: 0.025020666420459747\n",
            "Iteration 6910/10000, Loss: 0.02501990646123886\n",
            "Iteration 6911/10000, Loss: 0.025019142776727676\n",
            "Iteration 6912/10000, Loss: 0.02501838468015194\n",
            "Iteration 6913/10000, Loss: 0.025017619132995605\n",
            "Iteration 6914/10000, Loss: 0.02501685731112957\n",
            "Iteration 6915/10000, Loss: 0.025016095489263535\n",
            "Iteration 6916/10000, Loss: 0.02501533180475235\n",
            "Iteration 6917/10000, Loss: 0.025014571845531464\n",
            "Iteration 6918/10000, Loss: 0.025013810023665428\n",
            "Iteration 6919/10000, Loss: 0.025013046339154243\n",
            "Iteration 6920/10000, Loss: 0.025012284517288208\n",
            "Iteration 6921/10000, Loss: 0.025011520832777023\n",
            "Iteration 6922/10000, Loss: 0.025010764598846436\n",
            "Iteration 6923/10000, Loss: 0.0250100027769804\n",
            "Iteration 6924/10000, Loss: 0.025009240955114365\n",
            "Iteration 6925/10000, Loss: 0.02500847727060318\n",
            "Iteration 6926/10000, Loss: 0.025007715448737144\n",
            "Iteration 6927/10000, Loss: 0.025006955489516258\n",
            "Iteration 6928/10000, Loss: 0.025006191805005074\n",
            "Iteration 6929/10000, Loss: 0.025005431845784187\n",
            "Iteration 6930/10000, Loss: 0.025004670023918152\n",
            "Iteration 6931/10000, Loss: 0.025003911927342415\n",
            "Iteration 6932/10000, Loss: 0.025003153830766678\n",
            "Iteration 6933/10000, Loss: 0.02500239387154579\n",
            "Iteration 6934/10000, Loss: 0.025001633912324905\n",
            "Iteration 6935/10000, Loss: 0.02500087209045887\n",
            "Iteration 6936/10000, Loss: 0.025000115856528282\n",
            "Iteration 6937/10000, Loss: 0.024999355897307396\n",
            "Iteration 6938/10000, Loss: 0.02499859593808651\n",
            "Iteration 6939/10000, Loss: 0.024997837841510773\n",
            "Iteration 6940/10000, Loss: 0.024997076019644737\n",
            "Iteration 6941/10000, Loss: 0.02499631978571415\n",
            "Iteration 6942/10000, Loss: 0.024995561689138412\n",
            "Iteration 6943/10000, Loss: 0.024994799867272377\n",
            "Iteration 6944/10000, Loss: 0.02499403990805149\n",
            "Iteration 6945/10000, Loss: 0.024993281811475754\n",
            "Iteration 6946/10000, Loss: 0.024992527440190315\n",
            "Iteration 6947/10000, Loss: 0.02499176561832428\n",
            "Iteration 6948/10000, Loss: 0.02499101124703884\n",
            "Iteration 6949/10000, Loss: 0.024990251287817955\n",
            "Iteration 6950/10000, Loss: 0.024989493191242218\n",
            "Iteration 6951/10000, Loss: 0.02498873695731163\n",
            "Iteration 6952/10000, Loss: 0.024987978860735893\n",
            "Iteration 6953/10000, Loss: 0.024987220764160156\n",
            "Iteration 6954/10000, Loss: 0.02498646080493927\n",
            "Iteration 6955/10000, Loss: 0.02498570643365383\n",
            "Iteration 6956/10000, Loss: 0.024984948337078094\n",
            "Iteration 6957/10000, Loss: 0.024984190240502357\n",
            "Iteration 6958/10000, Loss: 0.02498343214392662\n",
            "Iteration 6959/10000, Loss: 0.024982674047350883\n",
            "Iteration 6960/10000, Loss: 0.024981915950775146\n",
            "Iteration 6961/10000, Loss: 0.024981161579489708\n",
            "Iteration 6962/10000, Loss: 0.02498040162026882\n",
            "Iteration 6963/10000, Loss: 0.024979647248983383\n",
            "Iteration 6964/10000, Loss: 0.024978887289762497\n",
            "Iteration 6965/10000, Loss: 0.02497812919318676\n",
            "Iteration 6966/10000, Loss: 0.02497737854719162\n",
            "Iteration 6967/10000, Loss: 0.024976618587970734\n",
            "Iteration 6968/10000, Loss: 0.024975862354040146\n",
            "Iteration 6969/10000, Loss: 0.02497510425746441\n",
            "Iteration 6970/10000, Loss: 0.02497435361146927\n",
            "Iteration 6971/10000, Loss: 0.024973591789603233\n",
            "Iteration 6972/10000, Loss: 0.024972835555672646\n",
            "Iteration 6973/10000, Loss: 0.02497207745909691\n",
            "Iteration 6974/10000, Loss: 0.02497132122516632\n",
            "Iteration 6975/10000, Loss: 0.024970564991235733\n",
            "Iteration 6976/10000, Loss: 0.024969808757305145\n",
            "Iteration 6977/10000, Loss: 0.024969050660729408\n",
            "Iteration 6978/10000, Loss: 0.02496829628944397\n",
            "Iteration 6979/10000, Loss: 0.024967536330223083\n",
            "Iteration 6980/10000, Loss: 0.024966781958937645\n",
            "Iteration 6981/10000, Loss: 0.024966025725007057\n",
            "Iteration 6982/10000, Loss: 0.02496526762843132\n",
            "Iteration 6983/10000, Loss: 0.02496451325714588\n",
            "Iteration 6984/10000, Loss: 0.024963755160570145\n",
            "Iteration 6985/10000, Loss: 0.024963000789284706\n",
            "Iteration 6986/10000, Loss: 0.02496224269270897\n",
            "Iteration 6987/10000, Loss: 0.02496148645877838\n",
            "Iteration 6988/10000, Loss: 0.024960730224847794\n",
            "Iteration 6989/10000, Loss: 0.024959970265626907\n",
            "Iteration 6990/10000, Loss: 0.024959217756986618\n",
            "Iteration 6991/10000, Loss: 0.02495845966041088\n",
            "Iteration 6992/10000, Loss: 0.024957707151770592\n",
            "Iteration 6993/10000, Loss: 0.024956949055194855\n",
            "Iteration 6994/10000, Loss: 0.024956190958619118\n",
            "Iteration 6995/10000, Loss: 0.02495543472468853\n",
            "Iteration 6996/10000, Loss: 0.02495468035340309\n",
            "Iteration 6997/10000, Loss: 0.024953922256827354\n",
            "Iteration 6998/10000, Loss: 0.024953166022896767\n",
            "Iteration 6999/10000, Loss: 0.02495240978896618\n",
            "Iteration 7000/10000, Loss: 0.02495165541768074\n",
            "Iteration 7001/10000, Loss: 0.024950899183750153\n",
            "Iteration 7002/10000, Loss: 0.024950141087174416\n",
            "Iteration 7003/10000, Loss: 0.024949384853243828\n",
            "Iteration 7004/10000, Loss: 0.02494863048195839\n",
            "Iteration 7005/10000, Loss: 0.0249478742480278\n",
            "Iteration 7006/10000, Loss: 0.024947119876742363\n",
            "Iteration 7007/10000, Loss: 0.024946361780166626\n",
            "Iteration 7008/10000, Loss: 0.024945605546236038\n",
            "Iteration 7009/10000, Loss: 0.02494484931230545\n",
            "Iteration 7010/10000, Loss: 0.024944094941020012\n",
            "Iteration 7011/10000, Loss: 0.024943338707089424\n",
            "Iteration 7012/10000, Loss: 0.024942582473158836\n",
            "Iteration 7013/10000, Loss: 0.02494182623922825\n",
            "Iteration 7014/10000, Loss: 0.02494107000529766\n",
            "Iteration 7015/10000, Loss: 0.024940313771367073\n",
            "Iteration 7016/10000, Loss: 0.024939561262726784\n",
            "Iteration 7017/10000, Loss: 0.024938801303505898\n",
            "Iteration 7018/10000, Loss: 0.024938048794865608\n",
            "Iteration 7019/10000, Loss: 0.02493729256093502\n",
            "Iteration 7020/10000, Loss: 0.024936534464359283\n",
            "Iteration 7021/10000, Loss: 0.024935778230428696\n",
            "Iteration 7022/10000, Loss: 0.024935023859143257\n",
            "Iteration 7023/10000, Loss: 0.02493426948785782\n",
            "Iteration 7024/10000, Loss: 0.02493351325392723\n",
            "Iteration 7025/10000, Loss: 0.024932757019996643\n",
            "Iteration 7026/10000, Loss: 0.024932000786066055\n",
            "Iteration 7027/10000, Loss: 0.024931248277425766\n",
            "Iteration 7028/10000, Loss: 0.024930492043495178\n",
            "Iteration 7029/10000, Loss: 0.02492973767220974\n",
            "Iteration 7030/10000, Loss: 0.024928979575634003\n",
            "Iteration 7031/10000, Loss: 0.024928227066993713\n",
            "Iteration 7032/10000, Loss: 0.024927467107772827\n",
            "Iteration 7033/10000, Loss: 0.024926714599132538\n",
            "Iteration 7034/10000, Loss: 0.02492595836520195\n",
            "Iteration 7035/10000, Loss: 0.024925202131271362\n",
            "Iteration 7036/10000, Loss: 0.024924447759985924\n",
            "Iteration 7037/10000, Loss: 0.024923691526055336\n",
            "Iteration 7038/10000, Loss: 0.024922937154769897\n",
            "Iteration 7039/10000, Loss: 0.02492218092083931\n",
            "Iteration 7040/10000, Loss: 0.024921424686908722\n",
            "Iteration 7041/10000, Loss: 0.024920670315623283\n",
            "Iteration 7042/10000, Loss: 0.024919914081692696\n",
            "Iteration 7043/10000, Loss: 0.024919159710407257\n",
            "Iteration 7044/10000, Loss: 0.02491840347647667\n",
            "Iteration 7045/10000, Loss: 0.02491764724254608\n",
            "Iteration 7046/10000, Loss: 0.024916894733905792\n",
            "Iteration 7047/10000, Loss: 0.024916136637330055\n",
            "Iteration 7048/10000, Loss: 0.024915384128689766\n",
            "Iteration 7049/10000, Loss: 0.024914629757404327\n",
            "Iteration 7050/10000, Loss: 0.02491387352347374\n",
            "Iteration 7051/10000, Loss: 0.024913115426898003\n",
            "Iteration 7052/10000, Loss: 0.024912364780902863\n",
            "Iteration 7053/10000, Loss: 0.024911606684327126\n",
            "Iteration 7054/10000, Loss: 0.024910854175686836\n",
            "Iteration 7055/10000, Loss: 0.0249100960791111\n",
            "Iteration 7056/10000, Loss: 0.02490934170782566\n",
            "Iteration 7057/10000, Loss: 0.02490858919918537\n",
            "Iteration 7058/10000, Loss: 0.024907832965254784\n",
            "Iteration 7059/10000, Loss: 0.024907074868679047\n",
            "Iteration 7060/10000, Loss: 0.024906322360038757\n",
            "Iteration 7061/10000, Loss: 0.02490556612610817\n",
            "Iteration 7062/10000, Loss: 0.02490481361746788\n",
            "Iteration 7063/10000, Loss: 0.024904057383537292\n",
            "Iteration 7064/10000, Loss: 0.024903303012251854\n",
            "Iteration 7065/10000, Loss: 0.024902546778321266\n",
            "Iteration 7066/10000, Loss: 0.024901792407035828\n",
            "Iteration 7067/10000, Loss: 0.02490103803575039\n",
            "Iteration 7068/10000, Loss: 0.02490028366446495\n",
            "Iteration 7069/10000, Loss: 0.024899529293179512\n",
            "Iteration 7070/10000, Loss: 0.024898773059248924\n",
            "Iteration 7071/10000, Loss: 0.024898016825318336\n",
            "Iteration 7072/10000, Loss: 0.024897266179323196\n",
            "Iteration 7073/10000, Loss: 0.02489650994539261\n",
            "Iteration 7074/10000, Loss: 0.02489575743675232\n",
            "Iteration 7075/10000, Loss: 0.02489500120282173\n",
            "Iteration 7076/10000, Loss: 0.024894248694181442\n",
            "Iteration 7077/10000, Loss: 0.024893492460250854\n",
            "Iteration 7078/10000, Loss: 0.024892738088965416\n",
            "Iteration 7079/10000, Loss: 0.024891981855034828\n",
            "Iteration 7080/10000, Loss: 0.02489122934639454\n",
            "Iteration 7081/10000, Loss: 0.0248904749751091\n",
            "Iteration 7082/10000, Loss: 0.024889720603823662\n",
            "Iteration 7083/10000, Loss: 0.024888962507247925\n",
            "Iteration 7084/10000, Loss: 0.024888209998607635\n",
            "Iteration 7085/10000, Loss: 0.024887455627322197\n",
            "Iteration 7086/10000, Loss: 0.024886703118681908\n",
            "Iteration 7087/10000, Loss: 0.02488594874739647\n",
            "Iteration 7088/10000, Loss: 0.024885190650820732\n",
            "Iteration 7089/10000, Loss: 0.024884438142180443\n",
            "Iteration 7090/10000, Loss: 0.024883683770895004\n",
            "Iteration 7091/10000, Loss: 0.024882931262254715\n",
            "Iteration 7092/10000, Loss: 0.024882176890969276\n",
            "Iteration 7093/10000, Loss: 0.02488142065703869\n",
            "Iteration 7094/10000, Loss: 0.0248806681483984\n",
            "Iteration 7095/10000, Loss: 0.02487991191446781\n",
            "Iteration 7096/10000, Loss: 0.024879157543182373\n",
            "Iteration 7097/10000, Loss: 0.024878405034542084\n",
            "Iteration 7098/10000, Loss: 0.024877652525901794\n",
            "Iteration 7099/10000, Loss: 0.024876892566680908\n",
            "Iteration 7100/10000, Loss: 0.024876143783330917\n",
            "Iteration 7101/10000, Loss: 0.02487538754940033\n",
            "Iteration 7102/10000, Loss: 0.02487463504076004\n",
            "Iteration 7103/10000, Loss: 0.024873880669474602\n",
            "Iteration 7104/10000, Loss: 0.024873126298189163\n",
            "Iteration 7105/10000, Loss: 0.024872371926903725\n",
            "Iteration 7106/10000, Loss: 0.024871619418263435\n",
            "Iteration 7107/10000, Loss: 0.024870863184332848\n",
            "Iteration 7108/10000, Loss: 0.02487011067569256\n",
            "Iteration 7109/10000, Loss: 0.02486935444176197\n",
            "Iteration 7110/10000, Loss: 0.024868600070476532\n",
            "Iteration 7111/10000, Loss: 0.024867849424481392\n",
            "Iteration 7112/10000, Loss: 0.024867093190550804\n",
            "Iteration 7113/10000, Loss: 0.024866340681910515\n",
            "Iteration 7114/10000, Loss: 0.024865584447979927\n",
            "Iteration 7115/10000, Loss: 0.024864831939339638\n",
            "Iteration 7116/10000, Loss: 0.0248640775680542\n",
            "Iteration 7117/10000, Loss: 0.02486332319676876\n",
            "Iteration 7118/10000, Loss: 0.02486257068812847\n",
            "Iteration 7119/10000, Loss: 0.024861818179488182\n",
            "Iteration 7120/10000, Loss: 0.024861065670847893\n",
            "Iteration 7121/10000, Loss: 0.024860307574272156\n",
            "Iteration 7122/10000, Loss: 0.024859555065631866\n",
            "Iteration 7123/10000, Loss: 0.02485879883170128\n",
            "Iteration 7124/10000, Loss: 0.024858050048351288\n",
            "Iteration 7125/10000, Loss: 0.02485729567706585\n",
            "Iteration 7126/10000, Loss: 0.02485654130578041\n",
            "Iteration 7127/10000, Loss: 0.024855785071849823\n",
            "Iteration 7128/10000, Loss: 0.024855032563209534\n",
            "Iteration 7129/10000, Loss: 0.024854281917214394\n",
            "Iteration 7130/10000, Loss: 0.024853529408574104\n",
            "Iteration 7131/10000, Loss: 0.024852773174643517\n",
            "Iteration 7132/10000, Loss: 0.024852022528648376\n",
            "Iteration 7133/10000, Loss: 0.02485126443207264\n",
            "Iteration 7134/10000, Loss: 0.0248505137860775\n",
            "Iteration 7135/10000, Loss: 0.02484976127743721\n",
            "Iteration 7136/10000, Loss: 0.02484900690615177\n",
            "Iteration 7137/10000, Loss: 0.024848254397511482\n",
            "Iteration 7138/10000, Loss: 0.024847500026226044\n",
            "Iteration 7139/10000, Loss: 0.024846749380230904\n",
            "Iteration 7140/10000, Loss: 0.024845996871590614\n",
            "Iteration 7141/10000, Loss: 0.024845238775014877\n",
            "Iteration 7142/10000, Loss: 0.024844486266374588\n",
            "Iteration 7143/10000, Loss: 0.024843735620379448\n",
            "Iteration 7144/10000, Loss: 0.02484298124909401\n",
            "Iteration 7145/10000, Loss: 0.02484223060309887\n",
            "Iteration 7146/10000, Loss: 0.02484147436916828\n",
            "Iteration 7147/10000, Loss: 0.02484072372317314\n",
            "Iteration 7148/10000, Loss: 0.024839969351887703\n",
            "Iteration 7149/10000, Loss: 0.024839218705892563\n",
            "Iteration 7150/10000, Loss: 0.024838466197252274\n",
            "Iteration 7151/10000, Loss: 0.024837709963321686\n",
            "Iteration 7152/10000, Loss: 0.024836957454681396\n",
            "Iteration 7153/10000, Loss: 0.024836206808686256\n",
            "Iteration 7154/10000, Loss: 0.024835456162691116\n",
            "Iteration 7155/10000, Loss: 0.02483469992876053\n",
            "Iteration 7156/10000, Loss: 0.024833951145410538\n",
            "Iteration 7157/10000, Loss: 0.02483319491147995\n",
            "Iteration 7158/10000, Loss: 0.02483244612812996\n",
            "Iteration 7159/10000, Loss: 0.02483169361948967\n",
            "Iteration 7160/10000, Loss: 0.02483093924820423\n",
            "Iteration 7161/10000, Loss: 0.02483018860220909\n",
            "Iteration 7162/10000, Loss: 0.024829436093568802\n",
            "Iteration 7163/10000, Loss: 0.024828683584928513\n",
            "Iteration 7164/10000, Loss: 0.024827932938933372\n",
            "Iteration 7165/10000, Loss: 0.024827176705002785\n",
            "Iteration 7166/10000, Loss: 0.024826424196362495\n",
            "Iteration 7167/10000, Loss: 0.024825673550367355\n",
            "Iteration 7168/10000, Loss: 0.024824921041727066\n",
            "Iteration 7169/10000, Loss: 0.024824168533086777\n",
            "Iteration 7170/10000, Loss: 0.024823416024446487\n",
            "Iteration 7171/10000, Loss: 0.024822663515806198\n",
            "Iteration 7172/10000, Loss: 0.02482191100716591\n",
            "Iteration 7173/10000, Loss: 0.024821162223815918\n",
            "Iteration 7174/10000, Loss: 0.02482040598988533\n",
            "Iteration 7175/10000, Loss: 0.02481965348124504\n",
            "Iteration 7176/10000, Loss: 0.0248189028352499\n",
            "Iteration 7177/10000, Loss: 0.02481815032660961\n",
            "Iteration 7178/10000, Loss: 0.024817397817969322\n",
            "Iteration 7179/10000, Loss: 0.024816643446683884\n",
            "Iteration 7180/10000, Loss: 0.024815892800688744\n",
            "Iteration 7181/10000, Loss: 0.024815142154693604\n",
            "Iteration 7182/10000, Loss: 0.024814389646053314\n",
            "Iteration 7183/10000, Loss: 0.024813639000058174\n",
            "Iteration 7184/10000, Loss: 0.024812882766127586\n",
            "Iteration 7185/10000, Loss: 0.024812130257487297\n",
            "Iteration 7186/10000, Loss: 0.024811383336782455\n",
            "Iteration 7187/10000, Loss: 0.024810630828142166\n",
            "Iteration 7188/10000, Loss: 0.024809876456856728\n",
            "Iteration 7189/10000, Loss: 0.024809125810861588\n",
            "Iteration 7190/10000, Loss: 0.024808373302221298\n",
            "Iteration 7191/10000, Loss: 0.024807628244161606\n",
            "Iteration 7192/10000, Loss: 0.024806875735521317\n",
            "Iteration 7193/10000, Loss: 0.024806125089526176\n",
            "Iteration 7194/10000, Loss: 0.024805374443531036\n",
            "Iteration 7195/10000, Loss: 0.024804627522826195\n",
            "Iteration 7196/10000, Loss: 0.024803878739476204\n",
            "Iteration 7197/10000, Loss: 0.024803128093481064\n",
            "Iteration 7198/10000, Loss: 0.024802381172776222\n",
            "Iteration 7199/10000, Loss: 0.024801626801490784\n",
            "Iteration 7200/10000, Loss: 0.024800879880785942\n",
            "Iteration 7201/10000, Loss: 0.024800129234790802\n",
            "Iteration 7202/10000, Loss: 0.02479938231408596\n",
            "Iteration 7203/10000, Loss: 0.02479863539338112\n",
            "Iteration 7204/10000, Loss: 0.02479788288474083\n",
            "Iteration 7205/10000, Loss: 0.024797137826681137\n",
            "Iteration 7206/10000, Loss: 0.0247963834553957\n",
            "Iteration 7207/10000, Loss: 0.024795636534690857\n",
            "Iteration 7208/10000, Loss: 0.024794884026050568\n",
            "Iteration 7209/10000, Loss: 0.024794137105345726\n",
            "Iteration 7210/10000, Loss: 0.024793388321995735\n",
            "Iteration 7211/10000, Loss: 0.024792637676000595\n",
            "Iteration 7212/10000, Loss: 0.024791888892650604\n",
            "Iteration 7213/10000, Loss: 0.024791141971945763\n",
            "Iteration 7214/10000, Loss: 0.024790393188595772\n",
            "Iteration 7215/10000, Loss: 0.02478964440524578\n",
            "Iteration 7216/10000, Loss: 0.02478889189660549\n",
            "Iteration 7217/10000, Loss: 0.0247881431132555\n",
            "Iteration 7218/10000, Loss: 0.02478739432990551\n",
            "Iteration 7219/10000, Loss: 0.02478664554655552\n",
            "Iteration 7220/10000, Loss: 0.024785898625850677\n",
            "Iteration 7221/10000, Loss: 0.024785149842500687\n",
            "Iteration 7222/10000, Loss: 0.024784397333860397\n",
            "Iteration 7223/10000, Loss: 0.024783650413155556\n",
            "Iteration 7224/10000, Loss: 0.024782903492450714\n",
            "Iteration 7225/10000, Loss: 0.024782152846455574\n",
            "Iteration 7226/10000, Loss: 0.024781404063105583\n",
            "Iteration 7227/10000, Loss: 0.024780653417110443\n",
            "Iteration 7228/10000, Loss: 0.024779904633760452\n",
            "Iteration 7229/10000, Loss: 0.02477915957570076\n",
            "Iteration 7230/10000, Loss: 0.02477840706706047\n",
            "Iteration 7231/10000, Loss: 0.02477765828371048\n",
            "Iteration 7232/10000, Loss: 0.02477690763771534\n",
            "Iteration 7233/10000, Loss: 0.024776160717010498\n",
            "Iteration 7234/10000, Loss: 0.024775413796305656\n",
            "Iteration 7235/10000, Loss: 0.024774666875600815\n",
            "Iteration 7236/10000, Loss: 0.024773916229605675\n",
            "Iteration 7237/10000, Loss: 0.024773169308900833\n",
            "Iteration 7238/10000, Loss: 0.024772416800260544\n",
            "Iteration 7239/10000, Loss: 0.024771669879555702\n",
            "Iteration 7240/10000, Loss: 0.02477092109620571\n",
            "Iteration 7241/10000, Loss: 0.02477017231285572\n",
            "Iteration 7242/10000, Loss: 0.02476942352950573\n",
            "Iteration 7243/10000, Loss: 0.024768676608800888\n",
            "Iteration 7244/10000, Loss: 0.024767927825450897\n",
            "Iteration 7245/10000, Loss: 0.024767180904746056\n",
            "Iteration 7246/10000, Loss: 0.024766430258750916\n",
            "Iteration 7247/10000, Loss: 0.024765683338046074\n",
            "Iteration 7248/10000, Loss: 0.024764936417341232\n",
            "Iteration 7249/10000, Loss: 0.024764185771346092\n",
            "Iteration 7250/10000, Loss: 0.02476343885064125\n",
            "Iteration 7251/10000, Loss: 0.024762693792581558\n",
            "Iteration 7252/10000, Loss: 0.024761943146586418\n",
            "Iteration 7253/10000, Loss: 0.024761196225881577\n",
            "Iteration 7254/10000, Loss: 0.024760445579886436\n",
            "Iteration 7255/10000, Loss: 0.024759694933891296\n",
            "Iteration 7256/10000, Loss: 0.024758948013186455\n",
            "Iteration 7257/10000, Loss: 0.024758199229836464\n",
            "Iteration 7258/10000, Loss: 0.024757452309131622\n",
            "Iteration 7259/10000, Loss: 0.02475670352578163\n",
            "Iteration 7260/10000, Loss: 0.02475595660507679\n",
            "Iteration 7261/10000, Loss: 0.0247552078217268\n",
            "Iteration 7262/10000, Loss: 0.024754459038376808\n",
            "Iteration 7263/10000, Loss: 0.024753712117671967\n",
            "Iteration 7264/10000, Loss: 0.024752963334321976\n",
            "Iteration 7265/10000, Loss: 0.024752216413617134\n",
            "Iteration 7266/10000, Loss: 0.024751467630267143\n",
            "Iteration 7267/10000, Loss: 0.0247507207095623\n",
            "Iteration 7268/10000, Loss: 0.02474997192621231\n",
            "Iteration 7269/10000, Loss: 0.02474922314286232\n",
            "Iteration 7270/10000, Loss: 0.02474847622215748\n",
            "Iteration 7271/10000, Loss: 0.024747727438807487\n",
            "Iteration 7272/10000, Loss: 0.024746978655457497\n",
            "Iteration 7273/10000, Loss: 0.024746231734752655\n",
            "Iteration 7274/10000, Loss: 0.024745486676692963\n",
            "Iteration 7275/10000, Loss: 0.024744736030697823\n",
            "Iteration 7276/10000, Loss: 0.02474398724734783\n",
            "Iteration 7277/10000, Loss: 0.02474324032664299\n",
            "Iteration 7278/10000, Loss: 0.024742491543293\n",
            "Iteration 7279/10000, Loss: 0.02474174275994301\n",
            "Iteration 7280/10000, Loss: 0.024740995839238167\n",
            "Iteration 7281/10000, Loss: 0.024740247055888176\n",
            "Iteration 7282/10000, Loss: 0.024739501997828484\n",
            "Iteration 7283/10000, Loss: 0.024738751351833344\n",
            "Iteration 7284/10000, Loss: 0.02473800629377365\n",
            "Iteration 7285/10000, Loss: 0.02473725937306881\n",
            "Iteration 7286/10000, Loss: 0.02473651058971882\n",
            "Iteration 7287/10000, Loss: 0.024735761806368828\n",
            "Iteration 7288/10000, Loss: 0.024735016748309135\n",
            "Iteration 7289/10000, Loss: 0.024734266102313995\n",
            "Iteration 7290/10000, Loss: 0.024733521044254303\n",
            "Iteration 7291/10000, Loss: 0.024732772260904312\n",
            "Iteration 7292/10000, Loss: 0.02473202347755432\n",
            "Iteration 7293/10000, Loss: 0.02473127841949463\n",
            "Iteration 7294/10000, Loss: 0.024730531498789787\n",
            "Iteration 7295/10000, Loss: 0.024729782715439796\n",
            "Iteration 7296/10000, Loss: 0.024729037657380104\n",
            "Iteration 7297/10000, Loss: 0.024728285148739815\n",
            "Iteration 7298/10000, Loss: 0.02472754195332527\n",
            "Iteration 7299/10000, Loss: 0.02472679316997528\n",
            "Iteration 7300/10000, Loss: 0.02472604624927044\n",
            "Iteration 7301/10000, Loss: 0.024725299328565598\n",
            "Iteration 7302/10000, Loss: 0.024724550545215607\n",
            "Iteration 7303/10000, Loss: 0.024723805487155914\n",
            "Iteration 7304/10000, Loss: 0.024723058566451073\n",
            "Iteration 7305/10000, Loss: 0.024722309783101082\n",
            "Iteration 7306/10000, Loss: 0.02472156099975109\n",
            "Iteration 7307/10000, Loss: 0.024720817804336548\n",
            "Iteration 7308/10000, Loss: 0.024720070883631706\n",
            "Iteration 7309/10000, Loss: 0.024719323962926865\n",
            "Iteration 7310/10000, Loss: 0.024718575179576874\n",
            "Iteration 7311/10000, Loss: 0.024717828258872032\n",
            "Iteration 7312/10000, Loss: 0.02471707947552204\n",
            "Iteration 7313/10000, Loss: 0.02471633441746235\n",
            "Iteration 7314/10000, Loss: 0.024715585634112358\n",
            "Iteration 7315/10000, Loss: 0.024714840576052666\n",
            "Iteration 7316/10000, Loss: 0.024714093655347824\n",
            "Iteration 7317/10000, Loss: 0.024713346734642982\n",
            "Iteration 7318/10000, Loss: 0.024712596088647842\n",
            "Iteration 7319/10000, Loss: 0.0247118528932333\n",
            "Iteration 7320/10000, Loss: 0.02471110410988331\n",
            "Iteration 7321/10000, Loss: 0.024710359051823616\n",
            "Iteration 7322/10000, Loss: 0.024709610268473625\n",
            "Iteration 7323/10000, Loss: 0.024708861485123634\n",
            "Iteration 7324/10000, Loss: 0.024708116427063942\n",
            "Iteration 7325/10000, Loss: 0.02470737136900425\n",
            "Iteration 7326/10000, Loss: 0.02470662258565426\n",
            "Iteration 7327/10000, Loss: 0.024705877527594566\n",
            "Iteration 7328/10000, Loss: 0.024705128744244576\n",
            "Iteration 7329/10000, Loss: 0.024704381823539734\n",
            "Iteration 7330/10000, Loss: 0.02470363862812519\n",
            "Iteration 7331/10000, Loss: 0.02470289170742035\n",
            "Iteration 7332/10000, Loss: 0.024702146649360657\n",
            "Iteration 7333/10000, Loss: 0.024701396003365517\n",
            "Iteration 7334/10000, Loss: 0.024700650945305824\n",
            "Iteration 7335/10000, Loss: 0.02469990774989128\n",
            "Iteration 7336/10000, Loss: 0.02469916082918644\n",
            "Iteration 7337/10000, Loss: 0.02469841204583645\n",
            "Iteration 7338/10000, Loss: 0.024697666987776756\n",
            "Iteration 7339/10000, Loss: 0.024696921929717064\n",
            "Iteration 7340/10000, Loss: 0.024696175009012222\n",
            "Iteration 7341/10000, Loss: 0.02469543181359768\n",
            "Iteration 7342/10000, Loss: 0.02469468303024769\n",
            "Iteration 7343/10000, Loss: 0.024693936109542847\n",
            "Iteration 7344/10000, Loss: 0.024693191051483154\n",
            "Iteration 7345/10000, Loss: 0.024692445993423462\n",
            "Iteration 7346/10000, Loss: 0.02469169907271862\n",
            "Iteration 7347/10000, Loss: 0.024690955877304077\n",
            "Iteration 7348/10000, Loss: 0.024690207093954086\n",
            "Iteration 7349/10000, Loss: 0.024689463898539543\n",
            "Iteration 7350/10000, Loss: 0.0246887169778347\n",
            "Iteration 7351/10000, Loss: 0.02468797005712986\n",
            "Iteration 7352/10000, Loss: 0.02468722127377987\n",
            "Iteration 7353/10000, Loss: 0.024686478078365326\n",
            "Iteration 7354/10000, Loss: 0.024685733020305634\n",
            "Iteration 7355/10000, Loss: 0.024684986099600792\n",
            "Iteration 7356/10000, Loss: 0.02468423917889595\n",
            "Iteration 7357/10000, Loss: 0.024683495983481407\n",
            "Iteration 7358/10000, Loss: 0.024682750925421715\n",
            "Iteration 7359/10000, Loss: 0.024682005867362022\n",
            "Iteration 7360/10000, Loss: 0.024681255221366882\n",
            "Iteration 7361/10000, Loss: 0.02468051202595234\n",
            "Iteration 7362/10000, Loss: 0.024679768830537796\n",
            "Iteration 7363/10000, Loss: 0.024679023772478104\n",
            "Iteration 7364/10000, Loss: 0.024678274989128113\n",
            "Iteration 7365/10000, Loss: 0.02467753179371357\n",
            "Iteration 7366/10000, Loss: 0.024676786735653877\n",
            "Iteration 7367/10000, Loss: 0.024676039814949036\n",
            "Iteration 7368/10000, Loss: 0.024675294756889343\n",
            "Iteration 7369/10000, Loss: 0.0246745478361845\n",
            "Iteration 7370/10000, Loss: 0.02467380277812481\n",
            "Iteration 7371/10000, Loss: 0.024673059582710266\n",
            "Iteration 7372/10000, Loss: 0.024672312662005424\n",
            "Iteration 7373/10000, Loss: 0.024671567603945732\n",
            "Iteration 7374/10000, Loss: 0.02467082254588604\n",
            "Iteration 7375/10000, Loss: 0.0246700718998909\n",
            "Iteration 7376/10000, Loss: 0.024669328704476357\n",
            "Iteration 7377/10000, Loss: 0.024668583646416664\n",
            "Iteration 7378/10000, Loss: 0.02466784417629242\n",
            "Iteration 7379/10000, Loss: 0.02466709353029728\n",
            "Iteration 7380/10000, Loss: 0.024666348472237587\n",
            "Iteration 7381/10000, Loss: 0.024665603414177895\n",
            "Iteration 7382/10000, Loss: 0.024664858356118202\n",
            "Iteration 7383/10000, Loss: 0.02466411329805851\n",
            "Iteration 7384/10000, Loss: 0.024663368239998817\n",
            "Iteration 7385/10000, Loss: 0.024662623181939125\n",
            "Iteration 7386/10000, Loss: 0.024661876261234283\n",
            "Iteration 7387/10000, Loss: 0.02466113306581974\n",
            "Iteration 7388/10000, Loss: 0.024660388007760048\n",
            "Iteration 7389/10000, Loss: 0.024659644812345505\n",
            "Iteration 7390/10000, Loss: 0.024658896028995514\n",
            "Iteration 7391/10000, Loss: 0.02465815283358097\n",
            "Iteration 7392/10000, Loss: 0.02465740591287613\n",
            "Iteration 7393/10000, Loss: 0.024656662717461586\n",
            "Iteration 7394/10000, Loss: 0.024655919522047043\n",
            "Iteration 7395/10000, Loss: 0.02465517446398735\n",
            "Iteration 7396/10000, Loss: 0.02465442568063736\n",
            "Iteration 7397/10000, Loss: 0.024653682485222816\n",
            "Iteration 7398/10000, Loss: 0.024652935564517975\n",
            "Iteration 7399/10000, Loss: 0.02465219236910343\n",
            "Iteration 7400/10000, Loss: 0.02465144544839859\n",
            "Iteration 7401/10000, Loss: 0.024650700390338898\n",
            "Iteration 7402/10000, Loss: 0.024649959057569504\n",
            "Iteration 7403/10000, Loss: 0.024649212136864662\n",
            "Iteration 7404/10000, Loss: 0.02464846894145012\n",
            "Iteration 7405/10000, Loss: 0.024647723883390427\n",
            "Iteration 7406/10000, Loss: 0.024646976962685585\n",
            "Iteration 7407/10000, Loss: 0.024646231904625893\n",
            "Iteration 7408/10000, Loss: 0.0246454868465662\n",
            "Iteration 7409/10000, Loss: 0.024644743651151657\n",
            "Iteration 7410/10000, Loss: 0.024644000455737114\n",
            "Iteration 7411/10000, Loss: 0.02464325539767742\n",
            "Iteration 7412/10000, Loss: 0.02464251220226288\n",
            "Iteration 7413/10000, Loss: 0.024641769006848335\n",
            "Iteration 7414/10000, Loss: 0.024641022086143494\n",
            "Iteration 7415/10000, Loss: 0.02464027889072895\n",
            "Iteration 7416/10000, Loss: 0.02463953197002411\n",
            "Iteration 7417/10000, Loss: 0.024638790637254715\n",
            "Iteration 7418/10000, Loss: 0.024638041853904724\n",
            "Iteration 7419/10000, Loss: 0.02463730052113533\n",
            "Iteration 7420/10000, Loss: 0.02463655360043049\n",
            "Iteration 7421/10000, Loss: 0.024635812267661095\n",
            "Iteration 7422/10000, Loss: 0.024635067209601402\n",
            "Iteration 7423/10000, Loss: 0.02463432401418686\n",
            "Iteration 7424/10000, Loss: 0.024633577093482018\n",
            "Iteration 7425/10000, Loss: 0.024632830172777176\n",
            "Iteration 7426/10000, Loss: 0.024632088840007782\n",
            "Iteration 7427/10000, Loss: 0.02463134378194809\n",
            "Iteration 7428/10000, Loss: 0.024630596861243248\n",
            "Iteration 7429/10000, Loss: 0.024629855528473854\n",
            "Iteration 7430/10000, Loss: 0.02462911233305931\n",
            "Iteration 7431/10000, Loss: 0.02462836541235447\n",
            "Iteration 7432/10000, Loss: 0.024627622216939926\n",
            "Iteration 7433/10000, Loss: 0.024626875296235085\n",
            "Iteration 7434/10000, Loss: 0.02462613582611084\n",
            "Iteration 7435/10000, Loss: 0.024625388905405998\n",
            "Iteration 7436/10000, Loss: 0.024624645709991455\n",
            "Iteration 7437/10000, Loss: 0.02462390437722206\n",
            "Iteration 7438/10000, Loss: 0.02462315745651722\n",
            "Iteration 7439/10000, Loss: 0.024622412398457527\n",
            "Iteration 7440/10000, Loss: 0.024621672928333282\n",
            "Iteration 7441/10000, Loss: 0.02462092600762844\n",
            "Iteration 7442/10000, Loss: 0.024620182812213898\n",
            "Iteration 7443/10000, Loss: 0.024619435891509056\n",
            "Iteration 7444/10000, Loss: 0.024618692696094513\n",
            "Iteration 7445/10000, Loss: 0.02461795136332512\n",
            "Iteration 7446/10000, Loss: 0.024617206305265427\n",
            "Iteration 7447/10000, Loss: 0.024616461247205734\n",
            "Iteration 7448/10000, Loss: 0.024615714326500893\n",
            "Iteration 7449/10000, Loss: 0.02461497113108635\n",
            "Iteration 7450/10000, Loss: 0.024614229798316956\n",
            "Iteration 7451/10000, Loss: 0.024613486602902412\n",
            "Iteration 7452/10000, Loss: 0.02461274154484272\n",
            "Iteration 7453/10000, Loss: 0.024612002074718475\n",
            "Iteration 7454/10000, Loss: 0.024611255154013634\n",
            "Iteration 7455/10000, Loss: 0.02461051195859909\n",
            "Iteration 7456/10000, Loss: 0.024609766900539398\n",
            "Iteration 7457/10000, Loss: 0.024609025567770004\n",
            "Iteration 7458/10000, Loss: 0.02460828796029091\n",
            "Iteration 7459/10000, Loss: 0.024607546627521515\n",
            "Iteration 7460/10000, Loss: 0.024606801569461823\n",
            "Iteration 7461/10000, Loss: 0.024606062099337578\n",
            "Iteration 7462/10000, Loss: 0.024605320766568184\n",
            "Iteration 7463/10000, Loss: 0.02460457757115364\n",
            "Iteration 7464/10000, Loss: 0.024603838101029396\n",
            "Iteration 7465/10000, Loss: 0.024603094905614853\n",
            "Iteration 7466/10000, Loss: 0.024602355435490608\n",
            "Iteration 7467/10000, Loss: 0.024601614102721214\n",
            "Iteration 7468/10000, Loss: 0.02460087463259697\n",
            "Iteration 7469/10000, Loss: 0.024600133299827576\n",
            "Iteration 7470/10000, Loss: 0.024599390104413033\n",
            "Iteration 7471/10000, Loss: 0.024598650634288788\n",
            "Iteration 7472/10000, Loss: 0.024597909301519394\n",
            "Iteration 7473/10000, Loss: 0.02459716796875\n",
            "Iteration 7474/10000, Loss: 0.024596426635980606\n",
            "Iteration 7475/10000, Loss: 0.024595683440566063\n",
            "Iteration 7476/10000, Loss: 0.024594945833086967\n",
            "Iteration 7477/10000, Loss: 0.024594202637672424\n",
            "Iteration 7478/10000, Loss: 0.02459346316754818\n",
            "Iteration 7479/10000, Loss: 0.024592721834778786\n",
            "Iteration 7480/10000, Loss: 0.024591980502009392\n",
            "Iteration 7481/10000, Loss: 0.024591241031885147\n",
            "Iteration 7482/10000, Loss: 0.024590497836470604\n",
            "Iteration 7483/10000, Loss: 0.02458975836634636\n",
            "Iteration 7484/10000, Loss: 0.024589015170931816\n",
            "Iteration 7485/10000, Loss: 0.02458827756345272\n",
            "Iteration 7486/10000, Loss: 0.024587534368038177\n",
            "Iteration 7487/10000, Loss: 0.024586794897913933\n",
            "Iteration 7488/10000, Loss: 0.024586055427789688\n",
            "Iteration 7489/10000, Loss: 0.024585314095020294\n",
            "Iteration 7490/10000, Loss: 0.02458457089960575\n",
            "Iteration 7491/10000, Loss: 0.024583829566836357\n",
            "Iteration 7492/10000, Loss: 0.024583088234066963\n",
            "Iteration 7493/10000, Loss: 0.02458234876394272\n",
            "Iteration 7494/10000, Loss: 0.024581609293818474\n",
            "Iteration 7495/10000, Loss: 0.02458086982369423\n",
            "Iteration 7496/10000, Loss: 0.024580124765634537\n",
            "Iteration 7497/10000, Loss: 0.024579385295510292\n",
            "Iteration 7498/10000, Loss: 0.024578645825386047\n",
            "Iteration 7499/10000, Loss: 0.024577904492616653\n",
            "Iteration 7500/10000, Loss: 0.024577166885137558\n",
            "Iteration 7501/10000, Loss: 0.024576423689723015\n",
            "Iteration 7502/10000, Loss: 0.02457568235695362\n",
            "Iteration 7503/10000, Loss: 0.024574942886829376\n",
            "Iteration 7504/10000, Loss: 0.02457420527935028\n",
            "Iteration 7505/10000, Loss: 0.02457346022129059\n",
            "Iteration 7506/10000, Loss: 0.024572718888521194\n",
            "Iteration 7507/10000, Loss: 0.0245719812810421\n",
            "Iteration 7508/10000, Loss: 0.024571239948272705\n",
            "Iteration 7509/10000, Loss: 0.02457050047814846\n",
            "Iteration 7510/10000, Loss: 0.024569757282733917\n",
            "Iteration 7511/10000, Loss: 0.024569015949964523\n",
            "Iteration 7512/10000, Loss: 0.024568278342485428\n",
            "Iteration 7513/10000, Loss: 0.024567538872361183\n",
            "Iteration 7514/10000, Loss: 0.02456679940223694\n",
            "Iteration 7515/10000, Loss: 0.024566058069467545\n",
            "Iteration 7516/10000, Loss: 0.02456532046198845\n",
            "Iteration 7517/10000, Loss: 0.024564579129219055\n",
            "Iteration 7518/10000, Loss: 0.02456383965909481\n",
            "Iteration 7519/10000, Loss: 0.024563096463680267\n",
            "Iteration 7520/10000, Loss: 0.02456236258149147\n",
            "Iteration 7521/10000, Loss: 0.024561623111367226\n",
            "Iteration 7522/10000, Loss: 0.02456088364124298\n",
            "Iteration 7523/10000, Loss: 0.024560140445828438\n",
            "Iteration 7524/10000, Loss: 0.024559402838349342\n",
            "Iteration 7525/10000, Loss: 0.024558663368225098\n",
            "Iteration 7526/10000, Loss: 0.024557923898100853\n",
            "Iteration 7527/10000, Loss: 0.02455718442797661\n",
            "Iteration 7528/10000, Loss: 0.024556443095207214\n",
            "Iteration 7529/10000, Loss: 0.024555707350373268\n",
            "Iteration 7530/10000, Loss: 0.024554964154958725\n",
            "Iteration 7531/10000, Loss: 0.02455422468483448\n",
            "Iteration 7532/10000, Loss: 0.024553487077355385\n",
            "Iteration 7533/10000, Loss: 0.024552742019295692\n",
            "Iteration 7534/10000, Loss: 0.024552008137106895\n",
            "Iteration 7535/10000, Loss: 0.02455126866698265\n",
            "Iteration 7536/10000, Loss: 0.024550529196858406\n",
            "Iteration 7537/10000, Loss: 0.024549787864089012\n",
            "Iteration 7538/10000, Loss: 0.024549050256609917\n",
            "Iteration 7539/10000, Loss: 0.02454831264913082\n",
            "Iteration 7540/10000, Loss: 0.024547571316361427\n",
            "Iteration 7541/10000, Loss: 0.024546831846237183\n",
            "Iteration 7542/10000, Loss: 0.02454609051346779\n",
            "Iteration 7543/10000, Loss: 0.02454535663127899\n",
            "Iteration 7544/10000, Loss: 0.0245446115732193\n",
            "Iteration 7545/10000, Loss: 0.024543873965740204\n",
            "Iteration 7546/10000, Loss: 0.02454313263297081\n",
            "Iteration 7547/10000, Loss: 0.024542396888136864\n",
            "Iteration 7548/10000, Loss: 0.02454165741801262\n",
            "Iteration 7549/10000, Loss: 0.024540917947888374\n",
            "Iteration 7550/10000, Loss: 0.02454017847776413\n",
            "Iteration 7551/10000, Loss: 0.024539439007639885\n",
            "Iteration 7552/10000, Loss: 0.02453870140016079\n",
            "Iteration 7553/10000, Loss: 0.024537960067391396\n",
            "Iteration 7554/10000, Loss: 0.0245372224599123\n",
            "Iteration 7555/10000, Loss: 0.024536481127142906\n",
            "Iteration 7556/10000, Loss: 0.02453574165701866\n",
            "Iteration 7557/10000, Loss: 0.024535005912184715\n",
            "Iteration 7558/10000, Loss: 0.024534262716770172\n",
            "Iteration 7559/10000, Loss: 0.024533526971936226\n",
            "Iteration 7560/10000, Loss: 0.02453278936445713\n",
            "Iteration 7561/10000, Loss: 0.024532049894332886\n",
            "Iteration 7562/10000, Loss: 0.02453131042420864\n",
            "Iteration 7563/10000, Loss: 0.024530570954084396\n",
            "Iteration 7564/10000, Loss: 0.0245298333466053\n",
            "Iteration 7565/10000, Loss: 0.024529092013835907\n",
            "Iteration 7566/10000, Loss: 0.024528352543711662\n",
            "Iteration 7567/10000, Loss: 0.024527614936232567\n",
            "Iteration 7568/10000, Loss: 0.024526875466108322\n",
            "Iteration 7569/10000, Loss: 0.024526137858629227\n",
            "Iteration 7570/10000, Loss: 0.024525398388504982\n",
            "Iteration 7571/10000, Loss: 0.024524660781025887\n",
            "Iteration 7572/10000, Loss: 0.024523921310901642\n",
            "Iteration 7573/10000, Loss: 0.024523181840777397\n",
            "Iteration 7574/10000, Loss: 0.0245224442332983\n",
            "Iteration 7575/10000, Loss: 0.024521706625819206\n",
            "Iteration 7576/10000, Loss: 0.024520965293049812\n",
            "Iteration 7577/10000, Loss: 0.024520227685570717\n",
            "Iteration 7578/10000, Loss: 0.02451949194073677\n",
            "Iteration 7579/10000, Loss: 0.024518754333257675\n",
            "Iteration 7580/10000, Loss: 0.02451801300048828\n",
            "Iteration 7581/10000, Loss: 0.024517273530364037\n",
            "Iteration 7582/10000, Loss: 0.02451653592288494\n",
            "Iteration 7583/10000, Loss: 0.024515798315405846\n",
            "Iteration 7584/10000, Loss: 0.02451506070792675\n",
            "Iteration 7585/10000, Loss: 0.024514323100447655\n",
            "Iteration 7586/10000, Loss: 0.02451358176767826\n",
            "Iteration 7587/10000, Loss: 0.024512846022844315\n",
            "Iteration 7588/10000, Loss: 0.02451210655272007\n",
            "Iteration 7589/10000, Loss: 0.024511368945240974\n",
            "Iteration 7590/10000, Loss: 0.02451062947511673\n",
            "Iteration 7591/10000, Loss: 0.024509890004992485\n",
            "Iteration 7592/10000, Loss: 0.02450915239751339\n",
            "Iteration 7593/10000, Loss: 0.024508416652679443\n",
            "Iteration 7594/10000, Loss: 0.02450767531991005\n",
            "Iteration 7595/10000, Loss: 0.024506937712430954\n",
            "Iteration 7596/10000, Loss: 0.024506201967597008\n",
            "Iteration 7597/10000, Loss: 0.024505462497472763\n",
            "Iteration 7598/10000, Loss: 0.024504724889993668\n",
            "Iteration 7599/10000, Loss: 0.024503983557224274\n",
            "Iteration 7600/10000, Loss: 0.024503247812390327\n",
            "Iteration 7601/10000, Loss: 0.024502510204911232\n",
            "Iteration 7602/10000, Loss: 0.024501772597432137\n",
            "Iteration 7603/10000, Loss: 0.024501033127307892\n",
            "Iteration 7604/10000, Loss: 0.024500293657183647\n",
            "Iteration 7605/10000, Loss: 0.0244995579123497\n",
            "Iteration 7606/10000, Loss: 0.024498818442225456\n",
            "Iteration 7607/10000, Loss: 0.02449807897210121\n",
            "Iteration 7608/10000, Loss: 0.024497341364622116\n",
            "Iteration 7609/10000, Loss: 0.02449660375714302\n",
            "Iteration 7610/10000, Loss: 0.024495866149663925\n",
            "Iteration 7611/10000, Loss: 0.02449513040482998\n",
            "Iteration 7612/10000, Loss: 0.024494389072060585\n",
            "Iteration 7613/10000, Loss: 0.02449365146458149\n",
            "Iteration 7614/10000, Loss: 0.024492913857102394\n",
            "Iteration 7615/10000, Loss: 0.024492178112268448\n",
            "Iteration 7616/10000, Loss: 0.0244914423674345\n",
            "Iteration 7617/10000, Loss: 0.024490702897310257\n",
            "Iteration 7618/10000, Loss: 0.02448996528983116\n",
            "Iteration 7619/10000, Loss: 0.024489225819706917\n",
            "Iteration 7620/10000, Loss: 0.02448848821222782\n",
            "Iteration 7621/10000, Loss: 0.024487748742103577\n",
            "Iteration 7622/10000, Loss: 0.02448701113462448\n",
            "Iteration 7623/10000, Loss: 0.024486273527145386\n",
            "Iteration 7624/10000, Loss: 0.02448553591966629\n",
            "Iteration 7625/10000, Loss: 0.024484798312187195\n",
            "Iteration 7626/10000, Loss: 0.0244840607047081\n",
            "Iteration 7627/10000, Loss: 0.024483324959874153\n",
            "Iteration 7628/10000, Loss: 0.024482591077685356\n",
            "Iteration 7629/10000, Loss: 0.024481847882270813\n",
            "Iteration 7630/10000, Loss: 0.024481110274791718\n",
            "Iteration 7631/10000, Loss: 0.02448037452995777\n",
            "Iteration 7632/10000, Loss: 0.024479636922478676\n",
            "Iteration 7633/10000, Loss: 0.02447890117764473\n",
            "Iteration 7634/10000, Loss: 0.024478161707520485\n",
            "Iteration 7635/10000, Loss: 0.02447742410004139\n",
            "Iteration 7636/10000, Loss: 0.024476688355207443\n",
            "Iteration 7637/10000, Loss: 0.024475950747728348\n",
            "Iteration 7638/10000, Loss: 0.024475211277604103\n",
            "Iteration 7639/10000, Loss: 0.024474473670125008\n",
            "Iteration 7640/10000, Loss: 0.02447373978793621\n",
            "Iteration 7641/10000, Loss: 0.024473000317811966\n",
            "Iteration 7642/10000, Loss: 0.02447226457297802\n",
            "Iteration 7643/10000, Loss: 0.024471526965498924\n",
            "Iteration 7644/10000, Loss: 0.02447078749537468\n",
            "Iteration 7645/10000, Loss: 0.024470051750540733\n",
            "Iteration 7646/10000, Loss: 0.024469316005706787\n",
            "Iteration 7647/10000, Loss: 0.024468576535582542\n",
            "Iteration 7648/10000, Loss: 0.024467838928103447\n",
            "Iteration 7649/10000, Loss: 0.0244671031832695\n",
            "Iteration 7650/10000, Loss: 0.024466365575790405\n",
            "Iteration 7651/10000, Loss: 0.02446562983095646\n",
            "Iteration 7652/10000, Loss: 0.024464890360832214\n",
            "Iteration 7653/10000, Loss: 0.024464154615998268\n",
            "Iteration 7654/10000, Loss: 0.024463418871164322\n",
            "Iteration 7655/10000, Loss: 0.024462679401040077\n",
            "Iteration 7656/10000, Loss: 0.02446194365620613\n",
            "Iteration 7657/10000, Loss: 0.024461206048727036\n",
            "Iteration 7658/10000, Loss: 0.02446047030389309\n",
            "Iteration 7659/10000, Loss: 0.024459732696413994\n",
            "Iteration 7660/10000, Loss: 0.02445899322628975\n",
            "Iteration 7661/10000, Loss: 0.024458257481455803\n",
            "Iteration 7662/10000, Loss: 0.024457521736621857\n",
            "Iteration 7663/10000, Loss: 0.02445678412914276\n",
            "Iteration 7664/10000, Loss: 0.024456048384308815\n",
            "Iteration 7665/10000, Loss: 0.02445531077682972\n",
            "Iteration 7666/10000, Loss: 0.024454573169350624\n",
            "Iteration 7667/10000, Loss: 0.024453839287161827\n",
            "Iteration 7668/10000, Loss: 0.02445310167968273\n",
            "Iteration 7669/10000, Loss: 0.024452364072203636\n",
            "Iteration 7670/10000, Loss: 0.02445162460207939\n",
            "Iteration 7671/10000, Loss: 0.024450892582535744\n",
            "Iteration 7672/10000, Loss: 0.0244501531124115\n",
            "Iteration 7673/10000, Loss: 0.024449417367577553\n",
            "Iteration 7674/10000, Loss: 0.024448677897453308\n",
            "Iteration 7675/10000, Loss: 0.02444794401526451\n",
            "Iteration 7676/10000, Loss: 0.024447206407785416\n",
            "Iteration 7677/10000, Loss: 0.02444646880030632\n",
            "Iteration 7678/10000, Loss: 0.024445733055472374\n",
            "Iteration 7679/10000, Loss: 0.02444499544799328\n",
            "Iteration 7680/10000, Loss: 0.024444259703159332\n",
            "Iteration 7681/10000, Loss: 0.024443525820970535\n",
            "Iteration 7682/10000, Loss: 0.02444278821349144\n",
            "Iteration 7683/10000, Loss: 0.024442050606012344\n",
            "Iteration 7684/10000, Loss: 0.02444131299853325\n",
            "Iteration 7685/10000, Loss: 0.024440577253699303\n",
            "Iteration 7686/10000, Loss: 0.024439839646220207\n",
            "Iteration 7687/10000, Loss: 0.02443910390138626\n",
            "Iteration 7688/10000, Loss: 0.024438368156552315\n",
            "Iteration 7689/10000, Loss: 0.02443763241171837\n",
            "Iteration 7690/10000, Loss: 0.024436894804239273\n",
            "Iteration 7691/10000, Loss: 0.024436159059405327\n",
            "Iteration 7692/10000, Loss: 0.02443542331457138\n",
            "Iteration 7693/10000, Loss: 0.024434689432382584\n",
            "Iteration 7694/10000, Loss: 0.024433951824903488\n",
            "Iteration 7695/10000, Loss: 0.024433214217424393\n",
            "Iteration 7696/10000, Loss: 0.024432478472590446\n",
            "Iteration 7697/10000, Loss: 0.0244317464530468\n",
            "Iteration 7698/10000, Loss: 0.024431006982922554\n",
            "Iteration 7699/10000, Loss: 0.02443026937544346\n",
            "Iteration 7700/10000, Loss: 0.02442953735589981\n",
            "Iteration 7701/10000, Loss: 0.024428797885775566\n",
            "Iteration 7702/10000, Loss: 0.02442806400358677\n",
            "Iteration 7703/10000, Loss: 0.024427328258752823\n",
            "Iteration 7704/10000, Loss: 0.024426594376564026\n",
            "Iteration 7705/10000, Loss: 0.02442585490643978\n",
            "Iteration 7706/10000, Loss: 0.024425121024250984\n",
            "Iteration 7707/10000, Loss: 0.024424387142062187\n",
            "Iteration 7708/10000, Loss: 0.02442365139722824\n",
            "Iteration 7709/10000, Loss: 0.024422913789749146\n",
            "Iteration 7710/10000, Loss: 0.02442217990756035\n",
            "Iteration 7711/10000, Loss: 0.02442144602537155\n",
            "Iteration 7712/10000, Loss: 0.024420710280537605\n",
            "Iteration 7713/10000, Loss: 0.02441997453570366\n",
            "Iteration 7714/10000, Loss: 0.024419235065579414\n",
            "Iteration 7715/10000, Loss: 0.024418503046035767\n",
            "Iteration 7716/10000, Loss: 0.02441776916384697\n",
            "Iteration 7717/10000, Loss: 0.024417031556367874\n",
            "Iteration 7718/10000, Loss: 0.02441629394888878\n",
            "Iteration 7719/10000, Loss: 0.02441556379199028\n",
            "Iteration 7720/10000, Loss: 0.024414826184511185\n",
            "Iteration 7721/10000, Loss: 0.02441409043967724\n",
            "Iteration 7722/10000, Loss: 0.024413354694843292\n",
            "Iteration 7723/10000, Loss: 0.024412620812654495\n",
            "Iteration 7724/10000, Loss: 0.02441188506782055\n",
            "Iteration 7725/10000, Loss: 0.024411149322986603\n",
            "Iteration 7726/10000, Loss: 0.024410417303442955\n",
            "Iteration 7727/10000, Loss: 0.024409683421254158\n",
            "Iteration 7728/10000, Loss: 0.02440894953906536\n",
            "Iteration 7729/10000, Loss: 0.024408219382166862\n",
            "Iteration 7730/10000, Loss: 0.024407485499978065\n",
            "Iteration 7731/10000, Loss: 0.02440675161778927\n",
            "Iteration 7732/10000, Loss: 0.02440601959824562\n",
            "Iteration 7733/10000, Loss: 0.024405289441347122\n",
            "Iteration 7734/10000, Loss: 0.024404551833868027\n",
            "Iteration 7735/10000, Loss: 0.024403823539614677\n",
            "Iteration 7736/10000, Loss: 0.02440308779478073\n",
            "Iteration 7737/10000, Loss: 0.024402355775237083\n",
            "Iteration 7738/10000, Loss: 0.024401623755693436\n",
            "Iteration 7739/10000, Loss: 0.02440088987350464\n",
            "Iteration 7740/10000, Loss: 0.02440015785396099\n",
            "Iteration 7741/10000, Loss: 0.024399425834417343\n",
            "Iteration 7742/10000, Loss: 0.024398693814873695\n",
            "Iteration 7743/10000, Loss: 0.0243979599326849\n",
            "Iteration 7744/10000, Loss: 0.0243972260504961\n",
            "Iteration 7745/10000, Loss: 0.024396495893597603\n",
            "Iteration 7746/10000, Loss: 0.024395762011408806\n",
            "Iteration 7747/10000, Loss: 0.024395033717155457\n",
            "Iteration 7748/10000, Loss: 0.02439429797232151\n",
            "Iteration 7749/10000, Loss: 0.024393565952777863\n",
            "Iteration 7750/10000, Loss: 0.024392833933234215\n",
            "Iteration 7751/10000, Loss: 0.024392101913690567\n",
            "Iteration 7752/10000, Loss: 0.02439136989414692\n",
            "Iteration 7753/10000, Loss: 0.024390636011958122\n",
            "Iteration 7754/10000, Loss: 0.024389905855059624\n",
            "Iteration 7755/10000, Loss: 0.024389171972870827\n",
            "Iteration 7756/10000, Loss: 0.02438843995332718\n",
            "Iteration 7757/10000, Loss: 0.02438770793378353\n",
            "Iteration 7758/10000, Loss: 0.024386975914239883\n",
            "Iteration 7759/10000, Loss: 0.024386243894696236\n",
            "Iteration 7760/10000, Loss: 0.024385511875152588\n",
            "Iteration 7761/10000, Loss: 0.02438477985560894\n",
            "Iteration 7762/10000, Loss: 0.024384047836065292\n",
            "Iteration 7763/10000, Loss: 0.024383315816521645\n",
            "Iteration 7764/10000, Loss: 0.024382583796977997\n",
            "Iteration 7765/10000, Loss: 0.0243818499147892\n",
            "Iteration 7766/10000, Loss: 0.024381117895245552\n",
            "Iteration 7767/10000, Loss: 0.024380387738347054\n",
            "Iteration 7768/10000, Loss: 0.024379653856158257\n",
            "Iteration 7769/10000, Loss: 0.02437891997396946\n",
            "Iteration 7770/10000, Loss: 0.024378187954425812\n",
            "Iteration 7771/10000, Loss: 0.024377459660172462\n",
            "Iteration 7772/10000, Loss: 0.024376725777983665\n",
            "Iteration 7773/10000, Loss: 0.024375993758440018\n",
            "Iteration 7774/10000, Loss: 0.02437526173889637\n",
            "Iteration 7775/10000, Loss: 0.02437453158199787\n",
            "Iteration 7776/10000, Loss: 0.024373799562454224\n",
            "Iteration 7777/10000, Loss: 0.024373065680265427\n",
            "Iteration 7778/10000, Loss: 0.02437233366072178\n",
            "Iteration 7779/10000, Loss: 0.024371599778532982\n",
            "Iteration 7780/10000, Loss: 0.024370869621634483\n",
            "Iteration 7781/10000, Loss: 0.024370137602090836\n",
            "Iteration 7782/10000, Loss: 0.024369407445192337\n",
            "Iteration 7783/10000, Loss: 0.02436867542564869\n",
            "Iteration 7784/10000, Loss: 0.02436794340610504\n",
            "Iteration 7785/10000, Loss: 0.024367211386561394\n",
            "Iteration 7786/10000, Loss: 0.024366479367017746\n",
            "Iteration 7787/10000, Loss: 0.024365747347474098\n",
            "Iteration 7788/10000, Loss: 0.0243650171905756\n",
            "Iteration 7789/10000, Loss: 0.024364285171031952\n",
            "Iteration 7790/10000, Loss: 0.024363553151488304\n",
            "Iteration 7791/10000, Loss: 0.024362821131944656\n",
            "Iteration 7792/10000, Loss: 0.024362090975046158\n",
            "Iteration 7793/10000, Loss: 0.02436135895550251\n",
            "Iteration 7794/10000, Loss: 0.02436062879860401\n",
            "Iteration 7795/10000, Loss: 0.024359894916415215\n",
            "Iteration 7796/10000, Loss: 0.024359162896871567\n",
            "Iteration 7797/10000, Loss: 0.02435843087732792\n",
            "Iteration 7798/10000, Loss: 0.02435770072042942\n",
            "Iteration 7799/10000, Loss: 0.024356968700885773\n",
            "Iteration 7800/10000, Loss: 0.024356236681342125\n",
            "Iteration 7801/10000, Loss: 0.024355506524443626\n",
            "Iteration 7802/10000, Loss: 0.024354776367545128\n",
            "Iteration 7803/10000, Loss: 0.02435404248535633\n",
            "Iteration 7804/10000, Loss: 0.024353312328457832\n",
            "Iteration 7805/10000, Loss: 0.024352584034204483\n",
            "Iteration 7806/10000, Loss: 0.024351850152015686\n",
            "Iteration 7807/10000, Loss: 0.024351119995117188\n",
            "Iteration 7808/10000, Loss: 0.02435038797557354\n",
            "Iteration 7809/10000, Loss: 0.024349655956029892\n",
            "Iteration 7810/10000, Loss: 0.024348925799131393\n",
            "Iteration 7811/10000, Loss: 0.024348197504878044\n",
            "Iteration 7812/10000, Loss: 0.024347463622689247\n",
            "Iteration 7813/10000, Loss: 0.02434673346579075\n",
            "Iteration 7814/10000, Loss: 0.02434600330889225\n",
            "Iteration 7815/10000, Loss: 0.024345271289348602\n",
            "Iteration 7816/10000, Loss: 0.024344537407159805\n",
            "Iteration 7817/10000, Loss: 0.024343807250261307\n",
            "Iteration 7818/10000, Loss: 0.024343078956007957\n",
            "Iteration 7819/10000, Loss: 0.02434234507381916\n",
            "Iteration 7820/10000, Loss: 0.02434161677956581\n",
            "Iteration 7821/10000, Loss: 0.024340886622667313\n",
            "Iteration 7822/10000, Loss: 0.024340154603123665\n",
            "Iteration 7823/10000, Loss: 0.024339424446225166\n",
            "Iteration 7824/10000, Loss: 0.02433869242668152\n",
            "Iteration 7825/10000, Loss: 0.02433795854449272\n",
            "Iteration 7826/10000, Loss: 0.024337228387594223\n",
            "Iteration 7827/10000, Loss: 0.024336498230695724\n",
            "Iteration 7828/10000, Loss: 0.024335768073797226\n",
            "Iteration 7829/10000, Loss: 0.024335036054253578\n",
            "Iteration 7830/10000, Loss: 0.02433430217206478\n",
            "Iteration 7831/10000, Loss: 0.02433357574045658\n",
            "Iteration 7832/10000, Loss: 0.024332845583558083\n",
            "Iteration 7833/10000, Loss: 0.024332113564014435\n",
            "Iteration 7834/10000, Loss: 0.024331381544470787\n",
            "Iteration 7835/10000, Loss: 0.024330653250217438\n",
            "Iteration 7836/10000, Loss: 0.02432992123067379\n",
            "Iteration 7837/10000, Loss: 0.024329187348484993\n",
            "Iteration 7838/10000, Loss: 0.024328459054231644\n",
            "Iteration 7839/10000, Loss: 0.024327730759978294\n",
            "Iteration 7840/10000, Loss: 0.024326996877789497\n",
            "Iteration 7841/10000, Loss: 0.024326268583536148\n",
            "Iteration 7842/10000, Loss: 0.0243255365639925\n",
            "Iteration 7843/10000, Loss: 0.024324804544448853\n",
            "Iteration 7844/10000, Loss: 0.024324078112840652\n",
            "Iteration 7845/10000, Loss: 0.024323346093297005\n",
            "Iteration 7846/10000, Loss: 0.024322614073753357\n",
            "Iteration 7847/10000, Loss: 0.024321885779500008\n",
            "Iteration 7848/10000, Loss: 0.02432115375995636\n",
            "Iteration 7849/10000, Loss: 0.02432042360305786\n",
            "Iteration 7850/10000, Loss: 0.024319691583514214\n",
            "Iteration 7851/10000, Loss: 0.024318961426615715\n",
            "Iteration 7852/10000, Loss: 0.024318233132362366\n",
            "Iteration 7853/10000, Loss: 0.024317504838109016\n",
            "Iteration 7854/10000, Loss: 0.02431677281856537\n",
            "Iteration 7855/10000, Loss: 0.02431604452431202\n",
            "Iteration 7856/10000, Loss: 0.02431531436741352\n",
            "Iteration 7857/10000, Loss: 0.02431458793580532\n",
            "Iteration 7858/10000, Loss: 0.024313855916261673\n",
            "Iteration 7859/10000, Loss: 0.024313127622008324\n",
            "Iteration 7860/10000, Loss: 0.024312397465109825\n",
            "Iteration 7861/10000, Loss: 0.024311669170856476\n",
            "Iteration 7862/10000, Loss: 0.024310942739248276\n",
            "Iteration 7863/10000, Loss: 0.024310212582349777\n",
            "Iteration 7864/10000, Loss: 0.02430948242545128\n",
            "Iteration 7865/10000, Loss: 0.024308757856488228\n",
            "Iteration 7866/10000, Loss: 0.02430802769958973\n",
            "Iteration 7867/10000, Loss: 0.02430729568004608\n",
            "Iteration 7868/10000, Loss: 0.024306567385792732\n",
            "Iteration 7869/10000, Loss: 0.024305840954184532\n",
            "Iteration 7870/10000, Loss: 0.024305112659931183\n",
            "Iteration 7871/10000, Loss: 0.024304382503032684\n",
            "Iteration 7872/10000, Loss: 0.024303654208779335\n",
            "Iteration 7873/10000, Loss: 0.024302924051880836\n",
            "Iteration 7874/10000, Loss: 0.024302197620272636\n",
            "Iteration 7875/10000, Loss: 0.02430146560072899\n",
            "Iteration 7876/10000, Loss: 0.02430073730647564\n",
            "Iteration 7877/10000, Loss: 0.02430001087486744\n",
            "Iteration 7878/10000, Loss: 0.02429928444325924\n",
            "Iteration 7879/10000, Loss: 0.02429855428636074\n",
            "Iteration 7880/10000, Loss: 0.024297824129462242\n",
            "Iteration 7881/10000, Loss: 0.024297093972563744\n",
            "Iteration 7882/10000, Loss: 0.024296369403600693\n",
            "Iteration 7883/10000, Loss: 0.024295639246702194\n",
            "Iteration 7884/10000, Loss: 0.024294909089803696\n",
            "Iteration 7885/10000, Loss: 0.024294180795550346\n",
            "Iteration 7886/10000, Loss: 0.024293456226587296\n",
            "Iteration 7887/10000, Loss: 0.024292724207043648\n",
            "Iteration 7888/10000, Loss: 0.0242919959127903\n",
            "Iteration 7889/10000, Loss: 0.02429126761853695\n",
            "Iteration 7890/10000, Loss: 0.0242905393242836\n",
            "Iteration 7891/10000, Loss: 0.0242898128926754\n",
            "Iteration 7892/10000, Loss: 0.02428908459842205\n",
            "Iteration 7893/10000, Loss: 0.0242883563041687\n",
            "Iteration 7894/10000, Loss: 0.024287628009915352\n",
            "Iteration 7895/10000, Loss: 0.024286901578307152\n",
            "Iteration 7896/10000, Loss: 0.024286173284053802\n",
            "Iteration 7897/10000, Loss: 0.024285443127155304\n",
            "Iteration 7898/10000, Loss: 0.024284716695547104\n",
            "Iteration 7899/10000, Loss: 0.024283988401293755\n",
            "Iteration 7900/10000, Loss: 0.024283258244395256\n",
            "Iteration 7901/10000, Loss: 0.024282533675432205\n",
            "Iteration 7902/10000, Loss: 0.024281805381178856\n",
            "Iteration 7903/10000, Loss: 0.024281080812215805\n",
            "Iteration 7904/10000, Loss: 0.024280350655317307\n",
            "Iteration 7905/10000, Loss: 0.024279622361063957\n",
            "Iteration 7906/10000, Loss: 0.024278895929455757\n",
            "Iteration 7907/10000, Loss: 0.024278167635202408\n",
            "Iteration 7908/10000, Loss: 0.02427743747830391\n",
            "Iteration 7909/10000, Loss: 0.02427671104669571\n",
            "Iteration 7910/10000, Loss: 0.02427598647773266\n",
            "Iteration 7911/10000, Loss: 0.02427525818347931\n",
            "Iteration 7912/10000, Loss: 0.02427453175187111\n",
            "Iteration 7913/10000, Loss: 0.02427380345761776\n",
            "Iteration 7914/10000, Loss: 0.02427307516336441\n",
            "Iteration 7915/10000, Loss: 0.02427234686911106\n",
            "Iteration 7916/10000, Loss: 0.02427162230014801\n",
            "Iteration 7917/10000, Loss: 0.02427089400589466\n",
            "Iteration 7918/10000, Loss: 0.02427016571164131\n",
            "Iteration 7919/10000, Loss: 0.02426943928003311\n",
            "Iteration 7920/10000, Loss: 0.024268710985779762\n",
            "Iteration 7921/10000, Loss: 0.024267984554171562\n",
            "Iteration 7922/10000, Loss: 0.024267254397273064\n",
            "Iteration 7923/10000, Loss: 0.024266526103019714\n",
            "Iteration 7924/10000, Loss: 0.024265803396701813\n",
            "Iteration 7925/10000, Loss: 0.024265075102448463\n",
            "Iteration 7926/10000, Loss: 0.024264348670840263\n",
            "Iteration 7927/10000, Loss: 0.024263620376586914\n",
            "Iteration 7928/10000, Loss: 0.024262893944978714\n",
            "Iteration 7929/10000, Loss: 0.024262167513370514\n",
            "Iteration 7930/10000, Loss: 0.024261439219117165\n",
            "Iteration 7931/10000, Loss: 0.024260710924863815\n",
            "Iteration 7932/10000, Loss: 0.024259980767965317\n",
            "Iteration 7933/10000, Loss: 0.024259256199002266\n",
            "Iteration 7934/10000, Loss: 0.024258527904748917\n",
            "Iteration 7935/10000, Loss: 0.024257803335785866\n",
            "Iteration 7936/10000, Loss: 0.024257073178887367\n",
            "Iteration 7937/10000, Loss: 0.024256350472569466\n",
            "Iteration 7938/10000, Loss: 0.024255622178316116\n",
            "Iteration 7939/10000, Loss: 0.024254893884062767\n",
            "Iteration 7940/10000, Loss: 0.024254169315099716\n",
            "Iteration 7941/10000, Loss: 0.024253439158201218\n",
            "Iteration 7942/10000, Loss: 0.024252714589238167\n",
            "Iteration 7943/10000, Loss: 0.024251988157629967\n",
            "Iteration 7944/10000, Loss: 0.024251258000731468\n",
            "Iteration 7945/10000, Loss: 0.024250535294413567\n",
            "Iteration 7946/10000, Loss: 0.024249807000160217\n",
            "Iteration 7947/10000, Loss: 0.024249078705906868\n",
            "Iteration 7948/10000, Loss: 0.02424835041165352\n",
            "Iteration 7949/10000, Loss: 0.024247625842690468\n",
            "Iteration 7950/10000, Loss: 0.024246901273727417\n",
            "Iteration 7951/10000, Loss: 0.024246172979474068\n",
            "Iteration 7952/10000, Loss: 0.024245446547865868\n",
            "Iteration 7953/10000, Loss: 0.02424471825361252\n",
            "Iteration 7954/10000, Loss: 0.024243991822004318\n",
            "Iteration 7955/10000, Loss: 0.024243265390396118\n",
            "Iteration 7956/10000, Loss: 0.024242538958787918\n",
            "Iteration 7957/10000, Loss: 0.024241812527179718\n",
            "Iteration 7958/10000, Loss: 0.024241086095571518\n",
            "Iteration 7959/10000, Loss: 0.024240359663963318\n",
            "Iteration 7960/10000, Loss: 0.02423963136970997\n",
            "Iteration 7961/10000, Loss: 0.02423890493810177\n",
            "Iteration 7962/10000, Loss: 0.024238180369138718\n",
            "Iteration 7963/10000, Loss: 0.02423745207488537\n",
            "Iteration 7964/10000, Loss: 0.02423672564327717\n",
            "Iteration 7965/10000, Loss: 0.024235999211668968\n",
            "Iteration 7966/10000, Loss: 0.024235274642705917\n",
            "Iteration 7967/10000, Loss: 0.024234548211097717\n",
            "Iteration 7968/10000, Loss: 0.024233819916844368\n",
            "Iteration 7969/10000, Loss: 0.024233093485236168\n",
            "Iteration 7970/10000, Loss: 0.02423236519098282\n",
            "Iteration 7971/10000, Loss: 0.024231642484664917\n",
            "Iteration 7972/10000, Loss: 0.024230914190411568\n",
            "Iteration 7973/10000, Loss: 0.024230187758803368\n",
            "Iteration 7974/10000, Loss: 0.024229461327195168\n",
            "Iteration 7975/10000, Loss: 0.024228736758232117\n",
            "Iteration 7976/10000, Loss: 0.024228008463978767\n",
            "Iteration 7977/10000, Loss: 0.024227282032370567\n",
            "Iteration 7978/10000, Loss: 0.024226555600762367\n",
            "Iteration 7979/10000, Loss: 0.024225831031799316\n",
            "Iteration 7980/10000, Loss: 0.024225104600191116\n",
            "Iteration 7981/10000, Loss: 0.024224378168582916\n",
            "Iteration 7982/10000, Loss: 0.024223651736974716\n",
            "Iteration 7983/10000, Loss: 0.024222929030656815\n",
            "Iteration 7984/10000, Loss: 0.024222200736403465\n",
            "Iteration 7985/10000, Loss: 0.024221472442150116\n",
            "Iteration 7986/10000, Loss: 0.024220746010541916\n",
            "Iteration 7987/10000, Loss: 0.024220023304224014\n",
            "Iteration 7988/10000, Loss: 0.024219296872615814\n",
            "Iteration 7989/10000, Loss: 0.024218570441007614\n",
            "Iteration 7990/10000, Loss: 0.024217845872044563\n",
            "Iteration 7991/10000, Loss: 0.024217117577791214\n",
            "Iteration 7992/10000, Loss: 0.024216393008828163\n",
            "Iteration 7993/10000, Loss: 0.024215666577219963\n",
            "Iteration 7994/10000, Loss: 0.024214940145611763\n",
            "Iteration 7995/10000, Loss: 0.024214211851358414\n",
            "Iteration 7996/10000, Loss: 0.024213489145040512\n",
            "Iteration 7997/10000, Loss: 0.02421276457607746\n",
            "Iteration 7998/10000, Loss: 0.02421204186975956\n",
            "Iteration 7999/10000, Loss: 0.02421131730079651\n",
            "Iteration 8000/10000, Loss: 0.024210596457123756\n",
            "Iteration 8001/10000, Loss: 0.024209871888160706\n",
            "Iteration 8002/10000, Loss: 0.024209147319197655\n",
            "Iteration 8003/10000, Loss: 0.024208420887589455\n",
            "Iteration 8004/10000, Loss: 0.024207700043916702\n",
            "Iteration 8005/10000, Loss: 0.02420697547495365\n",
            "Iteration 8006/10000, Loss: 0.0242062509059906\n",
            "Iteration 8007/10000, Loss: 0.024205530062317848\n",
            "Iteration 8008/10000, Loss: 0.024204807355999947\n",
            "Iteration 8009/10000, Loss: 0.024204080924391747\n",
            "Iteration 8010/10000, Loss: 0.024203360080718994\n",
            "Iteration 8011/10000, Loss: 0.024202633649110794\n",
            "Iteration 8012/10000, Loss: 0.02420191280543804\n",
            "Iteration 8013/10000, Loss: 0.02420119009912014\n",
            "Iteration 8014/10000, Loss: 0.02420046553015709\n",
            "Iteration 8015/10000, Loss: 0.02419974096119404\n",
            "Iteration 8016/10000, Loss: 0.024199018254876137\n",
            "Iteration 8017/10000, Loss: 0.024198293685913086\n",
            "Iteration 8018/10000, Loss: 0.024197570979595184\n",
            "Iteration 8019/10000, Loss: 0.024196846410632133\n",
            "Iteration 8020/10000, Loss: 0.02419612556695938\n",
            "Iteration 8021/10000, Loss: 0.02419540286064148\n",
            "Iteration 8022/10000, Loss: 0.024194680154323578\n",
            "Iteration 8023/10000, Loss: 0.024193955585360527\n",
            "Iteration 8024/10000, Loss: 0.024193232879042625\n",
            "Iteration 8025/10000, Loss: 0.024192512035369873\n",
            "Iteration 8026/10000, Loss: 0.024191787466406822\n",
            "Iteration 8027/10000, Loss: 0.02419106289744377\n",
            "Iteration 8028/10000, Loss: 0.02419034019112587\n",
            "Iteration 8029/10000, Loss: 0.024189619347453117\n",
            "Iteration 8030/10000, Loss: 0.024188896641135216\n",
            "Iteration 8031/10000, Loss: 0.024188173934817314\n",
            "Iteration 8032/10000, Loss: 0.024187447503209114\n",
            "Iteration 8033/10000, Loss: 0.02418672852218151\n",
            "Iteration 8034/10000, Loss: 0.02418600395321846\n",
            "Iteration 8035/10000, Loss: 0.02418528124690056\n",
            "Iteration 8036/10000, Loss: 0.024184556677937508\n",
            "Iteration 8037/10000, Loss: 0.024183835834264755\n",
            "Iteration 8038/10000, Loss: 0.024183111265301704\n",
            "Iteration 8039/10000, Loss: 0.024182390421628952\n",
            "Iteration 8040/10000, Loss: 0.0241816658526659\n",
            "Iteration 8041/10000, Loss: 0.02418094500899315\n",
            "Iteration 8042/10000, Loss: 0.024180224165320396\n",
            "Iteration 8043/10000, Loss: 0.024179499596357346\n",
            "Iteration 8044/10000, Loss: 0.024178776890039444\n",
            "Iteration 8045/10000, Loss: 0.024178054183721542\n",
            "Iteration 8046/10000, Loss: 0.02417733147740364\n",
            "Iteration 8047/10000, Loss: 0.02417660690844059\n",
            "Iteration 8048/10000, Loss: 0.024175886064767838\n",
            "Iteration 8049/10000, Loss: 0.024175163358449936\n",
            "Iteration 8050/10000, Loss: 0.024174440652132034\n",
            "Iteration 8051/10000, Loss: 0.024173717945814133\n",
            "Iteration 8052/10000, Loss: 0.02417299523949623\n",
            "Iteration 8053/10000, Loss: 0.02417227253317833\n",
            "Iteration 8054/10000, Loss: 0.024171549826860428\n",
            "Iteration 8055/10000, Loss: 0.024170827120542526\n",
            "Iteration 8056/10000, Loss: 0.024170104414224625\n",
            "Iteration 8057/10000, Loss: 0.024169379845261574\n",
            "Iteration 8058/10000, Loss: 0.02416865900158882\n",
            "Iteration 8059/10000, Loss: 0.02416793815791607\n",
            "Iteration 8060/10000, Loss: 0.024167213588953018\n",
            "Iteration 8061/10000, Loss: 0.024166494607925415\n",
            "Iteration 8062/10000, Loss: 0.024165770038962364\n",
            "Iteration 8063/10000, Loss: 0.024165049195289612\n",
            "Iteration 8064/10000, Loss: 0.02416432648897171\n",
            "Iteration 8065/10000, Loss: 0.02416360378265381\n",
            "Iteration 8066/10000, Loss: 0.024162882938981056\n",
            "Iteration 8067/10000, Loss: 0.024162160232663155\n",
            "Iteration 8068/10000, Loss: 0.024161437526345253\n",
            "Iteration 8069/10000, Loss: 0.024160712957382202\n",
            "Iteration 8070/10000, Loss: 0.02415999211370945\n",
            "Iteration 8071/10000, Loss: 0.024159269407391548\n",
            "Iteration 8072/10000, Loss: 0.024158550426363945\n",
            "Iteration 8073/10000, Loss: 0.024157823994755745\n",
            "Iteration 8074/10000, Loss: 0.02415710687637329\n",
            "Iteration 8075/10000, Loss: 0.02415638417005539\n",
            "Iteration 8076/10000, Loss: 0.02415565773844719\n",
            "Iteration 8077/10000, Loss: 0.024154936894774437\n",
            "Iteration 8078/10000, Loss: 0.024154216051101685\n",
            "Iteration 8079/10000, Loss: 0.024153495207428932\n",
            "Iteration 8080/10000, Loss: 0.02415277063846588\n",
            "Iteration 8081/10000, Loss: 0.024152051657438278\n",
            "Iteration 8082/10000, Loss: 0.024151330813765526\n",
            "Iteration 8083/10000, Loss: 0.024150608107447624\n",
            "Iteration 8084/10000, Loss: 0.024149887263774872\n",
            "Iteration 8085/10000, Loss: 0.02414916642010212\n",
            "Iteration 8086/10000, Loss: 0.02414844185113907\n",
            "Iteration 8087/10000, Loss: 0.024147722870111465\n",
            "Iteration 8088/10000, Loss: 0.024147002026438713\n",
            "Iteration 8089/10000, Loss: 0.02414627932012081\n",
            "Iteration 8090/10000, Loss: 0.02414555847644806\n",
            "Iteration 8091/10000, Loss: 0.024144839495420456\n",
            "Iteration 8092/10000, Loss: 0.024144114926457405\n",
            "Iteration 8093/10000, Loss: 0.024143395945429802\n",
            "Iteration 8094/10000, Loss: 0.02414267137646675\n",
            "Iteration 8095/10000, Loss: 0.024141952395439148\n",
            "Iteration 8096/10000, Loss: 0.024141231551766396\n",
            "Iteration 8097/10000, Loss: 0.024140508845448494\n",
            "Iteration 8098/10000, Loss: 0.024139786139130592\n",
            "Iteration 8099/10000, Loss: 0.02413906902074814\n",
            "Iteration 8100/10000, Loss: 0.024138346314430237\n",
            "Iteration 8101/10000, Loss: 0.024137623608112335\n",
            "Iteration 8102/10000, Loss: 0.024136902764439583\n",
            "Iteration 8103/10000, Loss: 0.02413618192076683\n",
            "Iteration 8104/10000, Loss: 0.024135462939739227\n",
            "Iteration 8105/10000, Loss: 0.024134742096066475\n",
            "Iteration 8106/10000, Loss: 0.024134019389748573\n",
            "Iteration 8107/10000, Loss: 0.02413330040872097\n",
            "Iteration 8108/10000, Loss: 0.02413257583975792\n",
            "Iteration 8109/10000, Loss: 0.024131856858730316\n",
            "Iteration 8110/10000, Loss: 0.024131136015057564\n",
            "Iteration 8111/10000, Loss: 0.02413041517138481\n",
            "Iteration 8112/10000, Loss: 0.02412969432771206\n",
            "Iteration 8113/10000, Loss: 0.024128973484039307\n",
            "Iteration 8114/10000, Loss: 0.024128250777721405\n",
            "Iteration 8115/10000, Loss: 0.02412753365933895\n",
            "Iteration 8116/10000, Loss: 0.02412681095302105\n",
            "Iteration 8117/10000, Loss: 0.024126088246703148\n",
            "Iteration 8118/10000, Loss: 0.024125367403030396\n",
            "Iteration 8119/10000, Loss: 0.024124648422002792\n",
            "Iteration 8120/10000, Loss: 0.02412392571568489\n",
            "Iteration 8121/10000, Loss: 0.024123206734657288\n",
            "Iteration 8122/10000, Loss: 0.024122482165694237\n",
            "Iteration 8123/10000, Loss: 0.024121765047311783\n",
            "Iteration 8124/10000, Loss: 0.02412104234099388\n",
            "Iteration 8125/10000, Loss: 0.02412031963467598\n",
            "Iteration 8126/10000, Loss: 0.024119600653648376\n",
            "Iteration 8127/10000, Loss: 0.024118879809975624\n",
            "Iteration 8128/10000, Loss: 0.02411816082894802\n",
            "Iteration 8129/10000, Loss: 0.02411743998527527\n",
            "Iteration 8130/10000, Loss: 0.024116719141602516\n",
            "Iteration 8131/10000, Loss: 0.024115996435284615\n",
            "Iteration 8132/10000, Loss: 0.02411527745425701\n",
            "Iteration 8133/10000, Loss: 0.024114558473229408\n",
            "Iteration 8134/10000, Loss: 0.024113835766911507\n",
            "Iteration 8135/10000, Loss: 0.024113114923238754\n",
            "Iteration 8136/10000, Loss: 0.0241123978048563\n",
            "Iteration 8137/10000, Loss: 0.024111678823828697\n",
            "Iteration 8138/10000, Loss: 0.024110954254865646\n",
            "Iteration 8139/10000, Loss: 0.024110235273838043\n",
            "Iteration 8140/10000, Loss: 0.02410951443016529\n",
            "Iteration 8141/10000, Loss: 0.02410879358649254\n",
            "Iteration 8142/10000, Loss: 0.024108072742819786\n",
            "Iteration 8143/10000, Loss: 0.024107351899147034\n",
            "Iteration 8144/10000, Loss: 0.02410663291811943\n",
            "Iteration 8145/10000, Loss: 0.024105913937091827\n",
            "Iteration 8146/10000, Loss: 0.024105191230773926\n",
            "Iteration 8147/10000, Loss: 0.024104472249746323\n",
            "Iteration 8148/10000, Loss: 0.02410375326871872\n",
            "Iteration 8149/10000, Loss: 0.024103030562400818\n",
            "Iteration 8150/10000, Loss: 0.024102309718728065\n",
            "Iteration 8151/10000, Loss: 0.02410159260034561\n",
            "Iteration 8152/10000, Loss: 0.02410087175667286\n",
            "Iteration 8153/10000, Loss: 0.024100152775645256\n",
            "Iteration 8154/10000, Loss: 0.024099430069327354\n",
            "Iteration 8155/10000, Loss: 0.02409871108829975\n",
            "Iteration 8156/10000, Loss: 0.024097992107272148\n",
            "Iteration 8157/10000, Loss: 0.024097273126244545\n",
            "Iteration 8158/10000, Loss: 0.024096550419926643\n",
            "Iteration 8159/10000, Loss: 0.02409583330154419\n",
            "Iteration 8160/10000, Loss: 0.024095110595226288\n",
            "Iteration 8161/10000, Loss: 0.024094393476843834\n",
            "Iteration 8162/10000, Loss: 0.024093670770525932\n",
            "Iteration 8163/10000, Loss: 0.02409294992685318\n",
            "Iteration 8164/10000, Loss: 0.024092232808470726\n",
            "Iteration 8165/10000, Loss: 0.024091513827443123\n",
            "Iteration 8166/10000, Loss: 0.02409079112112522\n",
            "Iteration 8167/10000, Loss: 0.02409007027745247\n",
            "Iteration 8168/10000, Loss: 0.024089351296424866\n",
            "Iteration 8169/10000, Loss: 0.024088632315397263\n",
            "Iteration 8170/10000, Loss: 0.02408791147172451\n",
            "Iteration 8171/10000, Loss: 0.024087190628051758\n",
            "Iteration 8172/10000, Loss: 0.024086471647024155\n",
            "Iteration 8173/10000, Loss: 0.024085750803351402\n",
            "Iteration 8174/10000, Loss: 0.0240850318223238\n",
            "Iteration 8175/10000, Loss: 0.024084312841296196\n",
            "Iteration 8176/10000, Loss: 0.024083591997623444\n",
            "Iteration 8177/10000, Loss: 0.02408287301659584\n",
            "Iteration 8178/10000, Loss: 0.024082154035568237\n",
            "Iteration 8179/10000, Loss: 0.024081433191895485\n",
            "Iteration 8180/10000, Loss: 0.024080712348222733\n",
            "Iteration 8181/10000, Loss: 0.02407999522984028\n",
            "Iteration 8182/10000, Loss: 0.024079276248812675\n",
            "Iteration 8183/10000, Loss: 0.024078553542494774\n",
            "Iteration 8184/10000, Loss: 0.02407783456146717\n",
            "Iteration 8185/10000, Loss: 0.024077115580439568\n",
            "Iteration 8186/10000, Loss: 0.024076396599411964\n",
            "Iteration 8187/10000, Loss: 0.024075675755739212\n",
            "Iteration 8188/10000, Loss: 0.02407495677471161\n",
            "Iteration 8189/10000, Loss: 0.024074239656329155\n",
            "Iteration 8190/10000, Loss: 0.024073516950011253\n",
            "Iteration 8191/10000, Loss: 0.0240727998316288\n",
            "Iteration 8192/10000, Loss: 0.024072078987956047\n",
            "Iteration 8193/10000, Loss: 0.024071361869573593\n",
            "Iteration 8194/10000, Loss: 0.02407064102590084\n",
            "Iteration 8195/10000, Loss: 0.02406991831958294\n",
            "Iteration 8196/10000, Loss: 0.024069199338555336\n",
            "Iteration 8197/10000, Loss: 0.02406848594546318\n",
            "Iteration 8198/10000, Loss: 0.02406776323914528\n",
            "Iteration 8199/10000, Loss: 0.024067044258117676\n",
            "Iteration 8200/10000, Loss: 0.024066325277090073\n",
            "Iteration 8201/10000, Loss: 0.02406560815870762\n",
            "Iteration 8202/10000, Loss: 0.024064887315034866\n",
            "Iteration 8203/10000, Loss: 0.024064166471362114\n",
            "Iteration 8204/10000, Loss: 0.02406344749033451\n",
            "Iteration 8205/10000, Loss: 0.024062730371952057\n",
            "Iteration 8206/10000, Loss: 0.024062011390924454\n",
            "Iteration 8207/10000, Loss: 0.02406129240989685\n",
            "Iteration 8208/10000, Loss: 0.024060573428869247\n",
            "Iteration 8209/10000, Loss: 0.024059852585196495\n",
            "Iteration 8210/10000, Loss: 0.024059133604168892\n",
            "Iteration 8211/10000, Loss: 0.02405841462314129\n",
            "Iteration 8212/10000, Loss: 0.024057693779468536\n",
            "Iteration 8213/10000, Loss: 0.02405697852373123\n",
            "Iteration 8214/10000, Loss: 0.02405625768005848\n",
            "Iteration 8215/10000, Loss: 0.024055538699030876\n",
            "Iteration 8216/10000, Loss: 0.024054821580648422\n",
            "Iteration 8217/10000, Loss: 0.02405410073697567\n",
            "Iteration 8218/10000, Loss: 0.024053383618593216\n",
            "Iteration 8219/10000, Loss: 0.024052662774920464\n",
            "Iteration 8220/10000, Loss: 0.02405194379389286\n",
            "Iteration 8221/10000, Loss: 0.024051228538155556\n",
            "Iteration 8222/10000, Loss: 0.024050505831837654\n",
            "Iteration 8223/10000, Loss: 0.0240497887134552\n",
            "Iteration 8224/10000, Loss: 0.024049067869782448\n",
            "Iteration 8225/10000, Loss: 0.024048352614045143\n",
            "Iteration 8226/10000, Loss: 0.02404762990772724\n",
            "Iteration 8227/10000, Loss: 0.024046912789344788\n",
            "Iteration 8228/10000, Loss: 0.024046195670962334\n",
            "Iteration 8229/10000, Loss: 0.02404547668993473\n",
            "Iteration 8230/10000, Loss: 0.024044755846261978\n",
            "Iteration 8231/10000, Loss: 0.024044038727879524\n",
            "Iteration 8232/10000, Loss: 0.02404331974685192\n",
            "Iteration 8233/10000, Loss: 0.024042602628469467\n",
            "Iteration 8234/10000, Loss: 0.024041881784796715\n",
            "Iteration 8235/10000, Loss: 0.02404116280376911\n",
            "Iteration 8236/10000, Loss: 0.02404044382274151\n",
            "Iteration 8237/10000, Loss: 0.024039722979068756\n",
            "Iteration 8238/10000, Loss: 0.024039005860686302\n",
            "Iteration 8239/10000, Loss: 0.02403828874230385\n",
            "Iteration 8240/10000, Loss: 0.024037569761276245\n",
            "Iteration 8241/10000, Loss: 0.02403685264289379\n",
            "Iteration 8242/10000, Loss: 0.024036133661866188\n",
            "Iteration 8243/10000, Loss: 0.024035416543483734\n",
            "Iteration 8244/10000, Loss: 0.024034695699810982\n",
            "Iteration 8245/10000, Loss: 0.024033980444073677\n",
            "Iteration 8246/10000, Loss: 0.024033259600400925\n",
            "Iteration 8247/10000, Loss: 0.02403254434466362\n",
            "Iteration 8248/10000, Loss: 0.02403181977570057\n",
            "Iteration 8249/10000, Loss: 0.024031104519963264\n",
            "Iteration 8250/10000, Loss: 0.02403038740158081\n",
            "Iteration 8251/10000, Loss: 0.024029668420553207\n",
            "Iteration 8252/10000, Loss: 0.024028949439525604\n",
            "Iteration 8253/10000, Loss: 0.02402823232114315\n",
            "Iteration 8254/10000, Loss: 0.024027513340115547\n",
            "Iteration 8255/10000, Loss: 0.024026796221733093\n",
            "Iteration 8256/10000, Loss: 0.02402607910335064\n",
            "Iteration 8257/10000, Loss: 0.024025358259677887\n",
            "Iteration 8258/10000, Loss: 0.024024641141295433\n",
            "Iteration 8259/10000, Loss: 0.02402392402291298\n",
            "Iteration 8260/10000, Loss: 0.024023205041885376\n",
            "Iteration 8261/10000, Loss: 0.024022487923502922\n",
            "Iteration 8262/10000, Loss: 0.02402176894247532\n",
            "Iteration 8263/10000, Loss: 0.024021049961447716\n",
            "Iteration 8264/10000, Loss: 0.024020332843065262\n",
            "Iteration 8265/10000, Loss: 0.024019617587327957\n",
            "Iteration 8266/10000, Loss: 0.024018898606300354\n",
            "Iteration 8267/10000, Loss: 0.0240181814879179\n",
            "Iteration 8268/10000, Loss: 0.024017466232180595\n",
            "Iteration 8269/10000, Loss: 0.024016747251152992\n",
            "Iteration 8270/10000, Loss: 0.024016033858060837\n",
            "Iteration 8271/10000, Loss: 0.02401532232761383\n",
            "Iteration 8272/10000, Loss: 0.024014605209231377\n",
            "Iteration 8273/10000, Loss: 0.024013888090848923\n",
            "Iteration 8274/10000, Loss: 0.024013174697756767\n",
            "Iteration 8275/10000, Loss: 0.024012459442019463\n",
            "Iteration 8276/10000, Loss: 0.024011744186282158\n",
            "Iteration 8277/10000, Loss: 0.024011028930544853\n",
            "Iteration 8278/10000, Loss: 0.024010315537452698\n",
            "Iteration 8279/10000, Loss: 0.024009598419070244\n",
            "Iteration 8280/10000, Loss: 0.02400888316333294\n",
            "Iteration 8281/10000, Loss: 0.024008169770240784\n",
            "Iteration 8282/10000, Loss: 0.02400745451450348\n",
            "Iteration 8283/10000, Loss: 0.024006737396121025\n",
            "Iteration 8284/10000, Loss: 0.02400602214038372\n",
            "Iteration 8285/10000, Loss: 0.024005306884646416\n",
            "Iteration 8286/10000, Loss: 0.02400459162890911\n",
            "Iteration 8287/10000, Loss: 0.024003878235816956\n",
            "Iteration 8288/10000, Loss: 0.0240031648427248\n",
            "Iteration 8289/10000, Loss: 0.024002443999052048\n",
            "Iteration 8290/10000, Loss: 0.02400173433125019\n",
            "Iteration 8291/10000, Loss: 0.024001019075512886\n",
            "Iteration 8292/10000, Loss: 0.02400030381977558\n",
            "Iteration 8293/10000, Loss: 0.023999590426683426\n",
            "Iteration 8294/10000, Loss: 0.02399887517094612\n",
            "Iteration 8295/10000, Loss: 0.023998159915208817\n",
            "Iteration 8296/10000, Loss: 0.023997442796826363\n",
            "Iteration 8297/10000, Loss: 0.023996727541089058\n",
            "Iteration 8298/10000, Loss: 0.0239960178732872\n",
            "Iteration 8299/10000, Loss: 0.023995298892259598\n",
            "Iteration 8300/10000, Loss: 0.023994585499167442\n",
            "Iteration 8301/10000, Loss: 0.02399386838078499\n",
            "Iteration 8302/10000, Loss: 0.023993154987692833\n",
            "Iteration 8303/10000, Loss: 0.023992441594600677\n",
            "Iteration 8304/10000, Loss: 0.023991726338863373\n",
            "Iteration 8305/10000, Loss: 0.02399100735783577\n",
            "Iteration 8306/10000, Loss: 0.023990299552679062\n",
            "Iteration 8307/10000, Loss: 0.023989584296941757\n",
            "Iteration 8308/10000, Loss: 0.023988869041204453\n",
            "Iteration 8309/10000, Loss: 0.023988151922822\n",
            "Iteration 8310/10000, Loss: 0.023987440392374992\n",
            "Iteration 8311/10000, Loss: 0.023986726999282837\n",
            "Iteration 8312/10000, Loss: 0.023986009880900383\n",
            "Iteration 8313/10000, Loss: 0.02398529462516308\n",
            "Iteration 8314/10000, Loss: 0.023984583094716072\n",
            "Iteration 8315/10000, Loss: 0.023983867838978767\n",
            "Iteration 8316/10000, Loss: 0.023983152583241463\n",
            "Iteration 8317/10000, Loss: 0.023982437327504158\n",
            "Iteration 8318/10000, Loss: 0.023981723934412003\n",
            "Iteration 8319/10000, Loss: 0.023981012403964996\n",
            "Iteration 8320/10000, Loss: 0.02398029714822769\n",
            "Iteration 8321/10000, Loss: 0.023979581892490387\n",
            "Iteration 8322/10000, Loss: 0.02397887036204338\n",
            "Iteration 8323/10000, Loss: 0.023978155106306076\n",
            "Iteration 8324/10000, Loss: 0.02397743985056877\n",
            "Iteration 8325/10000, Loss: 0.023976724594831467\n",
            "Iteration 8326/10000, Loss: 0.02397601306438446\n",
            "Iteration 8327/10000, Loss: 0.023975297808647156\n",
            "Iteration 8328/10000, Loss: 0.023974584415555\n",
            "Iteration 8329/10000, Loss: 0.023973867297172546\n",
            "Iteration 8330/10000, Loss: 0.02397315949201584\n",
            "Iteration 8331/10000, Loss: 0.023972442373633385\n",
            "Iteration 8332/10000, Loss: 0.02397172711789608\n",
            "Iteration 8333/10000, Loss: 0.023971011862158775\n",
            "Iteration 8334/10000, Loss: 0.02397029846906662\n",
            "Iteration 8335/10000, Loss: 0.023969585075974464\n",
            "Iteration 8336/10000, Loss: 0.02396887168288231\n",
            "Iteration 8337/10000, Loss: 0.023968156427145004\n",
            "Iteration 8338/10000, Loss: 0.023967444896697998\n",
            "Iteration 8339/10000, Loss: 0.023966731503605843\n",
            "Iteration 8340/10000, Loss: 0.023966018110513687\n",
            "Iteration 8341/10000, Loss: 0.023965299129486084\n",
            "Iteration 8342/10000, Loss: 0.02396458573639393\n",
            "Iteration 8343/10000, Loss: 0.023963874205946922\n",
            "Iteration 8344/10000, Loss: 0.023963158950209618\n",
            "Iteration 8345/10000, Loss: 0.023962443694472313\n",
            "Iteration 8346/10000, Loss: 0.023961732164025307\n",
            "Iteration 8347/10000, Loss: 0.023961016908288002\n",
            "Iteration 8348/10000, Loss: 0.023960307240486145\n",
            "Iteration 8349/10000, Loss: 0.02395959012210369\n",
            "Iteration 8350/10000, Loss: 0.023958878591656685\n",
            "Iteration 8351/10000, Loss: 0.02395816519856453\n",
            "Iteration 8352/10000, Loss: 0.023957448080182076\n",
            "Iteration 8353/10000, Loss: 0.02395673468708992\n",
            "Iteration 8354/10000, Loss: 0.023956023156642914\n",
            "Iteration 8355/10000, Loss: 0.02395530976355076\n",
            "Iteration 8356/10000, Loss: 0.023954594507813454\n",
            "Iteration 8357/10000, Loss: 0.023953877389431\n",
            "Iteration 8358/10000, Loss: 0.023953167721629143\n",
            "Iteration 8359/10000, Loss: 0.023952454328536987\n",
            "Iteration 8360/10000, Loss: 0.023951740935444832\n",
            "Iteration 8361/10000, Loss: 0.023951027542352676\n",
            "Iteration 8362/10000, Loss: 0.02395031601190567\n",
            "Iteration 8363/10000, Loss: 0.023949600756168365\n",
            "Iteration 8364/10000, Loss: 0.02394888736307621\n",
            "Iteration 8365/10000, Loss: 0.023948175832629204\n",
            "Iteration 8366/10000, Loss: 0.0239474605768919\n",
            "Iteration 8367/10000, Loss: 0.023946749046444893\n",
            "Iteration 8368/10000, Loss: 0.023946035653352737\n",
            "Iteration 8369/10000, Loss: 0.023945322260260582\n",
            "Iteration 8370/10000, Loss: 0.023944610729813576\n",
            "Iteration 8371/10000, Loss: 0.02394389547407627\n",
            "Iteration 8372/10000, Loss: 0.023943182080984116\n",
            "Iteration 8373/10000, Loss: 0.02394246868789196\n",
            "Iteration 8374/10000, Loss: 0.023941757157444954\n",
            "Iteration 8375/10000, Loss: 0.0239410437643528\n",
            "Iteration 8376/10000, Loss: 0.023940328508615494\n",
            "Iteration 8377/10000, Loss: 0.023939616978168488\n",
            "Iteration 8378/10000, Loss: 0.02393890544772148\n",
            "Iteration 8379/10000, Loss: 0.023938188329339027\n",
            "Iteration 8380/10000, Loss: 0.02393747866153717\n",
            "Iteration 8381/10000, Loss: 0.023936767131090164\n",
            "Iteration 8382/10000, Loss: 0.02393605373799801\n",
            "Iteration 8383/10000, Loss: 0.023935338482260704\n",
            "Iteration 8384/10000, Loss: 0.02393462508916855\n",
            "Iteration 8385/10000, Loss: 0.023933913558721542\n",
            "Iteration 8386/10000, Loss: 0.023933202028274536\n",
            "Iteration 8387/10000, Loss: 0.02393248677253723\n",
            "Iteration 8388/10000, Loss: 0.023931773379445076\n",
            "Iteration 8389/10000, Loss: 0.02393106371164322\n",
            "Iteration 8390/10000, Loss: 0.023930346593260765\n",
            "Iteration 8391/10000, Loss: 0.02392963506281376\n",
            "Iteration 8392/10000, Loss: 0.023928923532366753\n",
            "Iteration 8393/10000, Loss: 0.023928212001919746\n",
            "Iteration 8394/10000, Loss: 0.02392749674618244\n",
            "Iteration 8395/10000, Loss: 0.023926783353090286\n",
            "Iteration 8396/10000, Loss: 0.02392606995999813\n",
            "Iteration 8397/10000, Loss: 0.023925358429551125\n",
            "Iteration 8398/10000, Loss: 0.02392464131116867\n",
            "Iteration 8399/10000, Loss: 0.023923933506011963\n",
            "Iteration 8400/10000, Loss: 0.023923220112919807\n",
            "Iteration 8401/10000, Loss: 0.023922506719827652\n",
            "Iteration 8402/10000, Loss: 0.023921793326735497\n",
            "Iteration 8403/10000, Loss: 0.02392108179628849\n",
            "Iteration 8404/10000, Loss: 0.023920368403196335\n",
            "Iteration 8405/10000, Loss: 0.02391965687274933\n",
            "Iteration 8406/10000, Loss: 0.023918945342302322\n",
            "Iteration 8407/10000, Loss: 0.023918231949210167\n",
            "Iteration 8408/10000, Loss: 0.023917516693472862\n",
            "Iteration 8409/10000, Loss: 0.023916805163025856\n",
            "Iteration 8410/10000, Loss: 0.02391609363257885\n",
            "Iteration 8411/10000, Loss: 0.023915380239486694\n",
            "Iteration 8412/10000, Loss: 0.023914668709039688\n",
            "Iteration 8413/10000, Loss: 0.023913955315947533\n",
            "Iteration 8414/10000, Loss: 0.023913241922855377\n",
            "Iteration 8415/10000, Loss: 0.02391253225505352\n",
            "Iteration 8416/10000, Loss: 0.023911816999316216\n",
            "Iteration 8417/10000, Loss: 0.023911109194159508\n",
            "Iteration 8418/10000, Loss: 0.023910393938422203\n",
            "Iteration 8419/10000, Loss: 0.023909682407975197\n",
            "Iteration 8420/10000, Loss: 0.023908967152237892\n",
            "Iteration 8421/10000, Loss: 0.023908257484436035\n",
            "Iteration 8422/10000, Loss: 0.02390754409134388\n",
            "Iteration 8423/10000, Loss: 0.023906830698251724\n",
            "Iteration 8424/10000, Loss: 0.023906119167804718\n",
            "Iteration 8425/10000, Loss: 0.023905407637357712\n",
            "Iteration 8426/10000, Loss: 0.023904696106910706\n",
            "Iteration 8427/10000, Loss: 0.0239039845764637\n",
            "Iteration 8428/10000, Loss: 0.023903269320726395\n",
            "Iteration 8429/10000, Loss: 0.023902561515569687\n",
            "Iteration 8430/10000, Loss: 0.02390184812247753\n",
            "Iteration 8431/10000, Loss: 0.023901136592030525\n",
            "Iteration 8432/10000, Loss: 0.02390042506158352\n",
            "Iteration 8433/10000, Loss: 0.023899713531136513\n",
            "Iteration 8434/10000, Loss: 0.023899000138044357\n",
            "Iteration 8435/10000, Loss: 0.023898284882307053\n",
            "Iteration 8436/10000, Loss: 0.023897575214505196\n",
            "Iteration 8437/10000, Loss: 0.02389686368405819\n",
            "Iteration 8438/10000, Loss: 0.023896150290966034\n",
            "Iteration 8439/10000, Loss: 0.023895440623164177\n",
            "Iteration 8440/10000, Loss: 0.02389472723007202\n",
            "Iteration 8441/10000, Loss: 0.023894015699625015\n",
            "Iteration 8442/10000, Loss: 0.02389330416917801\n",
            "Iteration 8443/10000, Loss: 0.023892592638731003\n",
            "Iteration 8444/10000, Loss: 0.023891881108283997\n",
            "Iteration 8445/10000, Loss: 0.02389116771519184\n",
            "Iteration 8446/10000, Loss: 0.023890456184744835\n",
            "Iteration 8447/10000, Loss: 0.02388974279165268\n",
            "Iteration 8448/10000, Loss: 0.023889033123850822\n",
            "Iteration 8449/10000, Loss: 0.023888321593403816\n",
            "Iteration 8450/10000, Loss: 0.02388761006295681\n",
            "Iteration 8451/10000, Loss: 0.023886896669864655\n",
            "Iteration 8452/10000, Loss: 0.02388618513941765\n",
            "Iteration 8453/10000, Loss: 0.02388547547161579\n",
            "Iteration 8454/10000, Loss: 0.023884762078523636\n",
            "Iteration 8455/10000, Loss: 0.02388405054807663\n",
            "Iteration 8456/10000, Loss: 0.023883339017629623\n",
            "Iteration 8457/10000, Loss: 0.023882629349827766\n",
            "Iteration 8458/10000, Loss: 0.02388191595673561\n",
            "Iteration 8459/10000, Loss: 0.023881206288933754\n",
            "Iteration 8460/10000, Loss: 0.023880494758486748\n",
            "Iteration 8461/10000, Loss: 0.02387978509068489\n",
            "Iteration 8462/10000, Loss: 0.023879071697592735\n",
            "Iteration 8463/10000, Loss: 0.02387836016714573\n",
            "Iteration 8464/10000, Loss: 0.023877650499343872\n",
            "Iteration 8465/10000, Loss: 0.023876940831542015\n",
            "Iteration 8466/10000, Loss: 0.02387622930109501\n",
            "Iteration 8467/10000, Loss: 0.023875515908002853\n",
            "Iteration 8468/10000, Loss: 0.023874806240200996\n",
            "Iteration 8469/10000, Loss: 0.02387409470975399\n",
            "Iteration 8470/10000, Loss: 0.023873381316661835\n",
            "Iteration 8471/10000, Loss: 0.02387266978621483\n",
            "Iteration 8472/10000, Loss: 0.02387196384370327\n",
            "Iteration 8473/10000, Loss: 0.023871250450611115\n",
            "Iteration 8474/10000, Loss: 0.023870540782809258\n",
            "Iteration 8475/10000, Loss: 0.023869827389717102\n",
            "Iteration 8476/10000, Loss: 0.023869119584560394\n",
            "Iteration 8477/10000, Loss: 0.023868408054113388\n",
            "Iteration 8478/10000, Loss: 0.023867694661021233\n",
            "Iteration 8479/10000, Loss: 0.023866984993219376\n",
            "Iteration 8480/10000, Loss: 0.023866277188062668\n",
            "Iteration 8481/10000, Loss: 0.023865563794970512\n",
            "Iteration 8482/10000, Loss: 0.023864857852458954\n",
            "Iteration 8483/10000, Loss: 0.0238641444593668\n",
            "Iteration 8484/10000, Loss: 0.023863432928919792\n",
            "Iteration 8485/10000, Loss: 0.023862723261117935\n",
            "Iteration 8486/10000, Loss: 0.02386201173067093\n",
            "Iteration 8487/10000, Loss: 0.02386130392551422\n",
            "Iteration 8488/10000, Loss: 0.023860592395067215\n",
            "Iteration 8489/10000, Loss: 0.023859882727265358\n",
            "Iteration 8490/10000, Loss: 0.023859171196818352\n",
            "Iteration 8491/10000, Loss: 0.023858457803726196\n",
            "Iteration 8492/10000, Loss: 0.02385774999856949\n",
            "Iteration 8493/10000, Loss: 0.02385704033076763\n",
            "Iteration 8494/10000, Loss: 0.023856328800320625\n",
            "Iteration 8495/10000, Loss: 0.02385561913251877\n",
            "Iteration 8496/10000, Loss: 0.023854907602071762\n",
            "Iteration 8497/10000, Loss: 0.023854197934269905\n",
            "Iteration 8498/10000, Loss: 0.0238534864038229\n",
            "Iteration 8499/10000, Loss: 0.023852774873375893\n",
            "Iteration 8500/10000, Loss: 0.023852067068219185\n",
            "Iteration 8501/10000, Loss: 0.02385135553777218\n",
            "Iteration 8502/10000, Loss: 0.023850642144680023\n",
            "Iteration 8503/10000, Loss: 0.023849932476878166\n",
            "Iteration 8504/10000, Loss: 0.023849226534366608\n",
            "Iteration 8505/10000, Loss: 0.0238485150039196\n",
            "Iteration 8506/10000, Loss: 0.023847805336117744\n",
            "Iteration 8507/10000, Loss: 0.02384709194302559\n",
            "Iteration 8508/10000, Loss: 0.02384638413786888\n",
            "Iteration 8509/10000, Loss: 0.023845672607421875\n",
            "Iteration 8510/10000, Loss: 0.023844962939620018\n",
            "Iteration 8511/10000, Loss: 0.02384425513446331\n",
            "Iteration 8512/10000, Loss: 0.023843541741371155\n",
            "Iteration 8513/10000, Loss: 0.023842833936214447\n",
            "Iteration 8514/10000, Loss: 0.02384212054312229\n",
            "Iteration 8515/10000, Loss: 0.023841414600610733\n",
            "Iteration 8516/10000, Loss: 0.023840703070163727\n",
            "Iteration 8517/10000, Loss: 0.02383999153971672\n",
            "Iteration 8518/10000, Loss: 0.023839281871914864\n",
            "Iteration 8519/10000, Loss: 0.023838574066758156\n",
            "Iteration 8520/10000, Loss: 0.02383786253631115\n",
            "Iteration 8521/10000, Loss: 0.023837151005864143\n",
            "Iteration 8522/10000, Loss: 0.023836443200707436\n",
            "Iteration 8523/10000, Loss: 0.02383573167026043\n",
            "Iteration 8524/10000, Loss: 0.023835022002458572\n",
            "Iteration 8525/10000, Loss: 0.023834310472011566\n",
            "Iteration 8526/10000, Loss: 0.02383359894156456\n",
            "Iteration 8527/10000, Loss: 0.02383289486169815\n",
            "Iteration 8528/10000, Loss: 0.023832183331251144\n",
            "Iteration 8529/10000, Loss: 0.023831471800804138\n",
            "Iteration 8530/10000, Loss: 0.02383076213300228\n",
            "Iteration 8531/10000, Loss: 0.023830054327845573\n",
            "Iteration 8532/10000, Loss: 0.023829344660043716\n",
            "Iteration 8533/10000, Loss: 0.02382863312959671\n",
            "Iteration 8534/10000, Loss: 0.023827923461794853\n",
            "Iteration 8535/10000, Loss: 0.023827215656638145\n",
            "Iteration 8536/10000, Loss: 0.02382650226354599\n",
            "Iteration 8537/10000, Loss: 0.02382579632103443\n",
            "Iteration 8538/10000, Loss: 0.023825084790587425\n",
            "Iteration 8539/10000, Loss: 0.023824378848075867\n",
            "Iteration 8540/10000, Loss: 0.02382366731762886\n",
            "Iteration 8541/10000, Loss: 0.023822955787181854\n",
            "Iteration 8542/10000, Loss: 0.023822244256734848\n",
            "Iteration 8543/10000, Loss: 0.02382154017686844\n",
            "Iteration 8544/10000, Loss: 0.023820828646421432\n",
            "Iteration 8545/10000, Loss: 0.023820122703909874\n",
            "Iteration 8546/10000, Loss: 0.023819414898753166\n",
            "Iteration 8547/10000, Loss: 0.02381870709359646\n",
            "Iteration 8548/10000, Loss: 0.0238180011510849\n",
            "Iteration 8549/10000, Loss: 0.023817293345928192\n",
            "Iteration 8550/10000, Loss: 0.023816583678126335\n",
            "Iteration 8551/10000, Loss: 0.023815879598259926\n",
            "Iteration 8552/10000, Loss: 0.02381516993045807\n",
            "Iteration 8553/10000, Loss: 0.02381446398794651\n",
            "Iteration 8554/10000, Loss: 0.023813756182789803\n",
            "Iteration 8555/10000, Loss: 0.023813050240278244\n",
            "Iteration 8556/10000, Loss: 0.023812340572476387\n",
            "Iteration 8557/10000, Loss: 0.023811636492609978\n",
            "Iteration 8558/10000, Loss: 0.02381093055009842\n",
            "Iteration 8559/10000, Loss: 0.023810220882296562\n",
            "Iteration 8560/10000, Loss: 0.023809514939785004\n",
            "Iteration 8561/10000, Loss: 0.023808807134628296\n",
            "Iteration 8562/10000, Loss: 0.023808101192116737\n",
            "Iteration 8563/10000, Loss: 0.02380739152431488\n",
            "Iteration 8564/10000, Loss: 0.023806685581803322\n",
            "Iteration 8565/10000, Loss: 0.023805979639291763\n",
            "Iteration 8566/10000, Loss: 0.023805273696780205\n",
            "Iteration 8567/10000, Loss: 0.023804564028978348\n",
            "Iteration 8568/10000, Loss: 0.02380385994911194\n",
            "Iteration 8569/10000, Loss: 0.02380315214395523\n",
            "Iteration 8570/10000, Loss: 0.023802446201443672\n",
            "Iteration 8571/10000, Loss: 0.023801738396286964\n",
            "Iteration 8572/10000, Loss: 0.023801030591130257\n",
            "Iteration 8573/10000, Loss: 0.02380032278597355\n",
            "Iteration 8574/10000, Loss: 0.02379962056875229\n",
            "Iteration 8575/10000, Loss: 0.02379891276359558\n",
            "Iteration 8576/10000, Loss: 0.023798206821084023\n",
            "Iteration 8577/10000, Loss: 0.023797497153282166\n",
            "Iteration 8578/10000, Loss: 0.023796791210770607\n",
            "Iteration 8579/10000, Loss: 0.0237960834056139\n",
            "Iteration 8580/10000, Loss: 0.02379537560045719\n",
            "Iteration 8581/10000, Loss: 0.023794671520590782\n",
            "Iteration 8582/10000, Loss: 0.023793965578079224\n",
            "Iteration 8583/10000, Loss: 0.023793255910277367\n",
            "Iteration 8584/10000, Loss: 0.023792549967765808\n",
            "Iteration 8585/10000, Loss: 0.0237918458878994\n",
            "Iteration 8586/10000, Loss: 0.02379113808274269\n",
            "Iteration 8587/10000, Loss: 0.023790432140231133\n",
            "Iteration 8588/10000, Loss: 0.023789724335074425\n",
            "Iteration 8589/10000, Loss: 0.023789022117853165\n",
            "Iteration 8590/10000, Loss: 0.023788314312696457\n",
            "Iteration 8591/10000, Loss: 0.0237876046448946\n",
            "Iteration 8592/10000, Loss: 0.02378690056502819\n",
            "Iteration 8593/10000, Loss: 0.023786192759871483\n",
            "Iteration 8594/10000, Loss: 0.023785486817359924\n",
            "Iteration 8595/10000, Loss: 0.023784779012203217\n",
            "Iteration 8596/10000, Loss: 0.023784074932336807\n",
            "Iteration 8597/10000, Loss: 0.02378336898982525\n",
            "Iteration 8598/10000, Loss: 0.02378266304731369\n",
            "Iteration 8599/10000, Loss: 0.02378195710480213\n",
            "Iteration 8600/10000, Loss: 0.023781251162290573\n",
            "Iteration 8601/10000, Loss: 0.023780543357133865\n",
            "Iteration 8602/10000, Loss: 0.023779841139912605\n",
            "Iteration 8603/10000, Loss: 0.023779133334755898\n",
            "Iteration 8604/10000, Loss: 0.02377842552959919\n",
            "Iteration 8605/10000, Loss: 0.023777717724442482\n",
            "Iteration 8606/10000, Loss: 0.023777013644576073\n",
            "Iteration 8607/10000, Loss: 0.023776305839419365\n",
            "Iteration 8608/10000, Loss: 0.023775601759552956\n",
            "Iteration 8609/10000, Loss: 0.023774893954396248\n",
            "Iteration 8610/10000, Loss: 0.02377418987452984\n",
            "Iteration 8611/10000, Loss: 0.02377348393201828\n",
            "Iteration 8612/10000, Loss: 0.02377277985215187\n",
            "Iteration 8613/10000, Loss: 0.023772068321704865\n",
            "Iteration 8614/10000, Loss: 0.023771364241838455\n",
            "Iteration 8615/10000, Loss: 0.023770658299326897\n",
            "Iteration 8616/10000, Loss: 0.023769952356815338\n",
            "Iteration 8617/10000, Loss: 0.023769250139594078\n",
            "Iteration 8618/10000, Loss: 0.02376854047179222\n",
            "Iteration 8619/10000, Loss: 0.023767836391925812\n",
            "Iteration 8620/10000, Loss: 0.023767130449414253\n",
            "Iteration 8621/10000, Loss: 0.023766424506902695\n",
            "Iteration 8622/10000, Loss: 0.023765718564391136\n",
            "Iteration 8623/10000, Loss: 0.02376500889658928\n",
            "Iteration 8624/10000, Loss: 0.02376430667936802\n",
            "Iteration 8625/10000, Loss: 0.02376360073685646\n",
            "Iteration 8626/10000, Loss: 0.023762894794344902\n",
            "Iteration 8627/10000, Loss: 0.023762188851833344\n",
            "Iteration 8628/10000, Loss: 0.023761482909321785\n",
            "Iteration 8629/10000, Loss: 0.023760778829455376\n",
            "Iteration 8630/10000, Loss: 0.023760072886943817\n",
            "Iteration 8631/10000, Loss: 0.02375936321914196\n",
            "Iteration 8632/10000, Loss: 0.0237586610019207\n",
            "Iteration 8633/10000, Loss: 0.023757953196763992\n",
            "Iteration 8634/10000, Loss: 0.023757247254252434\n",
            "Iteration 8635/10000, Loss: 0.023756545037031174\n",
            "Iteration 8636/10000, Loss: 0.023755837231874466\n",
            "Iteration 8637/10000, Loss: 0.023755133152008057\n",
            "Iteration 8638/10000, Loss: 0.0237544234842062\n",
            "Iteration 8639/10000, Loss: 0.02375372126698494\n",
            "Iteration 8640/10000, Loss: 0.02375301718711853\n",
            "Iteration 8641/10000, Loss: 0.023752309381961823\n",
            "Iteration 8642/10000, Loss: 0.023751605302095413\n",
            "Iteration 8643/10000, Loss: 0.023750899359583855\n",
            "Iteration 8644/10000, Loss: 0.023750195279717445\n",
            "Iteration 8645/10000, Loss: 0.023749489337205887\n",
            "Iteration 8646/10000, Loss: 0.02374878153204918\n",
            "Iteration 8647/10000, Loss: 0.023748081177473068\n",
            "Iteration 8648/10000, Loss: 0.02374737337231636\n",
            "Iteration 8649/10000, Loss: 0.02374666929244995\n",
            "Iteration 8650/10000, Loss: 0.023745961487293243\n",
            "Iteration 8651/10000, Loss: 0.023745259270071983\n",
            "Iteration 8652/10000, Loss: 0.023744553327560425\n",
            "Iteration 8653/10000, Loss: 0.023743847385048866\n",
            "Iteration 8654/10000, Loss: 0.023743143305182457\n",
            "Iteration 8655/10000, Loss: 0.023742439225316048\n",
            "Iteration 8656/10000, Loss: 0.02374173328280449\n",
            "Iteration 8657/10000, Loss: 0.02374102734029293\n",
            "Iteration 8658/10000, Loss: 0.02374032326042652\n",
            "Iteration 8659/10000, Loss: 0.02373962104320526\n",
            "Iteration 8660/10000, Loss: 0.023738915100693703\n",
            "Iteration 8661/10000, Loss: 0.023738211020827293\n",
            "Iteration 8662/10000, Loss: 0.023737506940960884\n",
            "Iteration 8663/10000, Loss: 0.023736802861094475\n",
            "Iteration 8664/10000, Loss: 0.023736095055937767\n",
            "Iteration 8665/10000, Loss: 0.023735390976071358\n",
            "Iteration 8666/10000, Loss: 0.02373468689620495\n",
            "Iteration 8667/10000, Loss: 0.02373398281633854\n",
            "Iteration 8668/10000, Loss: 0.02373327687382698\n",
            "Iteration 8669/10000, Loss: 0.02373257279396057\n",
            "Iteration 8670/10000, Loss: 0.02373187057673931\n",
            "Iteration 8671/10000, Loss: 0.023731164634227753\n",
            "Iteration 8672/10000, Loss: 0.023730458691716194\n",
            "Iteration 8673/10000, Loss: 0.023729756474494934\n",
            "Iteration 8674/10000, Loss: 0.023729052394628525\n",
            "Iteration 8675/10000, Loss: 0.023728344589471817\n",
            "Iteration 8676/10000, Loss: 0.023727642372250557\n",
            "Iteration 8677/10000, Loss: 0.023726936429739\n",
            "Iteration 8678/10000, Loss: 0.02372623421251774\n",
            "Iteration 8679/10000, Loss: 0.02372553013265133\n",
            "Iteration 8680/10000, Loss: 0.02372482605278492\n",
            "Iteration 8681/10000, Loss: 0.02372412197291851\n",
            "Iteration 8682/10000, Loss: 0.023723416030406952\n",
            "Iteration 8683/10000, Loss: 0.023722713813185692\n",
            "Iteration 8684/10000, Loss: 0.023722006008028984\n",
            "Iteration 8685/10000, Loss: 0.023721303790807724\n",
            "Iteration 8686/10000, Loss: 0.023720601573586464\n",
            "Iteration 8687/10000, Loss: 0.023719897493720055\n",
            "Iteration 8688/10000, Loss: 0.023719189688563347\n",
            "Iteration 8689/10000, Loss: 0.023718487471342087\n",
            "Iteration 8690/10000, Loss: 0.023717781528830528\n",
            "Iteration 8691/10000, Loss: 0.02371707744896412\n",
            "Iteration 8692/10000, Loss: 0.02371637336909771\n",
            "Iteration 8693/10000, Loss: 0.02371567115187645\n",
            "Iteration 8694/10000, Loss: 0.02371496707201004\n",
            "Iteration 8695/10000, Loss: 0.02371426299214363\n",
            "Iteration 8696/10000, Loss: 0.02371355891227722\n",
            "Iteration 8697/10000, Loss: 0.023712854832410812\n",
            "Iteration 8698/10000, Loss: 0.023712152615189552\n",
            "Iteration 8699/10000, Loss: 0.023711448535323143\n",
            "Iteration 8700/10000, Loss: 0.023710742592811584\n",
            "Iteration 8701/10000, Loss: 0.023710040375590324\n",
            "Iteration 8702/10000, Loss: 0.023709336295723915\n",
            "Iteration 8703/10000, Loss: 0.023708628490567207\n",
            "Iteration 8704/10000, Loss: 0.023707929998636246\n",
            "Iteration 8705/10000, Loss: 0.023707224056124687\n",
            "Iteration 8706/10000, Loss: 0.023706521838903427\n",
            "Iteration 8707/10000, Loss: 0.02370581589639187\n",
            "Iteration 8708/10000, Loss: 0.02370511367917061\n",
            "Iteration 8709/10000, Loss: 0.0237044058740139\n",
            "Iteration 8710/10000, Loss: 0.02370370179414749\n",
            "Iteration 8711/10000, Loss: 0.02370299957692623\n",
            "Iteration 8712/10000, Loss: 0.02370229922235012\n",
            "Iteration 8713/10000, Loss: 0.02370159514248371\n",
            "Iteration 8714/10000, Loss: 0.023700889199972153\n",
            "Iteration 8715/10000, Loss: 0.023700185120105743\n",
            "Iteration 8716/10000, Loss: 0.023699484765529633\n",
            "Iteration 8717/10000, Loss: 0.023698780685663223\n",
            "Iteration 8718/10000, Loss: 0.023698076605796814\n",
            "Iteration 8719/10000, Loss: 0.023697370663285255\n",
            "Iteration 8720/10000, Loss: 0.023696670308709145\n",
            "Iteration 8721/10000, Loss: 0.023695964366197586\n",
            "Iteration 8722/10000, Loss: 0.023695264011621475\n",
            "Iteration 8723/10000, Loss: 0.023694559931755066\n",
            "Iteration 8724/10000, Loss: 0.023693855851888657\n",
            "Iteration 8725/10000, Loss: 0.023693151772022247\n",
            "Iteration 8726/10000, Loss: 0.023692447692155838\n",
            "Iteration 8727/10000, Loss: 0.023691747337579727\n",
            "Iteration 8728/10000, Loss: 0.02369104139506817\n",
            "Iteration 8729/10000, Loss: 0.02369033731520176\n",
            "Iteration 8730/10000, Loss: 0.0236896350979805\n",
            "Iteration 8731/10000, Loss: 0.02368893474340439\n",
            "Iteration 8732/10000, Loss: 0.02368822880089283\n",
            "Iteration 8733/10000, Loss: 0.02368752658367157\n",
            "Iteration 8734/10000, Loss: 0.02368682064116001\n",
            "Iteration 8735/10000, Loss: 0.0236861202865839\n",
            "Iteration 8736/10000, Loss: 0.02368541620671749\n",
            "Iteration 8737/10000, Loss: 0.023684710264205933\n",
            "Iteration 8738/10000, Loss: 0.023684008046984673\n",
            "Iteration 8739/10000, Loss: 0.023683305829763412\n",
            "Iteration 8740/10000, Loss: 0.023682601749897003\n",
            "Iteration 8741/10000, Loss: 0.023681897670030594\n",
            "Iteration 8742/10000, Loss: 0.023681195452809334\n",
            "Iteration 8743/10000, Loss: 0.023680493235588074\n",
            "Iteration 8744/10000, Loss: 0.023679789155721664\n",
            "Iteration 8745/10000, Loss: 0.023679086938500404\n",
            "Iteration 8746/10000, Loss: 0.023678382858633995\n",
            "Iteration 8747/10000, Loss: 0.023677680641412735\n",
            "Iteration 8748/10000, Loss: 0.023676978424191475\n",
            "Iteration 8749/10000, Loss: 0.023676272481679916\n",
            "Iteration 8750/10000, Loss: 0.023675572127103806\n",
            "Iteration 8751/10000, Loss: 0.023674869909882545\n",
            "Iteration 8752/10000, Loss: 0.023674163967370987\n",
            "Iteration 8753/10000, Loss: 0.023673463612794876\n",
            "Iteration 8754/10000, Loss: 0.023672763258218765\n",
            "Iteration 8755/10000, Loss: 0.023672055453062057\n",
            "Iteration 8756/10000, Loss: 0.023671351373195648\n",
            "Iteration 8757/10000, Loss: 0.023670651018619537\n",
            "Iteration 8758/10000, Loss: 0.023669950664043427\n",
            "Iteration 8759/10000, Loss: 0.023669244721531868\n",
            "Iteration 8760/10000, Loss: 0.02366854064166546\n",
            "Iteration 8761/10000, Loss: 0.023667840287089348\n",
            "Iteration 8762/10000, Loss: 0.023667138069868088\n",
            "Iteration 8763/10000, Loss: 0.02366643399000168\n",
            "Iteration 8764/10000, Loss: 0.023665735498070717\n",
            "Iteration 8765/10000, Loss: 0.023665033280849457\n",
            "Iteration 8766/10000, Loss: 0.02366432547569275\n",
            "Iteration 8767/10000, Loss: 0.02366362325847149\n",
            "Iteration 8768/10000, Loss: 0.023662924766540527\n",
            "Iteration 8769/10000, Loss: 0.023662224411964417\n",
            "Iteration 8770/10000, Loss: 0.023661520332098007\n",
            "Iteration 8771/10000, Loss: 0.023660818114876747\n",
            "Iteration 8772/10000, Loss: 0.023660119622945786\n",
            "Iteration 8773/10000, Loss: 0.023659415543079376\n",
            "Iteration 8774/10000, Loss: 0.023658715188503265\n",
            "Iteration 8775/10000, Loss: 0.023658016696572304\n",
            "Iteration 8776/10000, Loss: 0.023657310754060745\n",
            "Iteration 8777/10000, Loss: 0.023656610399484634\n",
            "Iteration 8778/10000, Loss: 0.023655911907553673\n",
            "Iteration 8779/10000, Loss: 0.023655209690332413\n",
            "Iteration 8780/10000, Loss: 0.02365451119840145\n",
            "Iteration 8781/10000, Loss: 0.023653807118535042\n",
            "Iteration 8782/10000, Loss: 0.02365310862660408\n",
            "Iteration 8783/10000, Loss: 0.02365240454673767\n",
            "Iteration 8784/10000, Loss: 0.02365170419216156\n",
            "Iteration 8785/10000, Loss: 0.02365100383758545\n",
            "Iteration 8786/10000, Loss: 0.02365030162036419\n",
            "Iteration 8787/10000, Loss: 0.02364960126578808\n",
            "Iteration 8788/10000, Loss: 0.023648899048566818\n",
            "Iteration 8789/10000, Loss: 0.023648200556635857\n",
            "Iteration 8790/10000, Loss: 0.023647496476769447\n",
            "Iteration 8791/10000, Loss: 0.023646796122193336\n",
            "Iteration 8792/10000, Loss: 0.023646097630262375\n",
            "Iteration 8793/10000, Loss: 0.023645395413041115\n",
            "Iteration 8794/10000, Loss: 0.023644693195819855\n",
            "Iteration 8795/10000, Loss: 0.023643990978598595\n",
            "Iteration 8796/10000, Loss: 0.023643290624022484\n",
            "Iteration 8797/10000, Loss: 0.023642590269446373\n",
            "Iteration 8798/10000, Loss: 0.023641888052225113\n",
            "Iteration 8799/10000, Loss: 0.023641187697649002\n",
            "Iteration 8800/10000, Loss: 0.02364048920571804\n",
            "Iteration 8801/10000, Loss: 0.02363978698849678\n",
            "Iteration 8802/10000, Loss: 0.02363908477127552\n",
            "Iteration 8803/10000, Loss: 0.023638388141989708\n",
            "Iteration 8804/10000, Loss: 0.0236376840621233\n",
            "Iteration 8805/10000, Loss: 0.023636985570192337\n",
            "Iteration 8806/10000, Loss: 0.023636281490325928\n",
            "Iteration 8807/10000, Loss: 0.023635584861040115\n",
            "Iteration 8808/10000, Loss: 0.023634882643818855\n",
            "Iteration 8809/10000, Loss: 0.023634182289242744\n",
            "Iteration 8810/10000, Loss: 0.023633483797311783\n",
            "Iteration 8811/10000, Loss: 0.023632781580090523\n",
            "Iteration 8812/10000, Loss: 0.023632079362869263\n",
            "Iteration 8813/10000, Loss: 0.023631379008293152\n",
            "Iteration 8814/10000, Loss: 0.02363067865371704\n",
            "Iteration 8815/10000, Loss: 0.02362997829914093\n",
            "Iteration 8816/10000, Loss: 0.02362927794456482\n",
            "Iteration 8817/10000, Loss: 0.02362857572734356\n",
            "Iteration 8818/10000, Loss: 0.023627879098057747\n",
            "Iteration 8819/10000, Loss: 0.023627178743481636\n",
            "Iteration 8820/10000, Loss: 0.023626476526260376\n",
            "Iteration 8821/10000, Loss: 0.023625778034329414\n",
            "Iteration 8822/10000, Loss: 0.023625075817108154\n",
            "Iteration 8823/10000, Loss: 0.023624377325177193\n",
            "Iteration 8824/10000, Loss: 0.023623676970601082\n",
            "Iteration 8825/10000, Loss: 0.02362297847867012\n",
            "Iteration 8826/10000, Loss: 0.023622281849384308\n",
            "Iteration 8827/10000, Loss: 0.023621583357453346\n",
            "Iteration 8828/10000, Loss: 0.023620884865522385\n",
            "Iteration 8829/10000, Loss: 0.02362019009888172\n",
            "Iteration 8830/10000, Loss: 0.02361948974430561\n",
            "Iteration 8831/10000, Loss: 0.0236187931150198\n",
            "Iteration 8832/10000, Loss: 0.023618094623088837\n",
            "Iteration 8833/10000, Loss: 0.023617396131157875\n",
            "Iteration 8834/10000, Loss: 0.023616699501872063\n",
            "Iteration 8835/10000, Loss: 0.023615999147295952\n",
            "Iteration 8836/10000, Loss: 0.02361530251801014\n",
            "Iteration 8837/10000, Loss: 0.02361460216343403\n",
            "Iteration 8838/10000, Loss: 0.023613905534148216\n",
            "Iteration 8839/10000, Loss: 0.023613207042217255\n",
            "Iteration 8840/10000, Loss: 0.02361251227557659\n",
            "Iteration 8841/10000, Loss: 0.02361181564629078\n",
            "Iteration 8842/10000, Loss: 0.02361111529171467\n",
            "Iteration 8843/10000, Loss: 0.023610416799783707\n",
            "Iteration 8844/10000, Loss: 0.023609722033143044\n",
            "Iteration 8845/10000, Loss: 0.023609021678566933\n",
            "Iteration 8846/10000, Loss: 0.02360832318663597\n",
            "Iteration 8847/10000, Loss: 0.02360762283205986\n",
            "Iteration 8848/10000, Loss: 0.023606933653354645\n",
            "Iteration 8849/10000, Loss: 0.023606233298778534\n",
            "Iteration 8850/10000, Loss: 0.023605534806847572\n",
            "Iteration 8851/10000, Loss: 0.02360483631491661\n",
            "Iteration 8852/10000, Loss: 0.023604143410921097\n",
            "Iteration 8853/10000, Loss: 0.023603443056344986\n",
            "Iteration 8854/10000, Loss: 0.023602744564414024\n",
            "Iteration 8855/10000, Loss: 0.02360205166041851\n",
            "Iteration 8856/10000, Loss: 0.02360135316848755\n",
            "Iteration 8857/10000, Loss: 0.023600656539201736\n",
            "Iteration 8858/10000, Loss: 0.023599956184625626\n",
            "Iteration 8859/10000, Loss: 0.023599261417984962\n",
            "Iteration 8860/10000, Loss: 0.023598562926054\n",
            "Iteration 8861/10000, Loss: 0.02359786257147789\n",
            "Iteration 8862/10000, Loss: 0.023597169667482376\n",
            "Iteration 8863/10000, Loss: 0.023596474900841713\n",
            "Iteration 8864/10000, Loss: 0.023595774546265602\n",
            "Iteration 8865/10000, Loss: 0.02359507977962494\n",
            "Iteration 8866/10000, Loss: 0.023594385012984276\n",
            "Iteration 8867/10000, Loss: 0.023593686521053314\n",
            "Iteration 8868/10000, Loss: 0.023592988029122353\n",
            "Iteration 8869/10000, Loss: 0.02359228953719139\n",
            "Iteration 8870/10000, Loss: 0.023591594770550728\n",
            "Iteration 8871/10000, Loss: 0.023590896278619766\n",
            "Iteration 8872/10000, Loss: 0.023590199649333954\n",
            "Iteration 8873/10000, Loss: 0.023589501157402992\n",
            "Iteration 8874/10000, Loss: 0.02358880825340748\n",
            "Iteration 8875/10000, Loss: 0.023588111624121666\n",
            "Iteration 8876/10000, Loss: 0.023587413132190704\n",
            "Iteration 8877/10000, Loss: 0.023586716502904892\n",
            "Iteration 8878/10000, Loss: 0.02358601987361908\n",
            "Iteration 8879/10000, Loss: 0.023585325106978416\n",
            "Iteration 8880/10000, Loss: 0.023584626615047455\n",
            "Iteration 8881/10000, Loss: 0.023583929985761642\n",
            "Iteration 8882/10000, Loss: 0.02358323521912098\n",
            "Iteration 8883/10000, Loss: 0.023582536727190018\n",
            "Iteration 8884/10000, Loss: 0.023581841960549355\n",
            "Iteration 8885/10000, Loss: 0.023581141605973244\n",
            "Iteration 8886/10000, Loss: 0.02358044683933258\n",
            "Iteration 8887/10000, Loss: 0.023579753935337067\n",
            "Iteration 8888/10000, Loss: 0.023579055443406105\n",
            "Iteration 8889/10000, Loss: 0.023578356951475143\n",
            "Iteration 8890/10000, Loss: 0.02357766032218933\n",
            "Iteration 8891/10000, Loss: 0.023576965555548668\n",
            "Iteration 8892/10000, Loss: 0.023576270788908005\n",
            "Iteration 8893/10000, Loss: 0.023575570434331894\n",
            "Iteration 8894/10000, Loss: 0.02357487566769123\n",
            "Iteration 8895/10000, Loss: 0.02357417903840542\n",
            "Iteration 8896/10000, Loss: 0.023573484271764755\n",
            "Iteration 8897/10000, Loss: 0.023572789505124092\n",
            "Iteration 8898/10000, Loss: 0.02357209101319313\n",
            "Iteration 8899/10000, Loss: 0.02357139252126217\n",
            "Iteration 8900/10000, Loss: 0.023570699617266655\n",
            "Iteration 8901/10000, Loss: 0.023570002987980843\n",
            "Iteration 8902/10000, Loss: 0.02356930449604988\n",
            "Iteration 8903/10000, Loss: 0.02356860600411892\n",
            "Iteration 8904/10000, Loss: 0.023567911237478256\n",
            "Iteration 8905/10000, Loss: 0.023567218333482742\n",
            "Iteration 8906/10000, Loss: 0.02356651984155178\n",
            "Iteration 8907/10000, Loss: 0.023565825074911118\n",
            "Iteration 8908/10000, Loss: 0.023565128445625305\n",
            "Iteration 8909/10000, Loss: 0.023564429953694344\n",
            "Iteration 8910/10000, Loss: 0.02356373518705368\n",
            "Iteration 8911/10000, Loss: 0.023563038557767868\n",
            "Iteration 8912/10000, Loss: 0.023562343791127205\n",
            "Iteration 8913/10000, Loss: 0.023561647161841393\n",
            "Iteration 8914/10000, Loss: 0.02356095053255558\n",
            "Iteration 8915/10000, Loss: 0.023560253903269768\n",
            "Iteration 8916/10000, Loss: 0.023559560999274254\n",
            "Iteration 8917/10000, Loss: 0.023558860644698143\n",
            "Iteration 8918/10000, Loss: 0.02355816960334778\n",
            "Iteration 8919/10000, Loss: 0.023557471111416817\n",
            "Iteration 8920/10000, Loss: 0.023556774482131004\n",
            "Iteration 8921/10000, Loss: 0.023556075990200043\n",
            "Iteration 8922/10000, Loss: 0.023555384948849678\n",
            "Iteration 8923/10000, Loss: 0.023554686456918716\n",
            "Iteration 8924/10000, Loss: 0.023553991690278053\n",
            "Iteration 8925/10000, Loss: 0.02355329319834709\n",
            "Iteration 8926/10000, Loss: 0.023552600294351578\n",
            "Iteration 8927/10000, Loss: 0.023551901802420616\n",
            "Iteration 8928/10000, Loss: 0.023551208898425102\n",
            "Iteration 8929/10000, Loss: 0.02355051226913929\n",
            "Iteration 8930/10000, Loss: 0.023549817502498627\n",
            "Iteration 8931/10000, Loss: 0.023549120873212814\n",
            "Iteration 8932/10000, Loss: 0.023548424243927002\n",
            "Iteration 8933/10000, Loss: 0.02354772947728634\n",
            "Iteration 8934/10000, Loss: 0.023547032848000526\n",
            "Iteration 8935/10000, Loss: 0.023546339944005013\n",
            "Iteration 8936/10000, Loss: 0.02354564145207405\n",
            "Iteration 8937/10000, Loss: 0.023544946685433388\n",
            "Iteration 8938/10000, Loss: 0.023544251918792725\n",
            "Iteration 8939/10000, Loss: 0.023543555289506912\n",
            "Iteration 8940/10000, Loss: 0.0235428623855114\n",
            "Iteration 8941/10000, Loss: 0.023542163893580437\n",
            "Iteration 8942/10000, Loss: 0.023541467264294624\n",
            "Iteration 8943/10000, Loss: 0.02354077622294426\n",
            "Iteration 8944/10000, Loss: 0.023540079593658447\n",
            "Iteration 8945/10000, Loss: 0.023539384827017784\n",
            "Iteration 8946/10000, Loss: 0.023538686335086823\n",
            "Iteration 8947/10000, Loss: 0.02353799156844616\n",
            "Iteration 8948/10000, Loss: 0.023537298664450645\n",
            "Iteration 8949/10000, Loss: 0.023536602035164833\n",
            "Iteration 8950/10000, Loss: 0.02353590540587902\n",
            "Iteration 8951/10000, Loss: 0.023535210639238358\n",
            "Iteration 8952/10000, Loss: 0.023534515872597694\n",
            "Iteration 8953/10000, Loss: 0.02353382110595703\n",
            "Iteration 8954/10000, Loss: 0.02353312261402607\n",
            "Iteration 8955/10000, Loss: 0.023532431572675705\n",
            "Iteration 8956/10000, Loss: 0.023531734943389893\n",
            "Iteration 8957/10000, Loss: 0.02353104017674923\n",
            "Iteration 8958/10000, Loss: 0.023530345410108566\n",
            "Iteration 8959/10000, Loss: 0.023529650643467903\n",
            "Iteration 8960/10000, Loss: 0.02352895401418209\n",
            "Iteration 8961/10000, Loss: 0.023528259247541428\n",
            "Iteration 8962/10000, Loss: 0.023527562618255615\n",
            "Iteration 8963/10000, Loss: 0.0235268697142601\n",
            "Iteration 8964/10000, Loss: 0.023526174947619438\n",
            "Iteration 8965/10000, Loss: 0.023525476455688477\n",
            "Iteration 8966/10000, Loss: 0.023524783551692963\n",
            "Iteration 8967/10000, Loss: 0.0235240887850523\n",
            "Iteration 8968/10000, Loss: 0.023523392155766487\n",
            "Iteration 8969/10000, Loss: 0.023522697389125824\n",
            "Iteration 8970/10000, Loss: 0.02352200075984001\n",
            "Iteration 8971/10000, Loss: 0.023521307855844498\n",
            "Iteration 8972/10000, Loss: 0.023520614951848984\n",
            "Iteration 8973/10000, Loss: 0.023519916459918022\n",
            "Iteration 8974/10000, Loss: 0.02351922169327736\n",
            "Iteration 8975/10000, Loss: 0.023518530651926994\n",
            "Iteration 8976/10000, Loss: 0.023517832159996033\n",
            "Iteration 8977/10000, Loss: 0.02351713925600052\n",
            "Iteration 8978/10000, Loss: 0.023516444489359856\n",
            "Iteration 8979/10000, Loss: 0.023515749722719193\n",
            "Iteration 8980/10000, Loss: 0.02351505309343338\n",
            "Iteration 8981/10000, Loss: 0.023514356464147568\n",
            "Iteration 8982/10000, Loss: 0.023513663560152054\n",
            "Iteration 8983/10000, Loss: 0.02351296693086624\n",
            "Iteration 8984/10000, Loss: 0.02351227216422558\n",
            "Iteration 8985/10000, Loss: 0.023511581122875214\n",
            "Iteration 8986/10000, Loss: 0.02351088635623455\n",
            "Iteration 8987/10000, Loss: 0.02351018786430359\n",
            "Iteration 8988/10000, Loss: 0.023509494960308075\n",
            "Iteration 8989/10000, Loss: 0.023508798331022263\n",
            "Iteration 8990/10000, Loss: 0.023508107289671898\n",
            "Iteration 8991/10000, Loss: 0.023507412523031235\n",
            "Iteration 8992/10000, Loss: 0.02350671961903572\n",
            "Iteration 8993/10000, Loss: 0.02350602298974991\n",
            "Iteration 8994/10000, Loss: 0.023505328223109245\n",
            "Iteration 8995/10000, Loss: 0.023504633456468582\n",
            "Iteration 8996/10000, Loss: 0.023503940552473068\n",
            "Iteration 8997/10000, Loss: 0.023503245785832405\n",
            "Iteration 8998/10000, Loss: 0.023502551019191742\n",
            "Iteration 8999/10000, Loss: 0.02350185438990593\n",
            "Iteration 9000/10000, Loss: 0.023501163348555565\n",
            "Iteration 9001/10000, Loss: 0.0235004685819149\n",
            "Iteration 9002/10000, Loss: 0.02349977381527424\n",
            "Iteration 9003/10000, Loss: 0.023499077185988426\n",
            "Iteration 9004/10000, Loss: 0.02349838614463806\n",
            "Iteration 9005/10000, Loss: 0.02349768951535225\n",
            "Iteration 9006/10000, Loss: 0.023496994748711586\n",
            "Iteration 9007/10000, Loss: 0.02349630370736122\n",
            "Iteration 9008/10000, Loss: 0.02349560707807541\n",
            "Iteration 9009/10000, Loss: 0.023494914174079895\n",
            "Iteration 9010/10000, Loss: 0.023494219407439232\n",
            "Iteration 9011/10000, Loss: 0.02349352464079857\n",
            "Iteration 9012/10000, Loss: 0.023492829874157906\n",
            "Iteration 9013/10000, Loss: 0.023492135107517242\n",
            "Iteration 9014/10000, Loss: 0.02349144034087658\n",
            "Iteration 9015/10000, Loss: 0.023490747436881065\n",
            "Iteration 9016/10000, Loss: 0.023490052670240402\n",
            "Iteration 9017/10000, Loss: 0.02348935976624489\n",
            "Iteration 9018/10000, Loss: 0.023488664999604225\n",
            "Iteration 9019/10000, Loss: 0.02348797209560871\n",
            "Iteration 9020/10000, Loss: 0.023487277328968048\n",
            "Iteration 9021/10000, Loss: 0.023486582562327385\n",
            "Iteration 9022/10000, Loss: 0.02348588965833187\n",
            "Iteration 9023/10000, Loss: 0.023485194891691208\n",
            "Iteration 9024/10000, Loss: 0.023484501987695694\n",
            "Iteration 9025/10000, Loss: 0.02348380908370018\n",
            "Iteration 9026/10000, Loss: 0.023483112454414368\n",
            "Iteration 9027/10000, Loss: 0.023482419550418854\n",
            "Iteration 9028/10000, Loss: 0.02348172478377819\n",
            "Iteration 9029/10000, Loss: 0.023481030017137527\n",
            "Iteration 9030/10000, Loss: 0.023480338975787163\n",
            "Iteration 9031/10000, Loss: 0.0234796442091465\n",
            "Iteration 9032/10000, Loss: 0.023478949442505836\n",
            "Iteration 9033/10000, Loss: 0.023478256538510323\n",
            "Iteration 9034/10000, Loss: 0.02347755990922451\n",
            "Iteration 9035/10000, Loss: 0.023476867005228996\n",
            "Iteration 9036/10000, Loss: 0.023476174101233482\n",
            "Iteration 9037/10000, Loss: 0.02347547933459282\n",
            "Iteration 9038/10000, Loss: 0.023474784567952156\n",
            "Iteration 9039/10000, Loss: 0.02347409352660179\n",
            "Iteration 9040/10000, Loss: 0.023473400622606277\n",
            "Iteration 9041/10000, Loss: 0.023472705855965614\n",
            "Iteration 9042/10000, Loss: 0.02347201108932495\n",
            "Iteration 9043/10000, Loss: 0.023471318185329437\n",
            "Iteration 9044/10000, Loss: 0.023470627143979073\n",
            "Iteration 9045/10000, Loss: 0.02346993237733841\n",
            "Iteration 9046/10000, Loss: 0.023469239473342896\n",
            "Iteration 9047/10000, Loss: 0.02346854656934738\n",
            "Iteration 9048/10000, Loss: 0.023467855527997017\n",
            "Iteration 9049/10000, Loss: 0.023467160761356354\n",
            "Iteration 9050/10000, Loss: 0.02346646972000599\n",
            "Iteration 9051/10000, Loss: 0.023465774953365326\n",
            "Iteration 9052/10000, Loss: 0.023465082049369812\n",
            "Iteration 9053/10000, Loss: 0.023464391008019447\n",
            "Iteration 9054/10000, Loss: 0.023463694378733635\n",
            "Iteration 9055/10000, Loss: 0.02346300333738327\n",
            "Iteration 9056/10000, Loss: 0.023462310433387756\n",
            "Iteration 9057/10000, Loss: 0.023461617529392242\n",
            "Iteration 9058/10000, Loss: 0.023460926488041878\n",
            "Iteration 9059/10000, Loss: 0.023460233584046364\n",
            "Iteration 9060/10000, Loss: 0.02345954068005085\n",
            "Iteration 9061/10000, Loss: 0.023458845913410187\n",
            "Iteration 9062/10000, Loss: 0.023458153009414673\n",
            "Iteration 9063/10000, Loss: 0.02345745824277401\n",
            "Iteration 9064/10000, Loss: 0.023456769064068794\n",
            "Iteration 9065/10000, Loss: 0.02345607429742813\n",
            "Iteration 9066/10000, Loss: 0.023455383256077766\n",
            "Iteration 9067/10000, Loss: 0.0234546922147274\n",
            "Iteration 9068/10000, Loss: 0.023453999310731888\n",
            "Iteration 9069/10000, Loss: 0.023453304544091225\n",
            "Iteration 9070/10000, Loss: 0.02345261164009571\n",
            "Iteration 9071/10000, Loss: 0.023451918736100197\n",
            "Iteration 9072/10000, Loss: 0.023451225832104683\n",
            "Iteration 9073/10000, Loss: 0.023450536653399467\n",
            "Iteration 9074/10000, Loss: 0.023449841886758804\n",
            "Iteration 9075/10000, Loss: 0.02344914898276329\n",
            "Iteration 9076/10000, Loss: 0.023448454216122627\n",
            "Iteration 9077/10000, Loss: 0.023447761312127113\n",
            "Iteration 9078/10000, Loss: 0.023447072133421898\n",
            "Iteration 9079/10000, Loss: 0.023446377366781235\n",
            "Iteration 9080/10000, Loss: 0.02344568446278572\n",
            "Iteration 9081/10000, Loss: 0.023444993421435356\n",
            "Iteration 9082/10000, Loss: 0.023444296792149544\n",
            "Iteration 9083/10000, Loss: 0.02344360761344433\n",
            "Iteration 9084/10000, Loss: 0.023442916572093964\n",
            "Iteration 9085/10000, Loss: 0.0234422255307436\n",
            "Iteration 9086/10000, Loss: 0.023441530764102936\n",
            "Iteration 9087/10000, Loss: 0.023440835997462273\n",
            "Iteration 9088/10000, Loss: 0.023440148681402206\n",
            "Iteration 9089/10000, Loss: 0.023439453914761543\n",
            "Iteration 9090/10000, Loss: 0.02343875914812088\n",
            "Iteration 9091/10000, Loss: 0.023438068106770515\n",
            "Iteration 9092/10000, Loss: 0.023437375202775\n",
            "Iteration 9093/10000, Loss: 0.023436684161424637\n",
            "Iteration 9094/10000, Loss: 0.023435989394783974\n",
            "Iteration 9095/10000, Loss: 0.02343529835343361\n",
            "Iteration 9096/10000, Loss: 0.023434607312083244\n",
            "Iteration 9097/10000, Loss: 0.02343391440808773\n",
            "Iteration 9098/10000, Loss: 0.023433225229382515\n",
            "Iteration 9099/10000, Loss: 0.023432528600096703\n",
            "Iteration 9100/10000, Loss: 0.023431839421391487\n",
            "Iteration 9101/10000, Loss: 0.023431144654750824\n",
            "Iteration 9102/10000, Loss: 0.02343045361340046\n",
            "Iteration 9103/10000, Loss: 0.023429762572050095\n",
            "Iteration 9104/10000, Loss: 0.02342907525599003\n",
            "Iteration 9105/10000, Loss: 0.023428384214639664\n",
            "Iteration 9106/10000, Loss: 0.023427695035934448\n",
            "Iteration 9107/10000, Loss: 0.023427003994584084\n",
            "Iteration 9108/10000, Loss: 0.023426314815878868\n",
            "Iteration 9109/10000, Loss: 0.023425627499818802\n",
            "Iteration 9110/10000, Loss: 0.023424936458468437\n",
            "Iteration 9111/10000, Loss: 0.023424245417118073\n",
            "Iteration 9112/10000, Loss: 0.023423554375767708\n",
            "Iteration 9113/10000, Loss: 0.02342286705970764\n",
            "Iteration 9114/10000, Loss: 0.023422177881002426\n",
            "Iteration 9115/10000, Loss: 0.02342148870229721\n",
            "Iteration 9116/10000, Loss: 0.023420797660946846\n",
            "Iteration 9117/10000, Loss: 0.02342010848224163\n",
            "Iteration 9118/10000, Loss: 0.023419419303536415\n",
            "Iteration 9119/10000, Loss: 0.0234187301248312\n",
            "Iteration 9120/10000, Loss: 0.023418040946125984\n",
            "Iteration 9121/10000, Loss: 0.02341734804213047\n",
            "Iteration 9122/10000, Loss: 0.023416658863425255\n",
            "Iteration 9123/10000, Loss: 0.02341597154736519\n",
            "Iteration 9124/10000, Loss: 0.023415282368659973\n",
            "Iteration 9125/10000, Loss: 0.02341459132730961\n",
            "Iteration 9126/10000, Loss: 0.023413904011249542\n",
            "Iteration 9127/10000, Loss: 0.023413212969899178\n",
            "Iteration 9128/10000, Loss: 0.023412523791193962\n",
            "Iteration 9129/10000, Loss: 0.023411834612488747\n",
            "Iteration 9130/10000, Loss: 0.02341114543378353\n",
            "Iteration 9131/10000, Loss: 0.023410458117723465\n",
            "Iteration 9132/10000, Loss: 0.0234097670763731\n",
            "Iteration 9133/10000, Loss: 0.023409074172377586\n",
            "Iteration 9134/10000, Loss: 0.02340838685631752\n",
            "Iteration 9135/10000, Loss: 0.023407699540257454\n",
            "Iteration 9136/10000, Loss: 0.02340700849890709\n",
            "Iteration 9137/10000, Loss: 0.023406319320201874\n",
            "Iteration 9138/10000, Loss: 0.02340563014149666\n",
            "Iteration 9139/10000, Loss: 0.023404940962791443\n",
            "Iteration 9140/10000, Loss: 0.023404249921441078\n",
            "Iteration 9141/10000, Loss: 0.023403562605381012\n",
            "Iteration 9142/10000, Loss: 0.023402873426675797\n",
            "Iteration 9143/10000, Loss: 0.02340218611061573\n",
            "Iteration 9144/10000, Loss: 0.023401493206620216\n",
            "Iteration 9145/10000, Loss: 0.0234008077532053\n",
            "Iteration 9146/10000, Loss: 0.023400114849209785\n",
            "Iteration 9147/10000, Loss: 0.02339942753314972\n",
            "Iteration 9148/10000, Loss: 0.023398738354444504\n",
            "Iteration 9149/10000, Loss: 0.023398051038384438\n",
            "Iteration 9150/10000, Loss: 0.023397359997034073\n",
            "Iteration 9151/10000, Loss: 0.023396672680974007\n",
            "Iteration 9152/10000, Loss: 0.023395981639623642\n",
            "Iteration 9153/10000, Loss: 0.023395292460918427\n",
            "Iteration 9154/10000, Loss: 0.02339460514485836\n",
            "Iteration 9155/10000, Loss: 0.023393915966153145\n",
            "Iteration 9156/10000, Loss: 0.02339322678744793\n",
            "Iteration 9157/10000, Loss: 0.023392537608742714\n",
            "Iteration 9158/10000, Loss: 0.02339184656739235\n",
            "Iteration 9159/10000, Loss: 0.023391159251332283\n",
            "Iteration 9160/10000, Loss: 0.023390473797917366\n",
            "Iteration 9161/10000, Loss: 0.023389782756567\n",
            "Iteration 9162/10000, Loss: 0.023389093577861786\n",
            "Iteration 9163/10000, Loss: 0.02338840626180172\n",
            "Iteration 9164/10000, Loss: 0.023387717083096504\n",
            "Iteration 9165/10000, Loss: 0.02338702790439129\n",
            "Iteration 9166/10000, Loss: 0.023386338725686073\n",
            "Iteration 9167/10000, Loss: 0.023385649546980858\n",
            "Iteration 9168/10000, Loss: 0.02338496223092079\n",
            "Iteration 9169/10000, Loss: 0.023384274914860725\n",
            "Iteration 9170/10000, Loss: 0.02338358387351036\n",
            "Iteration 9171/10000, Loss: 0.023382896557450294\n",
            "Iteration 9172/10000, Loss: 0.02338220551609993\n",
            "Iteration 9173/10000, Loss: 0.023381518200039864\n",
            "Iteration 9174/10000, Loss: 0.023380830883979797\n",
            "Iteration 9175/10000, Loss: 0.02338014543056488\n",
            "Iteration 9176/10000, Loss: 0.023379450663924217\n",
            "Iteration 9177/10000, Loss: 0.02337876334786415\n",
            "Iteration 9178/10000, Loss: 0.023378077894449234\n",
            "Iteration 9179/10000, Loss: 0.02337738685309887\n",
            "Iteration 9180/10000, Loss: 0.023376697674393654\n",
            "Iteration 9181/10000, Loss: 0.023376014083623886\n",
            "Iteration 9182/10000, Loss: 0.02337532304227352\n",
            "Iteration 9183/10000, Loss: 0.023374633863568306\n",
            "Iteration 9184/10000, Loss: 0.02337394468486309\n",
            "Iteration 9185/10000, Loss: 0.023373259231448174\n",
            "Iteration 9186/10000, Loss: 0.023372570052742958\n",
            "Iteration 9187/10000, Loss: 0.023371879011392593\n",
            "Iteration 9188/10000, Loss: 0.023371191695332527\n",
            "Iteration 9189/10000, Loss: 0.02337050251662731\n",
            "Iteration 9190/10000, Loss: 0.023369817063212395\n",
            "Iteration 9191/10000, Loss: 0.02336912415921688\n",
            "Iteration 9192/10000, Loss: 0.023368436843156815\n",
            "Iteration 9193/10000, Loss: 0.023367751389741898\n",
            "Iteration 9194/10000, Loss: 0.023367062211036682\n",
            "Iteration 9195/10000, Loss: 0.023366374894976616\n",
            "Iteration 9196/10000, Loss: 0.0233656857162714\n",
            "Iteration 9197/10000, Loss: 0.023364998400211334\n",
            "Iteration 9198/10000, Loss: 0.02336430922150612\n",
            "Iteration 9199/10000, Loss: 0.023363621905446053\n",
            "Iteration 9200/10000, Loss: 0.023362934589385986\n",
            "Iteration 9201/10000, Loss: 0.02336224727332592\n",
            "Iteration 9202/10000, Loss: 0.023361556231975555\n",
            "Iteration 9203/10000, Loss: 0.02336086891591549\n",
            "Iteration 9204/10000, Loss: 0.023360181599855423\n",
            "Iteration 9205/10000, Loss: 0.023359494283795357\n",
            "Iteration 9206/10000, Loss: 0.023358803242444992\n",
            "Iteration 9207/10000, Loss: 0.023358119651675224\n",
            "Iteration 9208/10000, Loss: 0.02335742861032486\n",
            "Iteration 9209/10000, Loss: 0.023356741294264793\n",
            "Iteration 9210/10000, Loss: 0.023356052115559578\n",
            "Iteration 9211/10000, Loss: 0.02335536479949951\n",
            "Iteration 9212/10000, Loss: 0.023354677483439445\n",
            "Iteration 9213/10000, Loss: 0.02335398644208908\n",
            "Iteration 9214/10000, Loss: 0.023353302851319313\n",
            "Iteration 9215/10000, Loss: 0.023352613672614098\n",
            "Iteration 9216/10000, Loss: 0.023351924493908882\n",
            "Iteration 9217/10000, Loss: 0.023351237177848816\n",
            "Iteration 9218/10000, Loss: 0.02335054986178875\n",
            "Iteration 9219/10000, Loss: 0.023349864408373833\n",
            "Iteration 9220/10000, Loss: 0.02334917150437832\n",
            "Iteration 9221/10000, Loss: 0.0233484897762537\n",
            "Iteration 9222/10000, Loss: 0.023347800597548485\n",
            "Iteration 9223/10000, Loss: 0.023347115144133568\n",
            "Iteration 9224/10000, Loss: 0.023346420377492905\n",
            "Iteration 9225/10000, Loss: 0.023345736786723137\n",
            "Iteration 9226/10000, Loss: 0.02334504947066307\n",
            "Iteration 9227/10000, Loss: 0.023344360291957855\n",
            "Iteration 9228/10000, Loss: 0.023343674838542938\n",
            "Iteration 9229/10000, Loss: 0.023342987522482872\n",
            "Iteration 9230/10000, Loss: 0.023342300206422806\n",
            "Iteration 9231/10000, Loss: 0.02334161102771759\n",
            "Iteration 9232/10000, Loss: 0.023340925574302673\n",
            "Iteration 9233/10000, Loss: 0.023340236395597458\n",
            "Iteration 9234/10000, Loss: 0.02333954907953739\n",
            "Iteration 9235/10000, Loss: 0.023338863626122475\n",
            "Iteration 9236/10000, Loss: 0.02333817444741726\n",
            "Iteration 9237/10000, Loss: 0.023337487131357193\n",
            "Iteration 9238/10000, Loss: 0.023336797952651978\n",
            "Iteration 9239/10000, Loss: 0.02333611436188221\n",
            "Iteration 9240/10000, Loss: 0.023335425183176994\n",
            "Iteration 9241/10000, Loss: 0.023334737867116928\n",
            "Iteration 9242/10000, Loss: 0.02333405241370201\n",
            "Iteration 9243/10000, Loss: 0.023333365097641945\n",
            "Iteration 9244/10000, Loss: 0.02333267778158188\n",
            "Iteration 9245/10000, Loss: 0.023331990465521812\n",
            "Iteration 9246/10000, Loss: 0.023331306874752045\n",
            "Iteration 9247/10000, Loss: 0.02333061955869198\n",
            "Iteration 9248/10000, Loss: 0.023329930379986763\n",
            "Iteration 9249/10000, Loss: 0.023329241201281548\n",
            "Iteration 9250/10000, Loss: 0.02332855761051178\n",
            "Iteration 9251/10000, Loss: 0.023327872157096863\n",
            "Iteration 9252/10000, Loss: 0.023327184841036797\n",
            "Iteration 9253/10000, Loss: 0.02332649752497673\n",
            "Iteration 9254/10000, Loss: 0.023325810208916664\n",
            "Iteration 9255/10000, Loss: 0.023325122892856598\n",
            "Iteration 9256/10000, Loss: 0.02332443743944168\n",
            "Iteration 9257/10000, Loss: 0.023323753848671913\n",
            "Iteration 9258/10000, Loss: 0.02332306280732155\n",
            "Iteration 9259/10000, Loss: 0.02332237735390663\n",
            "Iteration 9260/10000, Loss: 0.023321690037846565\n",
            "Iteration 9261/10000, Loss: 0.023321004584431648\n",
            "Iteration 9262/10000, Loss: 0.023320315405726433\n",
            "Iteration 9263/10000, Loss: 0.023319631814956665\n",
            "Iteration 9264/10000, Loss: 0.0233189444988966\n",
            "Iteration 9265/10000, Loss: 0.023318259045481682\n",
            "Iteration 9266/10000, Loss: 0.023317569866776466\n",
            "Iteration 9267/10000, Loss: 0.0233168862760067\n",
            "Iteration 9268/10000, Loss: 0.023316198959946632\n",
            "Iteration 9269/10000, Loss: 0.023315511643886566\n",
            "Iteration 9270/10000, Loss: 0.02331482246518135\n",
            "Iteration 9271/10000, Loss: 0.023314138874411583\n",
            "Iteration 9272/10000, Loss: 0.023313451558351517\n",
            "Iteration 9273/10000, Loss: 0.02331276424229145\n",
            "Iteration 9274/10000, Loss: 0.023312078788876534\n",
            "Iteration 9275/10000, Loss: 0.023311393335461617\n",
            "Iteration 9276/10000, Loss: 0.0233107078820467\n",
            "Iteration 9277/10000, Loss: 0.023310020565986633\n",
            "Iteration 9278/10000, Loss: 0.023309335112571716\n",
            "Iteration 9279/10000, Loss: 0.02330865152180195\n",
            "Iteration 9280/10000, Loss: 0.023307964205741882\n",
            "Iteration 9281/10000, Loss: 0.023307276889681816\n",
            "Iteration 9282/10000, Loss: 0.0233065914362669\n",
            "Iteration 9283/10000, Loss: 0.023305905982851982\n",
            "Iteration 9284/10000, Loss: 0.023305218666791916\n",
            "Iteration 9285/10000, Loss: 0.02330453135073185\n",
            "Iteration 9286/10000, Loss: 0.023303845897316933\n",
            "Iteration 9287/10000, Loss: 0.023303160443902016\n",
            "Iteration 9288/10000, Loss: 0.02330247312784195\n",
            "Iteration 9289/10000, Loss: 0.02330178953707218\n",
            "Iteration 9290/10000, Loss: 0.023301102221012115\n",
            "Iteration 9291/10000, Loss: 0.0233004167675972\n",
            "Iteration 9292/10000, Loss: 0.023299729451537132\n",
            "Iteration 9293/10000, Loss: 0.023299043998122215\n",
            "Iteration 9294/10000, Loss: 0.0232983585447073\n",
            "Iteration 9295/10000, Loss: 0.02329767309129238\n",
            "Iteration 9296/10000, Loss: 0.023296987637877464\n",
            "Iteration 9297/10000, Loss: 0.023296300321817398\n",
            "Iteration 9298/10000, Loss: 0.023295613005757332\n",
            "Iteration 9299/10000, Loss: 0.023294931277632713\n",
            "Iteration 9300/10000, Loss: 0.023294243961572647\n",
            "Iteration 9301/10000, Loss: 0.02329355664551258\n",
            "Iteration 9302/10000, Loss: 0.023292873054742813\n",
            "Iteration 9303/10000, Loss: 0.023292183876037598\n",
            "Iteration 9304/10000, Loss: 0.02329149842262268\n",
            "Iteration 9305/10000, Loss: 0.023290814831852913\n",
            "Iteration 9306/10000, Loss: 0.023290127515792847\n",
            "Iteration 9307/10000, Loss: 0.02328944392502308\n",
            "Iteration 9308/10000, Loss: 0.023288758471608162\n",
            "Iteration 9309/10000, Loss: 0.023288069292902946\n",
            "Iteration 9310/10000, Loss: 0.02328738570213318\n",
            "Iteration 9311/10000, Loss: 0.02328670024871826\n",
            "Iteration 9312/10000, Loss: 0.023286014795303345\n",
            "Iteration 9313/10000, Loss: 0.023285329341888428\n",
            "Iteration 9314/10000, Loss: 0.02328464575111866\n",
            "Iteration 9315/10000, Loss: 0.023283956572413445\n",
            "Iteration 9316/10000, Loss: 0.02328326925635338\n",
            "Iteration 9317/10000, Loss: 0.02328258566558361\n",
            "Iteration 9318/10000, Loss: 0.023281900212168694\n",
            "Iteration 9319/10000, Loss: 0.023281214758753777\n",
            "Iteration 9320/10000, Loss: 0.02328052930533886\n",
            "Iteration 9321/10000, Loss: 0.023279843851923943\n",
            "Iteration 9322/10000, Loss: 0.023279160261154175\n",
            "Iteration 9323/10000, Loss: 0.02327847294509411\n",
            "Iteration 9324/10000, Loss: 0.02327778749167919\n",
            "Iteration 9325/10000, Loss: 0.023277102038264275\n",
            "Iteration 9326/10000, Loss: 0.023276416584849358\n",
            "Iteration 9327/10000, Loss: 0.02327573113143444\n",
            "Iteration 9328/10000, Loss: 0.023275047540664673\n",
            "Iteration 9329/10000, Loss: 0.023274362087249756\n",
            "Iteration 9330/10000, Loss: 0.02327367290854454\n",
            "Iteration 9331/10000, Loss: 0.023272989317774773\n",
            "Iteration 9332/10000, Loss: 0.023272303864359856\n",
            "Iteration 9333/10000, Loss: 0.023271620273590088\n",
            "Iteration 9334/10000, Loss: 0.02327093482017517\n",
            "Iteration 9335/10000, Loss: 0.023270251229405403\n",
            "Iteration 9336/10000, Loss: 0.023269565775990486\n",
            "Iteration 9337/10000, Loss: 0.02326888032257557\n",
            "Iteration 9338/10000, Loss: 0.023268193006515503\n",
            "Iteration 9339/10000, Loss: 0.023267511278390884\n",
            "Iteration 9340/10000, Loss: 0.023266823962330818\n",
            "Iteration 9341/10000, Loss: 0.0232661385089159\n",
            "Iteration 9342/10000, Loss: 0.023265453055500984\n",
            "Iteration 9343/10000, Loss: 0.023264769464731216\n",
            "Iteration 9344/10000, Loss: 0.02326408214867115\n",
            "Iteration 9345/10000, Loss: 0.023263398557901382\n",
            "Iteration 9346/10000, Loss: 0.023262711241841316\n",
            "Iteration 9347/10000, Loss: 0.023262031376361847\n",
            "Iteration 9348/10000, Loss: 0.02326134406030178\n",
            "Iteration 9349/10000, Loss: 0.023260658606886864\n",
            "Iteration 9350/10000, Loss: 0.023259971290826797\n",
            "Iteration 9351/10000, Loss: 0.02325928956270218\n",
            "Iteration 9352/10000, Loss: 0.023258604109287262\n",
            "Iteration 9353/10000, Loss: 0.023257920518517494\n",
            "Iteration 9354/10000, Loss: 0.023257235065102577\n",
            "Iteration 9355/10000, Loss: 0.02325655147433281\n",
            "Iteration 9356/10000, Loss: 0.023255864158272743\n",
            "Iteration 9357/10000, Loss: 0.023255180567502975\n",
            "Iteration 9358/10000, Loss: 0.02325449511408806\n",
            "Iteration 9359/10000, Loss: 0.02325381152331829\n",
            "Iteration 9360/10000, Loss: 0.023253126069903374\n",
            "Iteration 9361/10000, Loss: 0.023252442479133606\n",
            "Iteration 9362/10000, Loss: 0.02325175516307354\n",
            "Iteration 9363/10000, Loss: 0.02325107343494892\n",
            "Iteration 9364/10000, Loss: 0.023250386118888855\n",
            "Iteration 9365/10000, Loss: 0.023249702528119087\n",
            "Iteration 9366/10000, Loss: 0.02324901893734932\n",
            "Iteration 9367/10000, Loss: 0.023248333483934402\n",
            "Iteration 9368/10000, Loss: 0.023247648030519485\n",
            "Iteration 9369/10000, Loss: 0.023246964439749718\n",
            "Iteration 9370/10000, Loss: 0.0232462789863348\n",
            "Iteration 9371/10000, Loss: 0.023245595395565033\n",
            "Iteration 9372/10000, Loss: 0.023244909942150116\n",
            "Iteration 9373/10000, Loss: 0.023244226351380348\n",
            "Iteration 9374/10000, Loss: 0.02324354089796543\n",
            "Iteration 9375/10000, Loss: 0.023242853581905365\n",
            "Iteration 9376/10000, Loss: 0.023242171853780746\n",
            "Iteration 9377/10000, Loss: 0.02324148640036583\n",
            "Iteration 9378/10000, Loss: 0.023240800946950912\n",
            "Iteration 9379/10000, Loss: 0.023240119218826294\n",
            "Iteration 9380/10000, Loss: 0.023239433765411377\n",
            "Iteration 9381/10000, Loss: 0.02323875203728676\n",
            "Iteration 9382/10000, Loss: 0.02323806658387184\n",
            "Iteration 9383/10000, Loss: 0.023237379267811775\n",
            "Iteration 9384/10000, Loss: 0.023236695677042007\n",
            "Iteration 9385/10000, Loss: 0.02323601208627224\n",
            "Iteration 9386/10000, Loss: 0.023235328495502472\n",
            "Iteration 9387/10000, Loss: 0.023234644904732704\n",
            "Iteration 9388/10000, Loss: 0.023233965039253235\n",
            "Iteration 9389/10000, Loss: 0.023233283311128616\n",
            "Iteration 9390/10000, Loss: 0.0232325978577137\n",
            "Iteration 9391/10000, Loss: 0.02323191426694393\n",
            "Iteration 9392/10000, Loss: 0.023231234401464462\n",
            "Iteration 9393/10000, Loss: 0.023230552673339844\n",
            "Iteration 9394/10000, Loss: 0.023229870945215225\n",
            "Iteration 9395/10000, Loss: 0.023229192942380905\n",
            "Iteration 9396/10000, Loss: 0.023228507488965988\n",
            "Iteration 9397/10000, Loss: 0.02322782762348652\n",
            "Iteration 9398/10000, Loss: 0.023227142170071602\n",
            "Iteration 9399/10000, Loss: 0.023226462304592133\n",
            "Iteration 9400/10000, Loss: 0.023225780576467514\n",
            "Iteration 9401/10000, Loss: 0.023225098848342896\n",
            "Iteration 9402/10000, Loss: 0.023224418982863426\n",
            "Iteration 9403/10000, Loss: 0.02322373539209366\n",
            "Iteration 9404/10000, Loss: 0.02322305366396904\n",
            "Iteration 9405/10000, Loss: 0.02322237379848957\n",
            "Iteration 9406/10000, Loss: 0.023221692070364952\n",
            "Iteration 9407/10000, Loss: 0.023221010342240334\n",
            "Iteration 9408/10000, Loss: 0.023220326751470566\n",
            "Iteration 9409/10000, Loss: 0.023219645023345947\n",
            "Iteration 9410/10000, Loss: 0.02321896143257618\n",
            "Iteration 9411/10000, Loss: 0.02321828156709671\n",
            "Iteration 9412/10000, Loss: 0.02321760170161724\n",
            "Iteration 9413/10000, Loss: 0.023216919973492622\n",
            "Iteration 9414/10000, Loss: 0.023216238245368004\n",
            "Iteration 9415/10000, Loss: 0.023215554654598236\n",
            "Iteration 9416/10000, Loss: 0.023214874789118767\n",
            "Iteration 9417/10000, Loss: 0.023214193060994148\n",
            "Iteration 9418/10000, Loss: 0.02321351133286953\n",
            "Iteration 9419/10000, Loss: 0.02321283146739006\n",
            "Iteration 9420/10000, Loss: 0.023212149739265442\n",
            "Iteration 9421/10000, Loss: 0.023211468011140823\n",
            "Iteration 9422/10000, Loss: 0.023210784420371056\n",
            "Iteration 9423/10000, Loss: 0.023210104554891586\n",
            "Iteration 9424/10000, Loss: 0.02320942096412182\n",
            "Iteration 9425/10000, Loss: 0.02320874109864235\n",
            "Iteration 9426/10000, Loss: 0.02320805750787258\n",
            "Iteration 9427/10000, Loss: 0.02320738136768341\n",
            "Iteration 9428/10000, Loss: 0.023206697776913643\n",
            "Iteration 9429/10000, Loss: 0.023206014186143875\n",
            "Iteration 9430/10000, Loss: 0.023205336183309555\n",
            "Iteration 9431/10000, Loss: 0.023204652592539787\n",
            "Iteration 9432/10000, Loss: 0.02320397086441517\n",
            "Iteration 9433/10000, Loss: 0.0232032872736454\n",
            "Iteration 9434/10000, Loss: 0.02320261113345623\n",
            "Iteration 9435/10000, Loss: 0.02320192940533161\n",
            "Iteration 9436/10000, Loss: 0.023201245814561844\n",
            "Iteration 9437/10000, Loss: 0.023200567811727524\n",
            "Iteration 9438/10000, Loss: 0.023199886083602905\n",
            "Iteration 9439/10000, Loss: 0.023199204355478287\n",
            "Iteration 9440/10000, Loss: 0.023198524489998817\n",
            "Iteration 9441/10000, Loss: 0.0231978427618742\n",
            "Iteration 9442/10000, Loss: 0.02319716289639473\n",
            "Iteration 9443/10000, Loss: 0.023196479305624962\n",
            "Iteration 9444/10000, Loss: 0.023195801302790642\n",
            "Iteration 9445/10000, Loss: 0.023195121437311172\n",
            "Iteration 9446/10000, Loss: 0.023194441571831703\n",
            "Iteration 9447/10000, Loss: 0.023193759843707085\n",
            "Iteration 9448/10000, Loss: 0.023193079978227615\n",
            "Iteration 9449/10000, Loss: 0.023192400112748146\n",
            "Iteration 9450/10000, Loss: 0.023191718384623528\n",
            "Iteration 9451/10000, Loss: 0.023191038519144058\n",
            "Iteration 9452/10000, Loss: 0.02319035865366459\n",
            "Iteration 9453/10000, Loss: 0.02318967692553997\n",
            "Iteration 9454/10000, Loss: 0.023188995197415352\n",
            "Iteration 9455/10000, Loss: 0.023188317194581032\n",
            "Iteration 9456/10000, Loss: 0.023187635466456413\n",
            "Iteration 9457/10000, Loss: 0.023186953738331795\n",
            "Iteration 9458/10000, Loss: 0.023186275735497475\n",
            "Iteration 9459/10000, Loss: 0.023185594007372856\n",
            "Iteration 9460/10000, Loss: 0.023184914141893387\n",
            "Iteration 9461/10000, Loss: 0.023184234276413918\n",
            "Iteration 9462/10000, Loss: 0.0231835525482893\n",
            "Iteration 9463/10000, Loss: 0.02318287454545498\n",
            "Iteration 9464/10000, Loss: 0.02318219281733036\n",
            "Iteration 9465/10000, Loss: 0.02318151481449604\n",
            "Iteration 9466/10000, Loss: 0.023180831223726273\n",
            "Iteration 9467/10000, Loss: 0.023180153220891953\n",
            "Iteration 9468/10000, Loss: 0.023179473355412483\n",
            "Iteration 9469/10000, Loss: 0.023178793489933014\n",
            "Iteration 9470/10000, Loss: 0.023178111761808395\n",
            "Iteration 9471/10000, Loss: 0.023177430033683777\n",
            "Iteration 9472/10000, Loss: 0.023176753893494606\n",
            "Iteration 9473/10000, Loss: 0.023176072165369987\n",
            "Iteration 9474/10000, Loss: 0.02317539043724537\n",
            "Iteration 9475/10000, Loss: 0.0231747105717659\n",
            "Iteration 9476/10000, Loss: 0.023174036294221878\n",
            "Iteration 9477/10000, Loss: 0.02317335084080696\n",
            "Iteration 9478/10000, Loss: 0.023172670975327492\n",
            "Iteration 9479/10000, Loss: 0.023171991109848022\n",
            "Iteration 9480/10000, Loss: 0.023171309381723404\n",
            "Iteration 9481/10000, Loss: 0.023170631378889084\n",
            "Iteration 9482/10000, Loss: 0.023169949650764465\n",
            "Iteration 9483/10000, Loss: 0.023169271647930145\n",
            "Iteration 9484/10000, Loss: 0.023168591782450676\n",
            "Iteration 9485/10000, Loss: 0.023167910054326057\n",
            "Iteration 9486/10000, Loss: 0.023167232051491737\n",
            "Iteration 9487/10000, Loss: 0.023166552186012268\n",
            "Iteration 9488/10000, Loss: 0.0231658723205328\n",
            "Iteration 9489/10000, Loss: 0.02316519059240818\n",
            "Iteration 9490/10000, Loss: 0.02316451258957386\n",
            "Iteration 9491/10000, Loss: 0.02316383272409439\n",
            "Iteration 9492/10000, Loss: 0.023163150995969772\n",
            "Iteration 9493/10000, Loss: 0.023162472993135452\n",
            "Iteration 9494/10000, Loss: 0.023161794990301132\n",
            "Iteration 9495/10000, Loss: 0.023161111399531364\n",
            "Iteration 9496/10000, Loss: 0.023160433396697044\n",
            "Iteration 9497/10000, Loss: 0.023159755393862724\n",
            "Iteration 9498/10000, Loss: 0.023159075528383255\n",
            "Iteration 9499/10000, Loss: 0.023158395662903786\n",
            "Iteration 9500/10000, Loss: 0.023157715797424316\n",
            "Iteration 9501/10000, Loss: 0.023157035931944847\n",
            "Iteration 9502/10000, Loss: 0.023156356066465378\n",
            "Iteration 9503/10000, Loss: 0.02315567620098591\n",
            "Iteration 9504/10000, Loss: 0.02315499633550644\n",
            "Iteration 9505/10000, Loss: 0.02315431647002697\n",
            "Iteration 9506/10000, Loss: 0.02315363846719265\n",
            "Iteration 9507/10000, Loss: 0.02315295860171318\n",
            "Iteration 9508/10000, Loss: 0.02315227873623371\n",
            "Iteration 9509/10000, Loss: 0.023151598870754242\n",
            "Iteration 9510/10000, Loss: 0.023150919005274773\n",
            "Iteration 9511/10000, Loss: 0.023150241002440453\n",
            "Iteration 9512/10000, Loss: 0.023149561136960983\n",
            "Iteration 9513/10000, Loss: 0.023148884996771812\n",
            "Iteration 9514/10000, Loss: 0.023148203268647194\n",
            "Iteration 9515/10000, Loss: 0.023147525265812874\n",
            "Iteration 9516/10000, Loss: 0.023146841675043106\n",
            "Iteration 9517/10000, Loss: 0.023146167397499084\n",
            "Iteration 9518/10000, Loss: 0.023145487532019615\n",
            "Iteration 9519/10000, Loss: 0.023144803941249847\n",
            "Iteration 9520/10000, Loss: 0.023144125938415527\n",
            "Iteration 9521/10000, Loss: 0.023143447935581207\n",
            "Iteration 9522/10000, Loss: 0.023142768070101738\n",
            "Iteration 9523/10000, Loss: 0.023142090067267418\n",
            "Iteration 9524/10000, Loss: 0.02314141020178795\n",
            "Iteration 9525/10000, Loss: 0.02314073033630848\n",
            "Iteration 9526/10000, Loss: 0.02314005233347416\n",
            "Iteration 9527/10000, Loss: 0.02313937246799469\n",
            "Iteration 9528/10000, Loss: 0.02313869260251522\n",
            "Iteration 9529/10000, Loss: 0.02313801646232605\n",
            "Iteration 9530/10000, Loss: 0.02313733659684658\n",
            "Iteration 9531/10000, Loss: 0.02313665673136711\n",
            "Iteration 9532/10000, Loss: 0.02313598059117794\n",
            "Iteration 9533/10000, Loss: 0.023135298863053322\n",
            "Iteration 9534/10000, Loss: 0.023134617134928703\n",
            "Iteration 9535/10000, Loss: 0.023133940994739532\n",
            "Iteration 9536/10000, Loss: 0.023133262991905212\n",
            "Iteration 9537/10000, Loss: 0.023132583126425743\n",
            "Iteration 9538/10000, Loss: 0.023131903260946274\n",
            "Iteration 9539/10000, Loss: 0.023131227120757103\n",
            "Iteration 9540/10000, Loss: 0.023130547255277634\n",
            "Iteration 9541/10000, Loss: 0.023129865527153015\n",
            "Iteration 9542/10000, Loss: 0.023129189386963844\n",
            "Iteration 9543/10000, Loss: 0.023128509521484375\n",
            "Iteration 9544/10000, Loss: 0.023127829656004906\n",
            "Iteration 9545/10000, Loss: 0.023127151653170586\n",
            "Iteration 9546/10000, Loss: 0.023126473650336266\n",
            "Iteration 9547/10000, Loss: 0.023125793784856796\n",
            "Iteration 9548/10000, Loss: 0.023125115782022476\n",
            "Iteration 9549/10000, Loss: 0.023124439641833305\n",
            "Iteration 9550/10000, Loss: 0.023123756051063538\n",
            "Iteration 9551/10000, Loss: 0.023123078048229218\n",
            "Iteration 9552/10000, Loss: 0.023122400045394897\n",
            "Iteration 9553/10000, Loss: 0.023121720179915428\n",
            "Iteration 9554/10000, Loss: 0.02312104031443596\n",
            "Iteration 9555/10000, Loss: 0.02312036231160164\n",
            "Iteration 9556/10000, Loss: 0.023119686171412468\n",
            "Iteration 9557/10000, Loss: 0.023119006305933\n",
            "Iteration 9558/10000, Loss: 0.02311832644045353\n",
            "Iteration 9559/10000, Loss: 0.02311765030026436\n",
            "Iteration 9560/10000, Loss: 0.023116974160075188\n",
            "Iteration 9561/10000, Loss: 0.02311629429459572\n",
            "Iteration 9562/10000, Loss: 0.0231156125664711\n",
            "Iteration 9563/10000, Loss: 0.023114938288927078\n",
            "Iteration 9564/10000, Loss: 0.02311425656080246\n",
            "Iteration 9565/10000, Loss: 0.02311357855796814\n",
            "Iteration 9566/10000, Loss: 0.02311290241777897\n",
            "Iteration 9567/10000, Loss: 0.02311222441494465\n",
            "Iteration 9568/10000, Loss: 0.02311154641211033\n",
            "Iteration 9569/10000, Loss: 0.02311086468398571\n",
            "Iteration 9570/10000, Loss: 0.02311018854379654\n",
            "Iteration 9571/10000, Loss: 0.02310950867831707\n",
            "Iteration 9572/10000, Loss: 0.0231088325381279\n",
            "Iteration 9573/10000, Loss: 0.02310815267264843\n",
            "Iteration 9574/10000, Loss: 0.02310747466981411\n",
            "Iteration 9575/10000, Loss: 0.02310679480433464\n",
            "Iteration 9576/10000, Loss: 0.02310612052679062\n",
            "Iteration 9577/10000, Loss: 0.0231054425239563\n",
            "Iteration 9578/10000, Loss: 0.02310476079583168\n",
            "Iteration 9579/10000, Loss: 0.02310408465564251\n",
            "Iteration 9580/10000, Loss: 0.02310340665280819\n",
            "Iteration 9581/10000, Loss: 0.02310272678732872\n",
            "Iteration 9582/10000, Loss: 0.0231020487844944\n",
            "Iteration 9583/10000, Loss: 0.02310137264430523\n",
            "Iteration 9584/10000, Loss: 0.02310069464147091\n",
            "Iteration 9585/10000, Loss: 0.02310001663863659\n",
            "Iteration 9586/10000, Loss: 0.02309933491051197\n",
            "Iteration 9587/10000, Loss: 0.02309866063296795\n",
            "Iteration 9588/10000, Loss: 0.02309798263013363\n",
            "Iteration 9589/10000, Loss: 0.02309730276465416\n",
            "Iteration 9590/10000, Loss: 0.02309662476181984\n",
            "Iteration 9591/10000, Loss: 0.02309594862163067\n",
            "Iteration 9592/10000, Loss: 0.02309527061879635\n",
            "Iteration 9593/10000, Loss: 0.02309459075331688\n",
            "Iteration 9594/10000, Loss: 0.023093916475772858\n",
            "Iteration 9595/10000, Loss: 0.02309323661029339\n",
            "Iteration 9596/10000, Loss: 0.02309255488216877\n",
            "Iteration 9597/10000, Loss: 0.023091882467269897\n",
            "Iteration 9598/10000, Loss: 0.023091202601790428\n",
            "Iteration 9599/10000, Loss: 0.02309052273631096\n",
            "Iteration 9600/10000, Loss: 0.02308984473347664\n",
            "Iteration 9601/10000, Loss: 0.023089168593287468\n",
            "Iteration 9602/10000, Loss: 0.023088488727808\n",
            "Iteration 9603/10000, Loss: 0.023087812587618828\n",
            "Iteration 9604/10000, Loss: 0.023087134584784508\n",
            "Iteration 9605/10000, Loss: 0.02308645471930504\n",
            "Iteration 9606/10000, Loss: 0.023085780441761017\n",
            "Iteration 9607/10000, Loss: 0.023085100576281548\n",
            "Iteration 9608/10000, Loss: 0.023084424436092377\n",
            "Iteration 9609/10000, Loss: 0.023083746433258057\n",
            "Iteration 9610/10000, Loss: 0.023083068430423737\n",
            "Iteration 9611/10000, Loss: 0.023082390427589417\n",
            "Iteration 9612/10000, Loss: 0.023081714287400246\n",
            "Iteration 9613/10000, Loss: 0.023081034421920776\n",
            "Iteration 9614/10000, Loss: 0.023080356419086456\n",
            "Iteration 9615/10000, Loss: 0.023079680278897285\n",
            "Iteration 9616/10000, Loss: 0.023079002276062965\n",
            "Iteration 9617/10000, Loss: 0.023078324273228645\n",
            "Iteration 9618/10000, Loss: 0.023077648133039474\n",
            "Iteration 9619/10000, Loss: 0.023076971992850304\n",
            "Iteration 9620/10000, Loss: 0.023076293990015984\n",
            "Iteration 9621/10000, Loss: 0.023075615987181664\n",
            "Iteration 9622/10000, Loss: 0.023074939846992493\n",
            "Iteration 9623/10000, Loss: 0.023074259981513023\n",
            "Iteration 9624/10000, Loss: 0.023073585703969002\n",
            "Iteration 9625/10000, Loss: 0.023072905838489532\n",
            "Iteration 9626/10000, Loss: 0.023072227835655212\n",
            "Iteration 9627/10000, Loss: 0.02307155355811119\n",
            "Iteration 9628/10000, Loss: 0.02307087555527687\n",
            "Iteration 9629/10000, Loss: 0.0230701994150877\n",
            "Iteration 9630/10000, Loss: 0.02306951954960823\n",
            "Iteration 9631/10000, Loss: 0.02306884154677391\n",
            "Iteration 9632/10000, Loss: 0.02306816540658474\n",
            "Iteration 9633/10000, Loss: 0.02306748926639557\n",
            "Iteration 9634/10000, Loss: 0.0230668094009161\n",
            "Iteration 9635/10000, Loss: 0.023066135123372078\n",
            "Iteration 9636/10000, Loss: 0.023065458983182907\n",
            "Iteration 9637/10000, Loss: 0.023064780980348587\n",
            "Iteration 9638/10000, Loss: 0.023064102977514267\n",
            "Iteration 9639/10000, Loss: 0.023063428699970245\n",
            "Iteration 9640/10000, Loss: 0.023062748834490776\n",
            "Iteration 9641/10000, Loss: 0.023062074556946754\n",
            "Iteration 9642/10000, Loss: 0.023061396554112434\n",
            "Iteration 9643/10000, Loss: 0.023060720413923264\n",
            "Iteration 9644/10000, Loss: 0.023060042411088943\n",
            "Iteration 9645/10000, Loss: 0.02305937185883522\n",
            "Iteration 9646/10000, Loss: 0.02305869199335575\n",
            "Iteration 9647/10000, Loss: 0.02305801399052143\n",
            "Iteration 9648/10000, Loss: 0.02305733971297741\n",
            "Iteration 9649/10000, Loss: 0.02305666357278824\n",
            "Iteration 9650/10000, Loss: 0.02305598556995392\n",
            "Iteration 9651/10000, Loss: 0.023055309429764748\n",
            "Iteration 9652/10000, Loss: 0.023054635152220726\n",
            "Iteration 9653/10000, Loss: 0.023053957149386406\n",
            "Iteration 9654/10000, Loss: 0.023053279146552086\n",
            "Iteration 9655/10000, Loss: 0.023052601143717766\n",
            "Iteration 9656/10000, Loss: 0.023051928728818893\n",
            "Iteration 9657/10000, Loss: 0.023051248863339424\n",
            "Iteration 9658/10000, Loss: 0.023050574585795403\n",
            "Iteration 9659/10000, Loss: 0.02304989844560623\n",
            "Iteration 9660/10000, Loss: 0.02304922603070736\n",
            "Iteration 9661/10000, Loss: 0.02304854430258274\n",
            "Iteration 9662/10000, Loss: 0.02304786816239357\n",
            "Iteration 9663/10000, Loss: 0.0230471920222044\n",
            "Iteration 9664/10000, Loss: 0.023046517744660378\n",
            "Iteration 9665/10000, Loss: 0.023045839741826057\n",
            "Iteration 9666/10000, Loss: 0.023045165464282036\n",
            "Iteration 9667/10000, Loss: 0.023044489324092865\n",
            "Iteration 9668/10000, Loss: 0.023043809458613396\n",
            "Iteration 9669/10000, Loss: 0.023043135181069374\n",
            "Iteration 9670/10000, Loss: 0.023042459040880203\n",
            "Iteration 9671/10000, Loss: 0.023041782900691032\n",
            "Iteration 9672/10000, Loss: 0.023041104897856712\n",
            "Iteration 9673/10000, Loss: 0.02304043062031269\n",
            "Iteration 9674/10000, Loss: 0.02303975261747837\n",
            "Iteration 9675/10000, Loss: 0.023039082065224648\n",
            "Iteration 9676/10000, Loss: 0.023038409650325775\n",
            "Iteration 9677/10000, Loss: 0.023037731647491455\n",
            "Iteration 9678/10000, Loss: 0.023037061095237732\n",
            "Iteration 9679/10000, Loss: 0.02303638495504856\n",
            "Iteration 9680/10000, Loss: 0.02303571254014969\n",
            "Iteration 9681/10000, Loss: 0.023035038262605667\n",
            "Iteration 9682/10000, Loss: 0.023034362122416496\n",
            "Iteration 9683/10000, Loss: 0.023033693432807922\n",
            "Iteration 9684/10000, Loss: 0.0230330191552639\n",
            "Iteration 9685/10000, Loss: 0.02303234301507473\n",
            "Iteration 9686/10000, Loss: 0.02303166873753071\n",
            "Iteration 9687/10000, Loss: 0.023030996322631836\n",
            "Iteration 9688/10000, Loss: 0.023030323907732964\n",
            "Iteration 9689/10000, Loss: 0.02302965335547924\n",
            "Iteration 9690/10000, Loss: 0.023028980940580368\n",
            "Iteration 9691/10000, Loss: 0.023028308525681496\n",
            "Iteration 9692/10000, Loss: 0.023027636110782623\n",
            "Iteration 9693/10000, Loss: 0.02302696369588375\n",
            "Iteration 9694/10000, Loss: 0.02302629128098488\n",
            "Iteration 9695/10000, Loss: 0.023025618866086006\n",
            "Iteration 9696/10000, Loss: 0.023024946451187134\n",
            "Iteration 9697/10000, Loss: 0.02302427403628826\n",
            "Iteration 9698/10000, Loss: 0.02302360162138939\n",
            "Iteration 9699/10000, Loss: 0.023022929206490517\n",
            "Iteration 9700/10000, Loss: 0.023022258654236794\n",
            "Iteration 9701/10000, Loss: 0.023021584376692772\n",
            "Iteration 9702/10000, Loss: 0.02302091382443905\n",
            "Iteration 9703/10000, Loss: 0.023020239546895027\n",
            "Iteration 9704/10000, Loss: 0.023019568994641304\n",
            "Iteration 9705/10000, Loss: 0.02301889657974243\n",
            "Iteration 9706/10000, Loss: 0.02301822416484356\n",
            "Iteration 9707/10000, Loss: 0.023017551749944687\n",
            "Iteration 9708/10000, Loss: 0.023016881197690964\n",
            "Iteration 9709/10000, Loss: 0.02301620878279209\n",
            "Iteration 9710/10000, Loss: 0.02301553636789322\n",
            "Iteration 9711/10000, Loss: 0.023014863952994347\n",
            "Iteration 9712/10000, Loss: 0.023014195263385773\n",
            "Iteration 9713/10000, Loss: 0.02301352098584175\n",
            "Iteration 9714/10000, Loss: 0.02301284857094288\n",
            "Iteration 9715/10000, Loss: 0.023012176156044006\n",
            "Iteration 9716/10000, Loss: 0.023011505603790283\n",
            "Iteration 9717/10000, Loss: 0.02301083318889141\n",
            "Iteration 9718/10000, Loss: 0.023010162636637688\n",
            "Iteration 9719/10000, Loss: 0.023009490221738815\n",
            "Iteration 9720/10000, Loss: 0.023008817806839943\n",
            "Iteration 9721/10000, Loss: 0.02300814911723137\n",
            "Iteration 9722/10000, Loss: 0.023007474839687347\n",
            "Iteration 9723/10000, Loss: 0.023006802424788475\n",
            "Iteration 9724/10000, Loss: 0.023006131872534752\n",
            "Iteration 9725/10000, Loss: 0.02300545759499073\n",
            "Iteration 9726/10000, Loss: 0.023004787042737007\n",
            "Iteration 9727/10000, Loss: 0.023004116490483284\n",
            "Iteration 9728/10000, Loss: 0.023003442212939262\n",
            "Iteration 9729/10000, Loss: 0.02300277352333069\n",
            "Iteration 9730/10000, Loss: 0.023002099245786667\n",
            "Iteration 9731/10000, Loss: 0.023001428693532944\n",
            "Iteration 9732/10000, Loss: 0.02300076000392437\n",
            "Iteration 9733/10000, Loss: 0.0230000838637352\n",
            "Iteration 9734/10000, Loss: 0.022999413311481476\n",
            "Iteration 9735/10000, Loss: 0.022998742759227753\n",
            "Iteration 9736/10000, Loss: 0.02299807034432888\n",
            "Iteration 9737/10000, Loss: 0.022997399792075157\n",
            "Iteration 9738/10000, Loss: 0.022996727377176285\n",
            "Iteration 9739/10000, Loss: 0.02299605682492256\n",
            "Iteration 9740/10000, Loss: 0.02299538627266884\n",
            "Iteration 9741/10000, Loss: 0.022994715720415115\n",
            "Iteration 9742/10000, Loss: 0.022994043305516243\n",
            "Iteration 9743/10000, Loss: 0.02299337275326252\n",
            "Iteration 9744/10000, Loss: 0.022992704063653946\n",
            "Iteration 9745/10000, Loss: 0.022992027923464775\n",
            "Iteration 9746/10000, Loss: 0.0229913592338562\n",
            "Iteration 9747/10000, Loss: 0.02299068681895733\n",
            "Iteration 9748/10000, Loss: 0.022990016266703606\n",
            "Iteration 9749/10000, Loss: 0.022989343851804733\n",
            "Iteration 9750/10000, Loss: 0.02298867329955101\n",
            "Iteration 9751/10000, Loss: 0.022988002747297287\n",
            "Iteration 9752/10000, Loss: 0.022987332195043564\n",
            "Iteration 9753/10000, Loss: 0.02298665978014469\n",
            "Iteration 9754/10000, Loss: 0.02298598922789097\n",
            "Iteration 9755/10000, Loss: 0.022985318675637245\n",
            "Iteration 9756/10000, Loss: 0.022984646260738373\n",
            "Iteration 9757/10000, Loss: 0.0229839738458395\n",
            "Iteration 9758/10000, Loss: 0.022983305156230927\n",
            "Iteration 9759/10000, Loss: 0.022982632741332054\n",
            "Iteration 9760/10000, Loss: 0.02298196218907833\n",
            "Iteration 9761/10000, Loss: 0.022981291636824608\n",
            "Iteration 9762/10000, Loss: 0.022980619221925735\n",
            "Iteration 9763/10000, Loss: 0.022979948669672012\n",
            "Iteration 9764/10000, Loss: 0.02297927998006344\n",
            "Iteration 9765/10000, Loss: 0.022978609427809715\n",
            "Iteration 9766/10000, Loss: 0.022977935150265694\n",
            "Iteration 9767/10000, Loss: 0.02297726273536682\n",
            "Iteration 9768/10000, Loss: 0.022976597771048546\n",
            "Iteration 9769/10000, Loss: 0.022975921630859375\n",
            "Iteration 9770/10000, Loss: 0.0229752529412508\n",
            "Iteration 9771/10000, Loss: 0.022974582388997078\n",
            "Iteration 9772/10000, Loss: 0.022973909974098206\n",
            "Iteration 9773/10000, Loss: 0.022973237559199333\n",
            "Iteration 9774/10000, Loss: 0.02297256700694561\n",
            "Iteration 9775/10000, Loss: 0.022971896454691887\n",
            "Iteration 9776/10000, Loss: 0.022971227765083313\n",
            "Iteration 9777/10000, Loss: 0.02297055721282959\n",
            "Iteration 9778/10000, Loss: 0.022969884797930717\n",
            "Iteration 9779/10000, Loss: 0.022969214245676994\n",
            "Iteration 9780/10000, Loss: 0.02296854369342327\n",
            "Iteration 9781/10000, Loss: 0.022967873141169548\n",
            "Iteration 9782/10000, Loss: 0.022967202588915825\n",
            "Iteration 9783/10000, Loss: 0.022966530174016953\n",
            "Iteration 9784/10000, Loss: 0.02296585775911808\n",
            "Iteration 9785/10000, Loss: 0.022965190932154655\n",
            "Iteration 9786/10000, Loss: 0.022964520379900932\n",
            "Iteration 9787/10000, Loss: 0.02296384796500206\n",
            "Iteration 9788/10000, Loss: 0.022963177412748337\n",
            "Iteration 9789/10000, Loss: 0.022962508723139763\n",
            "Iteration 9790/10000, Loss: 0.02296183630824089\n",
            "Iteration 9791/10000, Loss: 0.022961167618632317\n",
            "Iteration 9792/10000, Loss: 0.022960495203733444\n",
            "Iteration 9793/10000, Loss: 0.02295982465147972\n",
            "Iteration 9794/10000, Loss: 0.022959154099225998\n",
            "Iteration 9795/10000, Loss: 0.022958487272262573\n",
            "Iteration 9796/10000, Loss: 0.0229578148573637\n",
            "Iteration 9797/10000, Loss: 0.022957144305109978\n",
            "Iteration 9798/10000, Loss: 0.022956473752856255\n",
            "Iteration 9799/10000, Loss: 0.02295580506324768\n",
            "Iteration 9800/10000, Loss: 0.022955134510993958\n",
            "Iteration 9801/10000, Loss: 0.022954462096095085\n",
            "Iteration 9802/10000, Loss: 0.02295379340648651\n",
            "Iteration 9803/10000, Loss: 0.02295312099158764\n",
            "Iteration 9804/10000, Loss: 0.022952450439333916\n",
            "Iteration 9805/10000, Loss: 0.02295178361237049\n",
            "Iteration 9806/10000, Loss: 0.02295111119747162\n",
            "Iteration 9807/10000, Loss: 0.022950442507863045\n",
            "Iteration 9808/10000, Loss: 0.02294977195560932\n",
            "Iteration 9809/10000, Loss: 0.0229491014033556\n",
            "Iteration 9810/10000, Loss: 0.022948430851101875\n",
            "Iteration 9811/10000, Loss: 0.022947758436203003\n",
            "Iteration 9812/10000, Loss: 0.022947091609239578\n",
            "Iteration 9813/10000, Loss: 0.022946419194340706\n",
            "Iteration 9814/10000, Loss: 0.022945750504732132\n",
            "Iteration 9815/10000, Loss: 0.02294507995247841\n",
            "Iteration 9816/10000, Loss: 0.022944409400224686\n",
            "Iteration 9817/10000, Loss: 0.022943738847970963\n",
            "Iteration 9818/10000, Loss: 0.02294307015836239\n",
            "Iteration 9819/10000, Loss: 0.022942401468753815\n",
            "Iteration 9820/10000, Loss: 0.022941729053854942\n",
            "Iteration 9821/10000, Loss: 0.02294106036424637\n",
            "Iteration 9822/10000, Loss: 0.022940391674637794\n",
            "Iteration 9823/10000, Loss: 0.022939719259738922\n",
            "Iteration 9824/10000, Loss: 0.0229390487074852\n",
            "Iteration 9825/10000, Loss: 0.022938380017876625\n",
            "Iteration 9826/10000, Loss: 0.02293771132826805\n",
            "Iteration 9827/10000, Loss: 0.022937040776014328\n",
            "Iteration 9828/10000, Loss: 0.022936370223760605\n",
            "Iteration 9829/10000, Loss: 0.02293569967150688\n",
            "Iteration 9830/10000, Loss: 0.022935032844543457\n",
            "Iteration 9831/10000, Loss: 0.022934360429644585\n",
            "Iteration 9832/10000, Loss: 0.02293369360268116\n",
            "Iteration 9833/10000, Loss: 0.022933023050427437\n",
            "Iteration 9834/10000, Loss: 0.022932350635528564\n",
            "Iteration 9835/10000, Loss: 0.02293168194591999\n",
            "Iteration 9836/10000, Loss: 0.022931011393666267\n",
            "Iteration 9837/10000, Loss: 0.022930342704057693\n",
            "Iteration 9838/10000, Loss: 0.02292967401444912\n",
            "Iteration 9839/10000, Loss: 0.022929005324840546\n",
            "Iteration 9840/10000, Loss: 0.022928332909941673\n",
            "Iteration 9841/10000, Loss: 0.0229276642203331\n",
            "Iteration 9842/10000, Loss: 0.022926997393369675\n",
            "Iteration 9843/10000, Loss: 0.0229263287037611\n",
            "Iteration 9844/10000, Loss: 0.02292565628886223\n",
            "Iteration 9845/10000, Loss: 0.022924989461898804\n",
            "Iteration 9846/10000, Loss: 0.02292431890964508\n",
            "Iteration 9847/10000, Loss: 0.022923650220036507\n",
            "Iteration 9848/10000, Loss: 0.022922977805137634\n",
            "Iteration 9849/10000, Loss: 0.02292231097817421\n",
            "Iteration 9850/10000, Loss: 0.022921640425920486\n",
            "Iteration 9851/10000, Loss: 0.022920973598957062\n",
            "Iteration 9852/10000, Loss: 0.022920304909348488\n",
            "Iteration 9853/10000, Loss: 0.022919634357094765\n",
            "Iteration 9854/10000, Loss: 0.02291896566748619\n",
            "Iteration 9855/10000, Loss: 0.022918300703167915\n",
            "Iteration 9856/10000, Loss: 0.022917628288269043\n",
            "Iteration 9857/10000, Loss: 0.02291695773601532\n",
            "Iteration 9858/10000, Loss: 0.022916289046406746\n",
            "Iteration 9859/10000, Loss: 0.02291562221944332\n",
            "Iteration 9860/10000, Loss: 0.022914951667189598\n",
            "Iteration 9861/10000, Loss: 0.022914284840226173\n",
            "Iteration 9862/10000, Loss: 0.0229136161506176\n",
            "Iteration 9863/10000, Loss: 0.022912947461009026\n",
            "Iteration 9864/10000, Loss: 0.02291227877140045\n",
            "Iteration 9865/10000, Loss: 0.022911610081791878\n",
            "Iteration 9866/10000, Loss: 0.022910943254828453\n",
            "Iteration 9867/10000, Loss: 0.02291027270257473\n",
            "Iteration 9868/10000, Loss: 0.022909600287675858\n",
            "Iteration 9869/10000, Loss: 0.022908935323357582\n",
            "Iteration 9870/10000, Loss: 0.02290826477110386\n",
            "Iteration 9871/10000, Loss: 0.022907596081495285\n",
            "Iteration 9872/10000, Loss: 0.02290692925453186\n",
            "Iteration 9873/10000, Loss: 0.022906258702278137\n",
            "Iteration 9874/10000, Loss: 0.022905591875314713\n",
            "Iteration 9875/10000, Loss: 0.022904925048351288\n",
            "Iteration 9876/10000, Loss: 0.022904252633452415\n",
            "Iteration 9877/10000, Loss: 0.02290358394384384\n",
            "Iteration 9878/10000, Loss: 0.02290291339159012\n",
            "Iteration 9879/10000, Loss: 0.022902248427271843\n",
            "Iteration 9880/10000, Loss: 0.02290157973766327\n",
            "Iteration 9881/10000, Loss: 0.022900911048054695\n",
            "Iteration 9882/10000, Loss: 0.02290024235844612\n",
            "Iteration 9883/10000, Loss: 0.022899573668837547\n",
            "Iteration 9884/10000, Loss: 0.022898904979228973\n",
            "Iteration 9885/10000, Loss: 0.02289823815226555\n",
            "Iteration 9886/10000, Loss: 0.022897569462656975\n",
            "Iteration 9887/10000, Loss: 0.02289689891040325\n",
            "Iteration 9888/10000, Loss: 0.022896230220794678\n",
            "Iteration 9889/10000, Loss: 0.022895565256476402\n",
            "Iteration 9890/10000, Loss: 0.02289489470422268\n",
            "Iteration 9891/10000, Loss: 0.022894226014614105\n",
            "Iteration 9892/10000, Loss: 0.02289355918765068\n",
            "Iteration 9893/10000, Loss: 0.022892890498042107\n",
            "Iteration 9894/10000, Loss: 0.022892221808433533\n",
            "Iteration 9895/10000, Loss: 0.02289155311882496\n",
            "Iteration 9896/10000, Loss: 0.022890884429216385\n",
            "Iteration 9897/10000, Loss: 0.02289021760225296\n",
            "Iteration 9898/10000, Loss: 0.022889547049999237\n",
            "Iteration 9899/10000, Loss: 0.022888880223035812\n",
            "Iteration 9900/10000, Loss: 0.02288820967078209\n",
            "Iteration 9901/10000, Loss: 0.022887542843818665\n",
            "Iteration 9902/10000, Loss: 0.02288687787950039\n",
            "Iteration 9903/10000, Loss: 0.022886207327246666\n",
            "Iteration 9904/10000, Loss: 0.022885536774992943\n",
            "Iteration 9905/10000, Loss: 0.022884869948029518\n",
            "Iteration 9906/10000, Loss: 0.022884203121066093\n",
            "Iteration 9907/10000, Loss: 0.02288353629410267\n",
            "Iteration 9908/10000, Loss: 0.022882865741848946\n",
            "Iteration 9909/10000, Loss: 0.02288219891488552\n",
            "Iteration 9910/10000, Loss: 0.022881530225276947\n",
            "Iteration 9911/10000, Loss: 0.022880861535668373\n",
            "Iteration 9912/10000, Loss: 0.02288019470870495\n",
            "Iteration 9913/10000, Loss: 0.022879526019096375\n",
            "Iteration 9914/10000, Loss: 0.02287885919213295\n",
            "Iteration 9915/10000, Loss: 0.022878190502524376\n",
            "Iteration 9916/10000, Loss: 0.02287752367556095\n",
            "Iteration 9917/10000, Loss: 0.022876854985952377\n",
            "Iteration 9918/10000, Loss: 0.022876186296343803\n",
            "Iteration 9919/10000, Loss: 0.02287551760673523\n",
            "Iteration 9920/10000, Loss: 0.022874850779771805\n",
            "Iteration 9921/10000, Loss: 0.02287418022751808\n",
            "Iteration 9922/10000, Loss: 0.022873517125844955\n",
            "Iteration 9923/10000, Loss: 0.022872846573591232\n",
            "Iteration 9924/10000, Loss: 0.022872179746627808\n",
            "Iteration 9925/10000, Loss: 0.022871514782309532\n",
            "Iteration 9926/10000, Loss: 0.022870846092700958\n",
            "Iteration 9927/10000, Loss: 0.022870175540447235\n",
            "Iteration 9928/10000, Loss: 0.02286950871348381\n",
            "Iteration 9929/10000, Loss: 0.022868841886520386\n",
            "Iteration 9930/10000, Loss: 0.02286817505955696\n",
            "Iteration 9931/10000, Loss: 0.022867508232593536\n",
            "Iteration 9932/10000, Loss: 0.022866839542984962\n",
            "Iteration 9933/10000, Loss: 0.02286617085337639\n",
            "Iteration 9934/10000, Loss: 0.022865504026412964\n",
            "Iteration 9935/10000, Loss: 0.02286483719944954\n",
            "Iteration 9936/10000, Loss: 0.022864170372486115\n",
            "Iteration 9937/10000, Loss: 0.02286350168287754\n",
            "Iteration 9938/10000, Loss: 0.022862832993268967\n",
            "Iteration 9939/10000, Loss: 0.022862166166305542\n",
            "Iteration 9940/10000, Loss: 0.022861499339342117\n",
            "Iteration 9941/10000, Loss: 0.022860830649733543\n",
            "Iteration 9942/10000, Loss: 0.02286016382277012\n",
            "Iteration 9943/10000, Loss: 0.022859498858451843\n",
            "Iteration 9944/10000, Loss: 0.02285882644355297\n",
            "Iteration 9945/10000, Loss: 0.022858163341879845\n",
            "Iteration 9946/10000, Loss: 0.02285749278962612\n",
            "Iteration 9947/10000, Loss: 0.022856824100017548\n",
            "Iteration 9948/10000, Loss: 0.022856157273054123\n",
            "Iteration 9949/10000, Loss: 0.022855492308735847\n",
            "Iteration 9950/10000, Loss: 0.022854821756482124\n",
            "Iteration 9951/10000, Loss: 0.02285415679216385\n",
            "Iteration 9952/10000, Loss: 0.022853491827845573\n",
            "Iteration 9953/10000, Loss: 0.02285282127559185\n",
            "Iteration 9954/10000, Loss: 0.022852152585983276\n",
            "Iteration 9955/10000, Loss: 0.02285148948431015\n",
            "Iteration 9956/10000, Loss: 0.022850820794701576\n",
            "Iteration 9957/10000, Loss: 0.02285015396773815\n",
            "Iteration 9958/10000, Loss: 0.022849485278129578\n",
            "Iteration 9959/10000, Loss: 0.022848818451166153\n",
            "Iteration 9960/10000, Loss: 0.022848153486847878\n",
            "Iteration 9961/10000, Loss: 0.02284749038517475\n",
            "Iteration 9962/10000, Loss: 0.022846823558211327\n",
            "Iteration 9963/10000, Loss: 0.02284616231918335\n",
            "Iteration 9964/10000, Loss: 0.022845493629574776\n",
            "Iteration 9965/10000, Loss: 0.0228448286652565\n",
            "Iteration 9966/10000, Loss: 0.022844165563583374\n",
            "Iteration 9967/10000, Loss: 0.0228434968739748\n",
            "Iteration 9968/10000, Loss: 0.022842833772301674\n",
            "Iteration 9969/10000, Loss: 0.022842170670628548\n",
            "Iteration 9970/10000, Loss: 0.022841505706310272\n",
            "Iteration 9971/10000, Loss: 0.022840840741991997\n",
            "Iteration 9972/10000, Loss: 0.022840173915028572\n",
            "Iteration 9973/10000, Loss: 0.022839510813355446\n",
            "Iteration 9974/10000, Loss: 0.02283884771168232\n",
            "Iteration 9975/10000, Loss: 0.022838179022073746\n",
            "Iteration 9976/10000, Loss: 0.02283751592040062\n",
            "Iteration 9977/10000, Loss: 0.022836850956082344\n",
            "Iteration 9978/10000, Loss: 0.02283618412911892\n",
            "Iteration 9979/10000, Loss: 0.022835521027445793\n",
            "Iteration 9980/10000, Loss: 0.022834859788417816\n",
            "Iteration 9981/10000, Loss: 0.02283419296145439\n",
            "Iteration 9982/10000, Loss: 0.022833529859781265\n",
            "Iteration 9983/10000, Loss: 0.02283286303281784\n",
            "Iteration 9984/10000, Loss: 0.022832201793789864\n",
            "Iteration 9985/10000, Loss: 0.022831536829471588\n",
            "Iteration 9986/10000, Loss: 0.022830873727798462\n",
            "Iteration 9987/10000, Loss: 0.022830205038189888\n",
            "Iteration 9988/10000, Loss: 0.02282954379916191\n",
            "Iteration 9989/10000, Loss: 0.022828876972198486\n",
            "Iteration 9990/10000, Loss: 0.02282821573317051\n",
            "Iteration 9991/10000, Loss: 0.022827548906207085\n",
            "Iteration 9992/10000, Loss: 0.02282688580453396\n",
            "Iteration 9993/10000, Loss: 0.022826218977570534\n",
            "Iteration 9994/10000, Loss: 0.02282555401325226\n",
            "Iteration 9995/10000, Loss: 0.02282489277422428\n",
            "Iteration 9996/10000, Loss: 0.022824227809906006\n",
            "Iteration 9997/10000, Loss: 0.02282356098294258\n",
            "Iteration 9998/10000, Loss: 0.022822897881269455\n",
            "Iteration 9999/10000, Loss: 0.02282223291695118\n",
            "Iteration 10000/10000, Loss: 0.022821569815278053\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########  R2 score for Train Data\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "score = r2_score(y_train.detach().numpy(), y_pred.detach().numpy())\n",
        "print(f\"R² Score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY8KwzmY97K2",
        "outputId": "7c960bd1-4b86-4580-a96d-d0eb6a3d6c5f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score: -2.8240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######  R2 score for Test Data\n",
        "\n",
        "y_pred_test = f1(X_test, Theta)\n",
        "loss_test = J2(X_test, Theta, y_test)\n",
        "print(f\"Test Loss: {loss_test.item()}\")\n",
        "\n",
        "score_test = r2_score(y_test.detach().numpy(), y_pred_test.detach().numpy())\n",
        "print(f\"Test R² Score: {score_test:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIb78Qjg721m",
        "outputId": "b658ab2f-d359-4f13-c849-114c8e2ffff8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.030000828206539154\n",
            "Test R² Score: -3.4537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing/Experiments"
      ],
      "metadata": {
        "id": "qz_PNDj3tn_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)  # Optional: for reproducibility\n",
        "df1 = pd.DataFrame(np.random.randint(0, 100, size=(5, 5)))\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0e0-bYjjfOr",
        "outputId": "67918881-fdb9-4252-c97f-e7a4d1fdd7b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0   1   2   3   4\n",
            "0  51  92  14  71  60\n",
            "1  20  82  86  74  74\n",
            "2  87  99  23   2  21\n",
            "3  52   1  87  29  37\n",
            "4   1  63  59  20  32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_scale(df):\n",
        "    return (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "df_scaled = min_max_scale(df1)\n",
        "\n",
        "print(df_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jTFAmu7pk-e",
        "outputId": "d970a30c-97db-407d-fd36-d7071c8c17dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4\n",
            "0  0.581395  0.928571  0.000000  0.958333  0.735849\n",
            "1  0.220930  0.826531  0.986301  1.000000  1.000000\n",
            "2  1.000000  1.000000  0.123288  0.000000  0.000000\n",
            "3  0.593023  0.000000  1.000000  0.375000  0.301887\n",
            "4  0.000000  0.632653  0.616438  0.250000  0.207547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)  # Optional: for reproducibility\n",
        "df1 = pd.DataFrame(np.random.randint(0, 100, size=(5, 5)))\n",
        "\n",
        "# print(df)\n",
        "\n",
        "# df2 = df1.drop(columns=[4], axis=1)\n",
        "\n",
        "# print(df2)\n",
        "\n",
        "X, y = df1.iloc[:, :-1], df1.iloc[:, -1]\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "print(type(X))\n",
        "print(type(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjpEBASFtzB1",
        "outputId": "e5a46103-1965-41d5-ee38-a806cd7de949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing/Experiments"
      ],
      "metadata": {
        "id": "KkTAfQG6Mp7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "_Ii1U0t88S6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(X,Theta):\n",
        "  return torch.dot(X,Theta)\n"
      ],
      "metadata": {
        "id": "H6WEg6fHChxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1(X,Theta):\n",
        "  return torch.matmul(X,Theta)"
      ],
      "metadata": {
        "id": "p2Fmor14YWou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def J(X,Theta,y):\n",
        "  # return torch.sum((f(X,Theta) - y)**2) / (2*len(X))\n",
        "  # return torch.mean((f(X,Theta) - y)**2)\n",
        "  # return torch.mean(torch.square(f(X,Theta) - y))\n",
        "  return (torch.square(f(X,Theta) - y))/2"
      ],
      "metadata": {
        "id": "T2H2wxaAAW0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def J2(X,Theta,y):\n",
        "  return (torch.mean(torch.square(f1(X,Theta) - y))/2)"
      ],
      "metadata": {
        "id": "ZHd1u3znZ0BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([3.0, 5.0, 4.5])\n",
        "Theta = torch.tensor([2.0, 4.0, -1.0], requires_grad=True)\n",
        "y = torch.tensor(23.0)\n",
        "\n",
        "y2 = J(X,Theta,y)\n",
        "print(y2)\n",
        "y2.backward()\n",
        "print(Theta.grad)\n",
        "\n",
        "# print(X)\n",
        "# print(Theta)\n",
        "\n",
        "# y = f(X,Theta)\n",
        "# print(y)\n",
        "\n",
        "# y.backward()\n",
        "# print(Theta.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxvq4uxt_Hog",
        "outputId": "cb8302b0-a980-40a9-f191-c1de80e952b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1250, grad_fn=<DivBackward0>)\n",
            "tensor([-4.5000, -7.5000, -6.7500])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Taking Dataset of 3 samples each having 3 features\n",
        "\n",
        "X = torch.tensor([[3.0, 5.0, 4.5],\n",
        "                  [2.5, -1.2, 2.32],\n",
        "                  [5.1, 5.5, -2.3]])\n",
        "Theta = torch.tensor([2.0, 4.0, -1.0], requires_grad=True)\n",
        "y = torch.tensor([20.7, 1.5, 33.0])\n",
        "\n",
        "# y1 = f1(X,Theta)\n",
        "# print(y1)\n",
        "# # len(X) == no. of samples\n",
        "# print(y1 - y)\n",
        "# print(torch.square(y1 - y))\n",
        "# print(torch.sum(torch.square(y1 - y)))\n",
        "# print(torch.mean(torch.square(y1 - y))/2)\n",
        "\n",
        "y2 = J2(X,Theta,y)\n",
        "print(y2)\n",
        "y2.backward()\n",
        "print(Theta.grad)\n",
        "print(0.1*Theta.grad)\n",
        "print(Theta - 0.1*Theta.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OAmm_AqXuE5",
        "outputId": "c9d15f07-7caf-438b-88da-6affc6dde49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.6657, grad_fn=<DivBackward0>)\n",
            "tensor([ 0.3333,  5.5313, -2.7495])\n",
            "tensor([ 0.0333,  0.5531, -0.2749])\n",
            "tensor([ 1.9667,  3.4469, -0.7251], grad_fn=<SubBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "f = x**2 + 2*x + 1\n",
        "\n",
        "f.backward()\n",
        "\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QaoPpCT_gv8",
        "outputId": "5f9d7bcf-c3e2-4277-bf5d-2e52f617ce02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.tensor(2.0)\n",
        "x2 = torch.tensor(5.0)\n",
        "x3 = torch.tensor(7.0)\n",
        "\n",
        "th1 = torch.tensor(0.0, requires_grad=True)\n",
        "th2 = torch.tensor(0.0, requires_grad=True)\n",
        "th3 = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "f = th1 * x1 + th2 * x2 + th3 * x3\n",
        "\n",
        "\n",
        "\n",
        "f.backward()\n",
        "\n",
        "print(th1.grad)\n",
        "print(th2.grad)\n",
        "print(th3.grad)"
      ],
      "metadata": {
        "id": "mpld95RB8hkV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}